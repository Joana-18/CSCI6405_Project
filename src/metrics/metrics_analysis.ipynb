{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>accuracy_mean_up</th>\n",
       "      <th>accuracy_std_up</th>\n",
       "      <th>f1_score_false_mean_up</th>\n",
       "      <th>f1_score_false_std_up</th>\n",
       "      <th>recall_false_mean_up</th>\n",
       "      <th>recall_false_std_up</th>\n",
       "      <th>precision_false_mean_up</th>\n",
       "      <th>precision_false_std_up</th>\n",
       "      <th>...</th>\n",
       "      <th>recall_false_mean_com</th>\n",
       "      <th>recall_false_std_com</th>\n",
       "      <th>precision_false_mean_com</th>\n",
       "      <th>precision_false_std_com</th>\n",
       "      <th>f1_score_true_mean_com</th>\n",
       "      <th>f1_score_true_std_com</th>\n",
       "      <th>recall_true_mean_com</th>\n",
       "      <th>recall_true_std_com</th>\n",
       "      <th>precision_true_mean_com</th>\n",
       "      <th>precision_true_std_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text - w/o MOC</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 287}</td>\n",
       "      <td>0.88320</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>0.92892</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.95321</td>\n",
       "      <td>0.00174</td>\n",
       "      <td>0.90585</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95644</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.84546</td>\n",
       "      <td>0.00265</td>\n",
       "      <td>0.42072</td>\n",
       "      <td>0.01499</td>\n",
       "      <td>0.31217</td>\n",
       "      <td>0.01384</td>\n",
       "      <td>0.64546</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost - No Text - w/o MOC</td>\n",
       "      <td>{'max_depth': 10, 'learning_rate': 0.06426402737206406, 'n_estimators': 627, 'subsample': 0.8653...</td>\n",
       "      <td>0.88267</td>\n",
       "      <td>0.00253</td>\n",
       "      <td>0.92680</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.92773</td>\n",
       "      <td>0.00326</td>\n",
       "      <td>0.92588</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91413</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>0.87978</td>\n",
       "      <td>0.00201</td>\n",
       "      <td>0.55084</td>\n",
       "      <td>0.00786</td>\n",
       "      <td>0.50855</td>\n",
       "      <td>0.00909</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.00783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text - w/o MOC</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.03842817453744308, 'solver': 'adam', 'learning_rate': 'invscal...</td>\n",
       "      <td>0.81533</td>\n",
       "      <td>0.00942</td>\n",
       "      <td>0.89273</td>\n",
       "      <td>0.00407</td>\n",
       "      <td>0.95978</td>\n",
       "      <td>0.03369</td>\n",
       "      <td>0.83611</td>\n",
       "      <td>0.02586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94992</td>\n",
       "      <td>0.06259</td>\n",
       "      <td>0.81490</td>\n",
       "      <td>0.01880</td>\n",
       "      <td>0.17143</td>\n",
       "      <td>0.14615</td>\n",
       "      <td>0.14211</td>\n",
       "      <td>0.15175</td>\n",
       "      <td>0.42774</td>\n",
       "      <td>0.15913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF - No Text - MOC</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 302}</td>\n",
       "      <td>0.88173</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>0.92809</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>0.95321</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.90427</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.96012</td>\n",
       "      <td>0.00163</td>\n",
       "      <td>0.84627</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>0.42669</td>\n",
       "      <td>0.00977</td>\n",
       "      <td>0.31382</td>\n",
       "      <td>0.00933</td>\n",
       "      <td>0.66668</td>\n",
       "      <td>0.01003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost - No Text - MOC</td>\n",
       "      <td>{'max_depth': 10, 'learning_rate': 0.05759944777512442, 'n_estimators': 888, 'subsample': 0.8586...</td>\n",
       "      <td>0.88287</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>0.92694</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>0.92814</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.92575</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91321</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>0.88081</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.55389</td>\n",
       "      <td>0.00821</td>\n",
       "      <td>0.51382</td>\n",
       "      <td>0.00904</td>\n",
       "      <td>0.60079</td>\n",
       "      <td>0.00860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP - No Text - MOC</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.004675388913205716, 'solver': 'adam', 'learning_rate': 'consta...</td>\n",
       "      <td>0.82100</td>\n",
       "      <td>0.01423</td>\n",
       "      <td>0.89546</td>\n",
       "      <td>0.00614</td>\n",
       "      <td>0.95704</td>\n",
       "      <td>0.04048</td>\n",
       "      <td>0.84399</td>\n",
       "      <td>0.03416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94231</td>\n",
       "      <td>0.06450</td>\n",
       "      <td>0.81270</td>\n",
       "      <td>0.02669</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.16608</td>\n",
       "      <td>0.13191</td>\n",
       "      <td>0.18601</td>\n",
       "      <td>0.32858</td>\n",
       "      <td>0.14198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 12, 'max_features_title': 13, 'criterion': 'log_loss', 'max_depth': 8, 'mi...</td>\n",
       "      <td>0.86313</td>\n",
       "      <td>0.00288</td>\n",
       "      <td>0.91985</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>0.98085</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>0.86600</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97776</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.82341</td>\n",
       "      <td>0.00223</td>\n",
       "      <td>0.27701</td>\n",
       "      <td>0.01733</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.01322</td>\n",
       "      <td>0.66652</td>\n",
       "      <td>0.01814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 17, 'max_features_title': 36, 'activation': 'relu', 'alpha': 0.00193360154...</td>\n",
       "      <td>0.76780</td>\n",
       "      <td>0.05698</td>\n",
       "      <td>0.84625</td>\n",
       "      <td>0.05537</td>\n",
       "      <td>0.83064</td>\n",
       "      <td>0.13140</td>\n",
       "      <td>0.88283</td>\n",
       "      <td>0.04772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.88604</td>\n",
       "      <td>0.13136</td>\n",
       "      <td>0.83906</td>\n",
       "      <td>0.04096</td>\n",
       "      <td>0.25845</td>\n",
       "      <td>0.20372</td>\n",
       "      <td>0.29375</td>\n",
       "      <td>0.27969</td>\n",
       "      <td>0.36971</td>\n",
       "      <td>0.20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 12, 'max_features_title': 21, 'max_depth': 10, 'learning_rate': 0.04518236...</td>\n",
       "      <td>0.88893</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.93065</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>0.93081</td>\n",
       "      <td>0.00319</td>\n",
       "      <td>0.93051</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92684</td>\n",
       "      <td>0.00349</td>\n",
       "      <td>0.87698</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.55002</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>0.48849</td>\n",
       "      <td>0.00574</td>\n",
       "      <td>0.62941</td>\n",
       "      <td>0.01183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 37, 'max_features_title': 41, 'activation': 'relu', 'alpha': 0.00648253447...</td>\n",
       "      <td>0.82667</td>\n",
       "      <td>0.03026</td>\n",
       "      <td>0.89148</td>\n",
       "      <td>0.02607</td>\n",
       "      <td>0.90225</td>\n",
       "      <td>0.07475</td>\n",
       "      <td>0.88681</td>\n",
       "      <td>0.02818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94707</td>\n",
       "      <td>0.04954</td>\n",
       "      <td>0.82004</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.21515</td>\n",
       "      <td>0.13811</td>\n",
       "      <td>0.17204</td>\n",
       "      <td>0.16078</td>\n",
       "      <td>0.45739</td>\n",
       "      <td>0.06349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 14, 'max_features_title': 37, 'max_depth': 10, 'learning_rate': 0.05606693...</td>\n",
       "      <td>0.88740</td>\n",
       "      <td>0.00319</td>\n",
       "      <td>0.92980</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.93131</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.92830</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92341</td>\n",
       "      <td>0.00251</td>\n",
       "      <td>0.87979</td>\n",
       "      <td>0.00198</td>\n",
       "      <td>0.55803</td>\n",
       "      <td>0.00837</td>\n",
       "      <td>0.50362</td>\n",
       "      <td>0.00888</td>\n",
       "      <td>0.62571</td>\n",
       "      <td>0.00964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RF - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title': 10, 'criterion': 'entropy', 'max_depth': 10, 'mi...</td>\n",
       "      <td>0.87080</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.92354</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>0.97452</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>0.87763</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97266</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.83164</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>0.33809</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.22533</td>\n",
       "      <td>0.00515</td>\n",
       "      <td>0.67683</td>\n",
       "      <td>0.00968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  \\\n",
       "0         RF - No Text - w/o MOC   \n",
       "1    XGBoost - No Text - w/o MOC   \n",
       "2        MLP - No Text - w/o MOC   \n",
       "3             RF - No Text - MOC   \n",
       "4        XGBoost - No Text - MOC   \n",
       "5            MLP - No Text - MOC   \n",
       "6        RF - No TFIDF - w/o MOC   \n",
       "7       MLP - No TFIDF - w/o MOC   \n",
       "8          XGBoost - TFIDF - MOC   \n",
       "9              MLP - TFIDF - MOC   \n",
       "10  XGBoost - No TFIDF - w/o MOC   \n",
       "11              RF - TFIDF - MOC   \n",
       "\n",
       "                                                                                                 params  \\\n",
       "0                   {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 287}   \n",
       "1   {'max_depth': 10, 'learning_rate': 0.06426402737206406, 'n_estimators': 627, 'subsample': 0.8653...   \n",
       "2   {'activation': 'relu', 'alpha': 0.03842817453744308, 'solver': 'adam', 'learning_rate': 'invscal...   \n",
       "3                   {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 302}   \n",
       "4   {'max_depth': 10, 'learning_rate': 0.05759944777512442, 'n_estimators': 888, 'subsample': 0.8586...   \n",
       "5   {'activation': 'relu', 'alpha': 0.004675388913205716, 'solver': 'adam', 'learning_rate': 'consta...   \n",
       "6   {'max_features_text': 12, 'max_features_title': 13, 'criterion': 'log_loss', 'max_depth': 8, 'mi...   \n",
       "7   {'max_features_text': 17, 'max_features_title': 36, 'activation': 'relu', 'alpha': 0.00193360154...   \n",
       "8   {'max_features_text': 12, 'max_features_title': 21, 'max_depth': 10, 'learning_rate': 0.04518236...   \n",
       "9   {'max_features_text': 37, 'max_features_title': 41, 'activation': 'relu', 'alpha': 0.00648253447...   \n",
       "10  {'max_features_text': 14, 'max_features_title': 37, 'max_depth': 10, 'learning_rate': 0.05606693...   \n",
       "11  {'max_features_text': 10, 'max_features_title': 10, 'criterion': 'entropy', 'max_depth': 10, 'mi...   \n",
       "\n",
       "    accuracy_mean_up  accuracy_std_up  f1_score_false_mean_up  \\\n",
       "0            0.88320          0.00193                 0.92892   \n",
       "1            0.88267          0.00253                 0.92680   \n",
       "2            0.81533          0.00942                 0.89273   \n",
       "3            0.88173          0.00127                 0.92809   \n",
       "4            0.88287          0.00332                 0.92694   \n",
       "5            0.82100          0.01423                 0.89546   \n",
       "6            0.86313          0.00288                 0.91985   \n",
       "7            0.76780          0.05698                 0.84625   \n",
       "8            0.88893          0.00269                 0.93065   \n",
       "9            0.82667          0.03026                 0.89148   \n",
       "10           0.88740          0.00319                 0.92980   \n",
       "11           0.87080          0.00181                 0.92354   \n",
       "\n",
       "    f1_score_false_std_up  recall_false_mean_up  recall_false_std_up  \\\n",
       "0                 0.00114               0.95321              0.00174   \n",
       "1                 0.00167               0.92773              0.00326   \n",
       "2                 0.00407               0.95978              0.03369   \n",
       "3                 0.00074               0.95321              0.00110   \n",
       "4                 0.00215               0.92814              0.00331   \n",
       "5                 0.00614               0.95704              0.04048   \n",
       "6                 0.00151               0.98085              0.00171   \n",
       "7                 0.05537               0.83064              0.13140   \n",
       "8                 0.00169               0.93081              0.00319   \n",
       "9                 0.02607               0.90225              0.07475   \n",
       "10                0.00200               0.93131              0.00282   \n",
       "11                0.00104               0.97452              0.00119   \n",
       "\n",
       "    precision_false_mean_up  precision_false_std_up  ...  \\\n",
       "0                   0.90585                 0.00205  ...   \n",
       "1                   0.92588                 0.00151  ...   \n",
       "2                   0.83611                 0.02586  ...   \n",
       "3                   0.90427                 0.00139  ...   \n",
       "4                   0.92575                 0.00154  ...   \n",
       "5                   0.84399                 0.03416  ...   \n",
       "6                   0.86600                 0.00327  ...   \n",
       "7                   0.88283                 0.04772  ...   \n",
       "8                   0.93051                 0.00289  ...   \n",
       "9                   0.88681                 0.02818  ...   \n",
       "10                  0.92830                 0.00240  ...   \n",
       "11                  0.87763                 0.00148  ...   \n",
       "\n",
       "    recall_false_mean_com  recall_false_std_com  precision_false_mean_com  \\\n",
       "0                 0.95644               0.00203                   0.84546   \n",
       "1                 0.91413               0.00215                   0.87978   \n",
       "2                 0.94992               0.06259                   0.81490   \n",
       "3                 0.96012               0.00163                   0.84627   \n",
       "4                 0.91321               0.00233                   0.88081   \n",
       "5                 0.94231               0.06450                   0.81270   \n",
       "6                 0.97776               0.00176                   0.82341   \n",
       "7                 0.88604               0.13136                   0.83906   \n",
       "8                 0.92684               0.00349                   0.87698   \n",
       "9                 0.94707               0.04954                   0.82004   \n",
       "10                0.92341               0.00251                   0.87979   \n",
       "11                0.97266               0.00075                   0.83164   \n",
       "\n",
       "    precision_false_std_com  f1_score_true_mean_com  f1_score_true_std_com  \\\n",
       "0                   0.00265                 0.42072                0.01499   \n",
       "1                   0.00201                 0.55084                0.00786   \n",
       "2                   0.01880                 0.17143                0.14615   \n",
       "3                   0.00173                 0.42669                0.00977   \n",
       "4                   0.00204                 0.55389                0.00821   \n",
       "5                   0.02669                 0.14100                0.16608   \n",
       "6                   0.00223                 0.27701                0.01733   \n",
       "7                   0.04096                 0.25845                0.20372   \n",
       "8                   0.00132                 0.55002                0.00676   \n",
       "9                   0.02405                 0.21515                0.13811   \n",
       "10                  0.00198                 0.55803                0.00837   \n",
       "11                  0.00099                 0.33809                0.00685   \n",
       "\n",
       "    recall_true_mean_com  recall_true_std_com  precision_true_mean_com  \\\n",
       "0                0.31217              0.01384                  0.64546   \n",
       "1                0.50855              0.00909                  0.60087   \n",
       "2                0.14211              0.15175                  0.42774   \n",
       "3                0.31382              0.00933                  0.66668   \n",
       "4                0.51382              0.00904                  0.60079   \n",
       "5                0.13191              0.18601                  0.32858   \n",
       "6                0.17500              0.01322                  0.66652   \n",
       "7                0.29375              0.27969                  0.36971   \n",
       "8                0.48849              0.00574                  0.62941   \n",
       "9                0.17204              0.16078                  0.45739   \n",
       "10               0.50362              0.00888                  0.62571   \n",
       "11               0.22533              0.00515                  0.67683   \n",
       "\n",
       "    precision_true_std_com  \n",
       "0                  0.01511  \n",
       "1                  0.00783  \n",
       "2                  0.15913  \n",
       "3                  0.01003  \n",
       "4                  0.00860  \n",
       "5                  0.14198  \n",
       "6                  0.01814  \n",
       "7                  0.20664  \n",
       "8                  0.01183  \n",
       "9                  0.06349  \n",
       "10                 0.00964  \n",
       "11                 0.00968  \n",
       "\n",
       "[12 rows x 44 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = pd.read_csv(r\"C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\best_hyperparams_new_FINAL2.csv\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metrics.sort_values('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 17, 'max_features_title': 36, 'activation': 'relu', 'alpha': 0.0019336015407920722, 'solver': 'adam', 'learning_rate': 'invscaling', 'hidden_layer_sizes': (63, 89)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP - No Text - MOC</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.004675388913205716, 'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (91, 67)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text - w/o MOC</td>\n",
       "      <td>{'activation': 'relu', 'alpha': 0.03842817453744308, 'solver': 'adam', 'learning_rate': 'invscaling', 'hidden_layer_sizes': (13, 81)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 37, 'max_features_title': 41, 'activation': 'relu', 'alpha': 0.006482534474954724, 'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (65, 43)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 12, 'max_features_title': 13, 'criterion': 'log_loss', 'max_depth': 8, 'min_samples_split': 12, 'n_estimators': 157}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF - No Text - MOC</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 302}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text - w/o MOC</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 287}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RF - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title': 10, 'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 19, 'n_estimators': 941}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost - No TFIDF - w/o MOC</td>\n",
       "      <td>{'max_features_text': 14, 'max_features_title': 37, 'max_depth': 10, 'learning_rate': 0.056066935892836206, 'n_estimators': 984, 'subsample': 0.8297156954923897, 'reg_alpha': 3.19320566145566e-06, 'reg_lambda': 9.869561622673837e-05}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost - No Text - MOC</td>\n",
       "      <td>{'max_depth': 10, 'learning_rate': 0.05759944777512442, 'n_estimators': 888, 'subsample': 0.8586670966932264, 'reg_alpha': 0.020728619620384324, 'reg_lambda': 0.0015814388152238792}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost - No Text - w/o MOC</td>\n",
       "      <td>{'max_depth': 10, 'learning_rate': 0.06426402737206406, 'n_estimators': 627, 'subsample': 0.8653747817170453, 'reg_alpha': 5.61159205164921e-06, 'reg_lambda': 9.560415043948457e-05}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost - TFIDF - MOC</td>\n",
       "      <td>{'max_features_text': 12, 'max_features_title': 21, 'max_depth': 10, 'learning_rate': 0.04518236153255944, 'n_estimators': 700, 'subsample': 0.7753752238443586, 'reg_alpha': 7.69181067560494e-05, 'reg_lambda': 0.00016221834498403277}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  \\\n",
       "7       MLP - No TFIDF - w/o MOC   \n",
       "5            MLP - No Text - MOC   \n",
       "2        MLP - No Text - w/o MOC   \n",
       "9              MLP - TFIDF - MOC   \n",
       "6        RF - No TFIDF - w/o MOC   \n",
       "3             RF - No Text - MOC   \n",
       "0         RF - No Text - w/o MOC   \n",
       "11              RF - TFIDF - MOC   \n",
       "10  XGBoost - No TFIDF - w/o MOC   \n",
       "4        XGBoost - No Text - MOC   \n",
       "1    XGBoost - No Text - w/o MOC   \n",
       "8          XGBoost - TFIDF - MOC   \n",
       "\n",
       "                                                                                                                                                                                                                                       params  \n",
       "7                                                  {'max_features_text': 17, 'max_features_title': 36, 'activation': 'relu', 'alpha': 0.0019336015407920722, 'solver': 'adam', 'learning_rate': 'invscaling', 'hidden_layer_sizes': (63, 89)}  \n",
       "5                                                                                                        {'activation': 'relu', 'alpha': 0.004675388913205716, 'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (91, 67)}  \n",
       "2                                                                                                       {'activation': 'relu', 'alpha': 0.03842817453744308, 'solver': 'adam', 'learning_rate': 'invscaling', 'hidden_layer_sizes': (13, 81)}  \n",
       "9                                                     {'max_features_text': 37, 'max_features_title': 41, 'activation': 'relu', 'alpha': 0.006482534474954724, 'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (65, 43)}  \n",
       "6                                                                                                  {'max_features_text': 12, 'max_features_title': 13, 'criterion': 'log_loss', 'max_depth': 8, 'min_samples_split': 12, 'n_estimators': 157}  \n",
       "3                                                                                                                                                         {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 7, 'n_estimators': 302}  \n",
       "0                                                                                                                                                         {'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 3, 'n_estimators': 287}  \n",
       "11                                                                                                 {'max_features_text': 10, 'max_features_title': 10, 'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 19, 'n_estimators': 941}  \n",
       "10  {'max_features_text': 14, 'max_features_title': 37, 'max_depth': 10, 'learning_rate': 0.056066935892836206, 'n_estimators': 984, 'subsample': 0.8297156954923897, 'reg_alpha': 3.19320566145566e-06, 'reg_lambda': 9.869561622673837e-05}  \n",
       "4                                                       {'max_depth': 10, 'learning_rate': 0.05759944777512442, 'n_estimators': 888, 'subsample': 0.8586670966932264, 'reg_alpha': 0.020728619620384324, 'reg_lambda': 0.0015814388152238792}  \n",
       "1                                                       {'max_depth': 10, 'learning_rate': 0.06426402737206406, 'n_estimators': 627, 'subsample': 0.8653747817170453, 'reg_alpha': 5.61159205164921e-06, 'reg_lambda': 9.560415043948457e-05}  \n",
       "8   {'max_features_text': 12, 'max_features_title': 21, 'max_depth': 10, 'learning_rate': 0.04518236153255944, 'n_estimators': 700, 'subsample': 0.7753752238443586, 'reg_alpha': 7.69181067560494e-05, 'reg_lambda': 0.00016221834498403277}  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics[['model', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[['accuracy_mean_up', 'accuracy_std_up',\n",
    "       'f1_score_false_mean_up', 'f1_score_false_std_up',\n",
    "       'recall_false_mean_up', 'recall_false_std_up',\n",
    "       'precision_false_mean_up', 'precision_false_std_up',\n",
    "       'f1_score_true_mean_up', 'f1_score_true_std_up', 'recall_true_mean_up',\n",
    "       'recall_true_std_up', 'precision_true_mean_up', 'precision_true_std_up',\n",
    "       'accuracy_mean_down', 'accuracy_std_down', 'f1_score_false_mean_down',\n",
    "       'f1_score_false_std_down', 'recall_false_mean_down',\n",
    "       'recall_false_std_down', 'precision_false_mean_down',\n",
    "       'precision_false_std_down', 'f1_score_true_mean_down',\n",
    "       'f1_score_true_std_down', 'recall_true_mean_down',\n",
    "       'recall_true_std_down', 'precision_true_mean_down',\n",
    "       'precision_true_std_down', 'accuracy_mean_com', 'accuracy_std_com',\n",
    "       'f1_score_false_mean_com', 'f1_score_false_std_com',\n",
    "       'recall_false_mean_com', 'recall_false_std_com',\n",
    "       'precision_false_mean_com', 'precision_false_std_com',\n",
    "       'f1_score_true_mean_com', 'f1_score_true_std_com',\n",
    "       'recall_true_mean_com', 'recall_true_std_com',\n",
    "       'precision_true_mean_com', 'precision_true_std_com']] = metrics[['accuracy_mean_up', 'accuracy_std_up',\n",
    "       'f1_score_false_mean_up', 'f1_score_false_std_up',\n",
    "       'recall_false_mean_up', 'recall_false_std_up',\n",
    "       'precision_false_mean_up', 'precision_false_std_up',\n",
    "       'f1_score_true_mean_up', 'f1_score_true_std_up', 'recall_true_mean_up',\n",
    "       'recall_true_std_up', 'precision_true_mean_up', 'precision_true_std_up',\n",
    "       'accuracy_mean_down', 'accuracy_std_down', 'f1_score_false_mean_down',\n",
    "       'f1_score_false_std_down', 'recall_false_mean_down',\n",
    "       'recall_false_std_down', 'precision_false_mean_down',\n",
    "       'precision_false_std_down', 'f1_score_true_mean_down',\n",
    "       'f1_score_true_std_down', 'recall_true_mean_down',\n",
    "       'recall_true_std_down', 'precision_true_mean_down',\n",
    "       'precision_true_std_down', 'accuracy_mean_com', 'accuracy_std_com',\n",
    "       'f1_score_false_mean_com', 'f1_score_false_std_com',\n",
    "       'recall_false_mean_com', 'recall_false_std_com',\n",
    "       'precision_false_mean_com', 'precision_false_std_com',\n",
    "       'f1_score_true_mean_com', 'f1_score_true_std_com',\n",
    "       'recall_true_mean_com', 'recall_true_std_com',\n",
    "       'precision_true_mean_com', 'precision_true_std_com']].apply(lambda x: np.round(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = metrics[['model','accuracy_mean_up', 'accuracy_std_up',\n",
    "       'f1_score_false_mean_up', 'f1_score_false_std_up',\n",
    "       'recall_false_mean_up', 'recall_false_std_up',\n",
    "       'precision_false_mean_up', 'precision_false_std_up',\n",
    "       'f1_score_true_mean_up', 'f1_score_true_std_up', 'recall_true_mean_up',\n",
    "       'recall_true_std_up', 'precision_true_mean_up', 'precision_true_std_up',\n",
    "       'accuracy_mean_down', 'accuracy_std_down', 'f1_score_false_mean_down',\n",
    "       'f1_score_false_std_down', 'recall_false_mean_down',\n",
    "       'recall_false_std_down', 'precision_false_mean_down',\n",
    "       'precision_false_std_down', 'f1_score_true_mean_down',\n",
    "       'f1_score_true_std_down', 'recall_true_mean_down',\n",
    "       'recall_true_std_down', 'precision_true_mean_down',\n",
    "       'precision_true_std_down', 'accuracy_mean_com', 'accuracy_std_com',\n",
    "       'f1_score_false_mean_com', 'f1_score_false_std_com',\n",
    "       'recall_false_mean_com', 'recall_false_std_com',\n",
    "       'precision_false_mean_com', 'precision_false_std_com',\n",
    "       'f1_score_true_mean_com', 'f1_score_true_std_com',\n",
    "       'recall_true_mean_com', 'recall_true_std_com',\n",
    "       'precision_true_mean_com', 'precision_true_std_com']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy_mean_up</th>\n",
       "      <th>accuracy_std_up</th>\n",
       "      <th>f1_score_false_mean_up</th>\n",
       "      <th>f1_score_false_std_up</th>\n",
       "      <th>recall_false_mean_up</th>\n",
       "      <th>recall_false_std_up</th>\n",
       "      <th>precision_false_mean_up</th>\n",
       "      <th>precision_false_std_up</th>\n",
       "      <th>f1_score_true_mean_up</th>\n",
       "      <th>f1_score_true_std_up</th>\n",
       "      <th>recall_true_mean_up</th>\n",
       "      <th>recall_true_std_up</th>\n",
       "      <th>precision_true_mean_up</th>\n",
       "      <th>precision_true_std_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP - No TFIDF - w/o MOC</td>\n",
       "      <td>0.76780</td>\n",
       "      <td>0.05698</td>\n",
       "      <td>0.84625</td>\n",
       "      <td>0.05537</td>\n",
       "      <td>0.83064</td>\n",
       "      <td>0.13140</td>\n",
       "      <td>0.88283</td>\n",
       "      <td>0.04772</td>\n",
       "      <td>0.42567</td>\n",
       "      <td>0.16442</td>\n",
       "      <td>0.51538</td>\n",
       "      <td>0.27860</td>\n",
       "      <td>0.45015</td>\n",
       "      <td>0.20338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP - No Text - MOC</td>\n",
       "      <td>0.82100</td>\n",
       "      <td>0.01423</td>\n",
       "      <td>0.89546</td>\n",
       "      <td>0.00614</td>\n",
       "      <td>0.95704</td>\n",
       "      <td>0.04048</td>\n",
       "      <td>0.84399</td>\n",
       "      <td>0.03416</td>\n",
       "      <td>0.32812</td>\n",
       "      <td>0.20990</td>\n",
       "      <td>0.27458</td>\n",
       "      <td>0.21137</td>\n",
       "      <td>0.63710</td>\n",
       "      <td>0.15345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text - w/o MOC</td>\n",
       "      <td>0.81533</td>\n",
       "      <td>0.00942</td>\n",
       "      <td>0.89273</td>\n",
       "      <td>0.00407</td>\n",
       "      <td>0.95978</td>\n",
       "      <td>0.03369</td>\n",
       "      <td>0.83611</td>\n",
       "      <td>0.02586</td>\n",
       "      <td>0.30037</td>\n",
       "      <td>0.17560</td>\n",
       "      <td>0.23512</td>\n",
       "      <td>0.16683</td>\n",
       "      <td>0.55629</td>\n",
       "      <td>0.20791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLP - TFIDF - MOC</td>\n",
       "      <td>0.82667</td>\n",
       "      <td>0.03026</td>\n",
       "      <td>0.89148</td>\n",
       "      <td>0.02607</td>\n",
       "      <td>0.90225</td>\n",
       "      <td>0.07475</td>\n",
       "      <td>0.88681</td>\n",
       "      <td>0.02818</td>\n",
       "      <td>0.53737</td>\n",
       "      <td>0.05476</td>\n",
       "      <td>0.52308</td>\n",
       "      <td>0.15997</td>\n",
       "      <td>0.62313</td>\n",
       "      <td>0.10592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RF - No TFIDF - w/o MOC</td>\n",
       "      <td>0.86313</td>\n",
       "      <td>0.00288</td>\n",
       "      <td>0.91985</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>0.98085</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>0.86600</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>0.53178</td>\n",
       "      <td>0.01646</td>\n",
       "      <td>0.39030</td>\n",
       "      <td>0.01789</td>\n",
       "      <td>0.83556</td>\n",
       "      <td>0.00952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF - No Text - MOC</td>\n",
       "      <td>0.88173</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>0.92809</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>0.95321</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.90427</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.66715</td>\n",
       "      <td>0.00458</td>\n",
       "      <td>0.59465</td>\n",
       "      <td>0.00666</td>\n",
       "      <td>0.75985</td>\n",
       "      <td>0.00396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text - w/o MOC</td>\n",
       "      <td>0.88320</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>0.92892</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>0.95321</td>\n",
       "      <td>0.00174</td>\n",
       "      <td>0.90585</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.67261</td>\n",
       "      <td>0.00675</td>\n",
       "      <td>0.60201</td>\n",
       "      <td>0.00981</td>\n",
       "      <td>0.76211</td>\n",
       "      <td>0.00632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RF - TFIDF - MOC</td>\n",
       "      <td>0.87080</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.92354</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>0.97452</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>0.87763</td>\n",
       "      <td>0.00148</td>\n",
       "      <td>0.58356</td>\n",
       "      <td>0.00710</td>\n",
       "      <td>0.45418</td>\n",
       "      <td>0.00745</td>\n",
       "      <td>0.81613</td>\n",
       "      <td>0.00757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>XGBoost - No TFIDF - w/o MOC</td>\n",
       "      <td>0.88740</td>\n",
       "      <td>0.00319</td>\n",
       "      <td>0.92980</td>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.93131</td>\n",
       "      <td>0.00282</td>\n",
       "      <td>0.92830</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.71570</td>\n",
       "      <td>0.00812</td>\n",
       "      <td>0.71104</td>\n",
       "      <td>0.01028</td>\n",
       "      <td>0.72049</td>\n",
       "      <td>0.00907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost - No Text - MOC</td>\n",
       "      <td>0.88287</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>0.92694</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>0.92814</td>\n",
       "      <td>0.00331</td>\n",
       "      <td>0.92575</td>\n",
       "      <td>0.00154</td>\n",
       "      <td>0.70468</td>\n",
       "      <td>0.00727</td>\n",
       "      <td>0.70100</td>\n",
       "      <td>0.00620</td>\n",
       "      <td>0.70845</td>\n",
       "      <td>0.01038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost - No Text - w/o MOC</td>\n",
       "      <td>0.88267</td>\n",
       "      <td>0.00253</td>\n",
       "      <td>0.92680</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.92773</td>\n",
       "      <td>0.00326</td>\n",
       "      <td>0.92588</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>0.70451</td>\n",
       "      <td>0.00530</td>\n",
       "      <td>0.70167</td>\n",
       "      <td>0.00682</td>\n",
       "      <td>0.70747</td>\n",
       "      <td>0.00891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBoost - TFIDF - MOC</td>\n",
       "      <td>0.88893</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>0.93065</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>0.93081</td>\n",
       "      <td>0.00319</td>\n",
       "      <td>0.93051</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>0.72118</td>\n",
       "      <td>0.00747</td>\n",
       "      <td>0.72074</td>\n",
       "      <td>0.01289</td>\n",
       "      <td>0.72180</td>\n",
       "      <td>0.00817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           model  accuracy_mean_up  accuracy_std_up  \\\n",
       "7       MLP - No TFIDF - w/o MOC           0.76780          0.05698   \n",
       "5            MLP - No Text - MOC           0.82100          0.01423   \n",
       "2        MLP - No Text - w/o MOC           0.81533          0.00942   \n",
       "9              MLP - TFIDF - MOC           0.82667          0.03026   \n",
       "6        RF - No TFIDF - w/o MOC           0.86313          0.00288   \n",
       "3             RF - No Text - MOC           0.88173          0.00127   \n",
       "0         RF - No Text - w/o MOC           0.88320          0.00193   \n",
       "11              RF - TFIDF - MOC           0.87080          0.00181   \n",
       "10  XGBoost - No TFIDF - w/o MOC           0.88740          0.00319   \n",
       "4        XGBoost - No Text - MOC           0.88287          0.00332   \n",
       "1    XGBoost - No Text - w/o MOC           0.88267          0.00253   \n",
       "8          XGBoost - TFIDF - MOC           0.88893          0.00269   \n",
       "\n",
       "    f1_score_false_mean_up  f1_score_false_std_up  recall_false_mean_up  \\\n",
       "7                  0.84625                0.05537               0.83064   \n",
       "5                  0.89546                0.00614               0.95704   \n",
       "2                  0.89273                0.00407               0.95978   \n",
       "9                  0.89148                0.02607               0.90225   \n",
       "6                  0.91985                0.00151               0.98085   \n",
       "3                  0.92809                0.00074               0.95321   \n",
       "0                  0.92892                0.00114               0.95321   \n",
       "11                 0.92354                0.00104               0.97452   \n",
       "10                 0.92980                0.00200               0.93131   \n",
       "4                  0.92694                0.00215               0.92814   \n",
       "1                  0.92680                0.00167               0.92773   \n",
       "8                  0.93065                0.00169               0.93081   \n",
       "\n",
       "    recall_false_std_up  precision_false_mean_up  precision_false_std_up  \\\n",
       "7               0.13140                  0.88283                 0.04772   \n",
       "5               0.04048                  0.84399                 0.03416   \n",
       "2               0.03369                  0.83611                 0.02586   \n",
       "9               0.07475                  0.88681                 0.02818   \n",
       "6               0.00171                  0.86600                 0.00327   \n",
       "3               0.00110                  0.90427                 0.00139   \n",
       "0               0.00174                  0.90585                 0.00205   \n",
       "11              0.00119                  0.87763                 0.00148   \n",
       "10              0.00282                  0.92830                 0.00240   \n",
       "4               0.00331                  0.92575                 0.00154   \n",
       "1               0.00326                  0.92588                 0.00151   \n",
       "8               0.00319                  0.93051                 0.00289   \n",
       "\n",
       "    f1_score_true_mean_up  f1_score_true_std_up  recall_true_mean_up  \\\n",
       "7                 0.42567               0.16442              0.51538   \n",
       "5                 0.32812               0.20990              0.27458   \n",
       "2                 0.30037               0.17560              0.23512   \n",
       "9                 0.53737               0.05476              0.52308   \n",
       "6                 0.53178               0.01646              0.39030   \n",
       "3                 0.66715               0.00458              0.59465   \n",
       "0                 0.67261               0.00675              0.60201   \n",
       "11                0.58356               0.00710              0.45418   \n",
       "10                0.71570               0.00812              0.71104   \n",
       "4                 0.70468               0.00727              0.70100   \n",
       "1                 0.70451               0.00530              0.70167   \n",
       "8                 0.72118               0.00747              0.72074   \n",
       "\n",
       "    recall_true_std_up  precision_true_mean_up  precision_true_std_up  \n",
       "7              0.27860                 0.45015                0.20338  \n",
       "5              0.21137                 0.63710                0.15345  \n",
       "2              0.16683                 0.55629                0.20791  \n",
       "9              0.15997                 0.62313                0.10592  \n",
       "6              0.01789                 0.83556                0.00952  \n",
       "3              0.00666                 0.75985                0.00396  \n",
       "0              0.00981                 0.76211                0.00632  \n",
       "11             0.00745                 0.81613                0.00757  \n",
       "10             0.01028                 0.72049                0.00907  \n",
       "4              0.00620                 0.70845                0.01038  \n",
       "1              0.00682                 0.70747                0.00891  \n",
       "8              0.01289                 0.72180                0.00817  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.iloc[:, 0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'matthews_corrcoef',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import get_scorer_names\n",
    "get_scorer_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_mean_down</th>\n",
       "      <th>accuracy_std_down</th>\n",
       "      <th>f1_score_false_mean_down</th>\n",
       "      <th>f1_score_false_std_down</th>\n",
       "      <th>recall_false_mean_down</th>\n",
       "      <th>recall_false_std_down</th>\n",
       "      <th>precision_false_mean_down</th>\n",
       "      <th>precision_false_std_down</th>\n",
       "      <th>f1_score_true_mean_down</th>\n",
       "      <th>f1_score_true_std_down</th>\n",
       "      <th>recall_true_mean_down</th>\n",
       "      <th>recall_true_std_down</th>\n",
       "      <th>precision_true_mean_down</th>\n",
       "      <th>precision_true_std_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.67340</td>\n",
       "      <td>0.09579</td>\n",
       "      <td>0.76790</td>\n",
       "      <td>0.12595</td>\n",
       "      <td>0.80569</td>\n",
       "      <td>0.20673</td>\n",
       "      <td>0.77066</td>\n",
       "      <td>0.03025</td>\n",
       "      <td>0.27349</td>\n",
       "      <td>0.15186</td>\n",
       "      <td>0.29949</td>\n",
       "      <td>0.24603</td>\n",
       "      <td>0.38654</td>\n",
       "      <td>0.09140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.70073</td>\n",
       "      <td>0.05527</td>\n",
       "      <td>0.80167</td>\n",
       "      <td>0.06539</td>\n",
       "      <td>0.85162</td>\n",
       "      <td>0.14099</td>\n",
       "      <td>0.77301</td>\n",
       "      <td>0.02701</td>\n",
       "      <td>0.28012</td>\n",
       "      <td>0.14915</td>\n",
       "      <td>0.27423</td>\n",
       "      <td>0.20687</td>\n",
       "      <td>0.37443</td>\n",
       "      <td>0.13119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.70567</td>\n",
       "      <td>0.02275</td>\n",
       "      <td>0.81246</td>\n",
       "      <td>0.02644</td>\n",
       "      <td>0.87347</td>\n",
       "      <td>0.08943</td>\n",
       "      <td>0.76549</td>\n",
       "      <td>0.02302</td>\n",
       "      <td>0.24696</td>\n",
       "      <td>0.16779</td>\n",
       "      <td>0.23138</td>\n",
       "      <td>0.17361</td>\n",
       "      <td>0.40289</td>\n",
       "      <td>0.06440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.72320</td>\n",
       "      <td>0.01635</td>\n",
       "      <td>0.82724</td>\n",
       "      <td>0.02056</td>\n",
       "      <td>0.90460</td>\n",
       "      <td>0.07459</td>\n",
       "      <td>0.76631</td>\n",
       "      <td>0.02088</td>\n",
       "      <td>0.24919</td>\n",
       "      <td>0.14868</td>\n",
       "      <td>0.21046</td>\n",
       "      <td>0.15302</td>\n",
       "      <td>0.43702</td>\n",
       "      <td>0.08539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.76133</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>0.85777</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.97428</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>0.76615</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>0.25880</td>\n",
       "      <td>0.00398</td>\n",
       "      <td>0.15944</td>\n",
       "      <td>0.00261</td>\n",
       "      <td>0.68687</td>\n",
       "      <td>0.01088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.76713</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>0.85920</td>\n",
       "      <td>0.00152</td>\n",
       "      <td>0.96191</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>0.77631</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>0.32708</td>\n",
       "      <td>0.00910</td>\n",
       "      <td>0.21658</td>\n",
       "      <td>0.00660</td>\n",
       "      <td>0.66788</td>\n",
       "      <td>0.01417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.76693</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>0.85886</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.96002</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.77699</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>0.33151</td>\n",
       "      <td>0.00863</td>\n",
       "      <td>0.22117</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.66186</td>\n",
       "      <td>0.01231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.76193</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>0.85780</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>0.97211</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>0.76755</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>0.26928</td>\n",
       "      <td>0.00349</td>\n",
       "      <td>0.16786</td>\n",
       "      <td>0.00250</td>\n",
       "      <td>0.68053</td>\n",
       "      <td>0.00901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.77960</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.85817</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>0.90271</td>\n",
       "      <td>0.00302</td>\n",
       "      <td>0.81783</td>\n",
       "      <td>0.00149</td>\n",
       "      <td>0.50582</td>\n",
       "      <td>0.00520</td>\n",
       "      <td>0.43163</td>\n",
       "      <td>0.00568</td>\n",
       "      <td>0.61090</td>\n",
       "      <td>0.00750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.76680</td>\n",
       "      <td>0.00286</td>\n",
       "      <td>0.84702</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.87401</td>\n",
       "      <td>0.00294</td>\n",
       "      <td>0.82166</td>\n",
       "      <td>0.00251</td>\n",
       "      <td>0.50964</td>\n",
       "      <td>0.00750</td>\n",
       "      <td>0.46378</td>\n",
       "      <td>0.00946</td>\n",
       "      <td>0.56566</td>\n",
       "      <td>0.00660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.76787</td>\n",
       "      <td>0.00455</td>\n",
       "      <td>0.84788</td>\n",
       "      <td>0.00315</td>\n",
       "      <td>0.87581</td>\n",
       "      <td>0.00469</td>\n",
       "      <td>0.82168</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>0.51027</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>0.46276</td>\n",
       "      <td>0.00870</td>\n",
       "      <td>0.56875</td>\n",
       "      <td>0.01119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.78527</td>\n",
       "      <td>0.00409</td>\n",
       "      <td>0.86149</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>0.90406</td>\n",
       "      <td>0.00457</td>\n",
       "      <td>0.82275</td>\n",
       "      <td>0.00229</td>\n",
       "      <td>0.52246</td>\n",
       "      <td>0.00813</td>\n",
       "      <td>0.44949</td>\n",
       "      <td>0.00797</td>\n",
       "      <td>0.62386</td>\n",
       "      <td>0.01204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy_mean_down  accuracy_std_down  f1_score_false_mean_down  \\\n",
       "7              0.67340            0.09579                   0.76790   \n",
       "5              0.70073            0.05527                   0.80167   \n",
       "2              0.70567            0.02275                   0.81246   \n",
       "9              0.72320            0.01635                   0.82724   \n",
       "6              0.76133            0.00123                   0.85777   \n",
       "3              0.76713            0.00262                   0.85920   \n",
       "0              0.76693            0.00224                   0.85886   \n",
       "11             0.76193            0.00101                   0.85780   \n",
       "10             0.77960            0.00246                   0.85817   \n",
       "4              0.76680            0.00286                   0.84702   \n",
       "1              0.76787            0.00455                   0.84788   \n",
       "8              0.78527            0.00409                   0.86149   \n",
       "\n",
       "    f1_score_false_std_down  recall_false_mean_down  recall_false_std_down  \\\n",
       "7                   0.12595                 0.80569                0.20673   \n",
       "5                   0.06539                 0.85162                0.14099   \n",
       "2                   0.02644                 0.87347                0.08943   \n",
       "9                   0.02056                 0.90460                0.07459   \n",
       "6                   0.00075                 0.97428                0.00109   \n",
       "3                   0.00152                 0.96191                0.00144   \n",
       "0                   0.00132                 0.96002                0.00185   \n",
       "11                  0.00063                 0.97211                0.00110   \n",
       "10                  0.00170                 0.90271                0.00302   \n",
       "4                   0.00186                 0.87401                0.00294   \n",
       "1                   0.00315                 0.87581                0.00469   \n",
       "8                   0.00281                 0.90406                0.00457   \n",
       "\n",
       "    precision_false_mean_down  precision_false_std_down  \\\n",
       "7                     0.77066                   0.03025   \n",
       "5                     0.77301                   0.02701   \n",
       "2                     0.76549                   0.02302   \n",
       "9                     0.76631                   0.02088   \n",
       "6                     0.76615                   0.00066   \n",
       "3                     0.77631                   0.00167   \n",
       "0                     0.77699                   0.00156   \n",
       "11                    0.76755                   0.00056   \n",
       "10                    0.81783                   0.00149   \n",
       "4                     0.82166                   0.00251   \n",
       "1                     0.82168                   0.00267   \n",
       "8                     0.82275                   0.00229   \n",
       "\n",
       "    f1_score_true_mean_down  f1_score_true_std_down  recall_true_mean_down  \\\n",
       "7                   0.27349                 0.15186                0.29949   \n",
       "5                   0.28012                 0.14915                0.27423   \n",
       "2                   0.24696                 0.16779                0.23138   \n",
       "9                   0.24919                 0.14868                0.21046   \n",
       "6                   0.25880                 0.00398                0.15944   \n",
       "3                   0.32708                 0.00910                0.21658   \n",
       "0                   0.33151                 0.00863                0.22117   \n",
       "11                  0.26928                 0.00349                0.16786   \n",
       "10                  0.50582                 0.00520                0.43163   \n",
       "4                   0.50964                 0.00750                0.46378   \n",
       "1                   0.51027                 0.00872                0.46276   \n",
       "8                   0.52246                 0.00813                0.44949   \n",
       "\n",
       "    recall_true_std_down  precision_true_mean_down  precision_true_std_down  \n",
       "7                0.24603                   0.38654                  0.09140  \n",
       "5                0.20687                   0.37443                  0.13119  \n",
       "2                0.17361                   0.40289                  0.06440  \n",
       "9                0.15302                   0.43702                  0.08539  \n",
       "6                0.00261                   0.68687                  0.01088  \n",
       "3                0.00660                   0.66788                  0.01417  \n",
       "0                0.00685                   0.66186                  0.01231  \n",
       "11               0.00250                   0.68053                  0.00901  \n",
       "10               0.00568                   0.61090                  0.00750  \n",
       "4                0.00946                   0.56566                  0.00660  \n",
       "1                0.00870                   0.56875                  0.01119  \n",
       "8                0.00797                   0.62386                  0.01204  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.iloc[:, 15:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy_mean_com</th>\n",
       "      <th>accuracy_std_com</th>\n",
       "      <th>f1_score_false_mean_com</th>\n",
       "      <th>f1_score_false_std_com</th>\n",
       "      <th>recall_false_mean_com</th>\n",
       "      <th>recall_false_std_com</th>\n",
       "      <th>precision_false_mean_com</th>\n",
       "      <th>precision_false_std_com</th>\n",
       "      <th>f1_score_true_mean_com</th>\n",
       "      <th>f1_score_true_std_com</th>\n",
       "      <th>recall_true_mean_com</th>\n",
       "      <th>recall_true_std_com</th>\n",
       "      <th>precision_true_mean_com</th>\n",
       "      <th>precision_true_std_com</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.76600</td>\n",
       "      <td>0.05369</td>\n",
       "      <td>0.85311</td>\n",
       "      <td>0.05353</td>\n",
       "      <td>0.88604</td>\n",
       "      <td>0.13136</td>\n",
       "      <td>0.83906</td>\n",
       "      <td>0.04096</td>\n",
       "      <td>0.25845</td>\n",
       "      <td>0.20372</td>\n",
       "      <td>0.29375</td>\n",
       "      <td>0.27969</td>\n",
       "      <td>0.36971</td>\n",
       "      <td>0.20664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.77807</td>\n",
       "      <td>0.01799</td>\n",
       "      <td>0.87053</td>\n",
       "      <td>0.01653</td>\n",
       "      <td>0.94231</td>\n",
       "      <td>0.06450</td>\n",
       "      <td>0.81270</td>\n",
       "      <td>0.02669</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.16608</td>\n",
       "      <td>0.13191</td>\n",
       "      <td>0.18601</td>\n",
       "      <td>0.32858</td>\n",
       "      <td>0.14198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.78620</td>\n",
       "      <td>0.02054</td>\n",
       "      <td>0.87547</td>\n",
       "      <td>0.01882</td>\n",
       "      <td>0.94992</td>\n",
       "      <td>0.06259</td>\n",
       "      <td>0.81490</td>\n",
       "      <td>0.01880</td>\n",
       "      <td>0.17143</td>\n",
       "      <td>0.14615</td>\n",
       "      <td>0.14211</td>\n",
       "      <td>0.15175</td>\n",
       "      <td>0.42774</td>\n",
       "      <td>0.15913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.79000</td>\n",
       "      <td>0.00951</td>\n",
       "      <td>0.87752</td>\n",
       "      <td>0.01055</td>\n",
       "      <td>0.94707</td>\n",
       "      <td>0.04954</td>\n",
       "      <td>0.82004</td>\n",
       "      <td>0.02405</td>\n",
       "      <td>0.21515</td>\n",
       "      <td>0.13811</td>\n",
       "      <td>0.17204</td>\n",
       "      <td>0.16078</td>\n",
       "      <td>0.45739</td>\n",
       "      <td>0.06349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.81507</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.89397</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>0.97776</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>0.82341</td>\n",
       "      <td>0.00223</td>\n",
       "      <td>0.27701</td>\n",
       "      <td>0.01733</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.01322</td>\n",
       "      <td>0.66652</td>\n",
       "      <td>0.01814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.82913</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.89960</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.96012</td>\n",
       "      <td>0.00163</td>\n",
       "      <td>0.84627</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>0.42669</td>\n",
       "      <td>0.00977</td>\n",
       "      <td>0.31382</td>\n",
       "      <td>0.00933</td>\n",
       "      <td>0.66668</td>\n",
       "      <td>0.01003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82587</td>\n",
       "      <td>0.00329</td>\n",
       "      <td>0.89753</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.95644</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>0.84546</td>\n",
       "      <td>0.00265</td>\n",
       "      <td>0.42072</td>\n",
       "      <td>0.01499</td>\n",
       "      <td>0.31217</td>\n",
       "      <td>0.01384</td>\n",
       "      <td>0.64546</td>\n",
       "      <td>0.01511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.82120</td>\n",
       "      <td>0.00145</td>\n",
       "      <td>0.89664</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>0.97266</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.83164</td>\n",
       "      <td>0.00099</td>\n",
       "      <td>0.33809</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.22533</td>\n",
       "      <td>0.00515</td>\n",
       "      <td>0.67683</td>\n",
       "      <td>0.00968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.83833</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.90107</td>\n",
       "      <td>0.00186</td>\n",
       "      <td>0.92341</td>\n",
       "      <td>0.00251</td>\n",
       "      <td>0.87979</td>\n",
       "      <td>0.00198</td>\n",
       "      <td>0.55803</td>\n",
       "      <td>0.00837</td>\n",
       "      <td>0.50362</td>\n",
       "      <td>0.00888</td>\n",
       "      <td>0.62571</td>\n",
       "      <td>0.00964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.83227</td>\n",
       "      <td>0.00292</td>\n",
       "      <td>0.89672</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>0.91321</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>0.88081</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>0.55389</td>\n",
       "      <td>0.00821</td>\n",
       "      <td>0.51382</td>\n",
       "      <td>0.00904</td>\n",
       "      <td>0.60079</td>\n",
       "      <td>0.00860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.83193</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>0.89663</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>0.91413</td>\n",
       "      <td>0.00215</td>\n",
       "      <td>0.87978</td>\n",
       "      <td>0.00201</td>\n",
       "      <td>0.55084</td>\n",
       "      <td>0.00786</td>\n",
       "      <td>0.50855</td>\n",
       "      <td>0.00909</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.00783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.83800</td>\n",
       "      <td>0.00316</td>\n",
       "      <td>0.90122</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.92684</td>\n",
       "      <td>0.00349</td>\n",
       "      <td>0.87698</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.55002</td>\n",
       "      <td>0.00676</td>\n",
       "      <td>0.48849</td>\n",
       "      <td>0.00574</td>\n",
       "      <td>0.62941</td>\n",
       "      <td>0.01183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    accuracy_mean_com  accuracy_std_com  f1_score_false_mean_com  \\\n",
       "7             0.76600           0.05369                  0.85311   \n",
       "5             0.77807           0.01799                  0.87053   \n",
       "2             0.78620           0.02054                  0.87547   \n",
       "9             0.79000           0.00951                  0.87752   \n",
       "6             0.81507           0.00241                  0.89397   \n",
       "3             0.82913           0.00205                  0.89960   \n",
       "0             0.82587           0.00329                  0.89753   \n",
       "11            0.82120           0.00145                  0.89664   \n",
       "10            0.83833           0.00300                  0.90107   \n",
       "4             0.83227           0.00292                  0.89672   \n",
       "1             0.83193           0.00267                  0.89663   \n",
       "8             0.83800           0.00316                  0.90122   \n",
       "\n",
       "    f1_score_false_std_com  recall_false_mean_com  recall_false_std_com  \\\n",
       "7                  0.05353                0.88604               0.13136   \n",
       "5                  0.01653                0.94231               0.06450   \n",
       "2                  0.01882                0.94992               0.06259   \n",
       "9                  0.01055                0.94707               0.04954   \n",
       "6                  0.00126                0.97776               0.00176   \n",
       "3                  0.00115                0.96012               0.00163   \n",
       "0                  0.00185                0.95644               0.00203   \n",
       "11                 0.00081                0.97266               0.00075   \n",
       "10                 0.00186                0.92341               0.00251   \n",
       "4                  0.00181                0.91321               0.00233   \n",
       "1                  0.00164                0.91413               0.00215   \n",
       "8                  0.00205                0.92684               0.00349   \n",
       "\n",
       "    precision_false_mean_com  precision_false_std_com  f1_score_true_mean_com  \\\n",
       "7                    0.83906                  0.04096                 0.25845   \n",
       "5                    0.81270                  0.02669                 0.14100   \n",
       "2                    0.81490                  0.01880                 0.17143   \n",
       "9                    0.82004                  0.02405                 0.21515   \n",
       "6                    0.82341                  0.00223                 0.27701   \n",
       "3                    0.84627                  0.00173                 0.42669   \n",
       "0                    0.84546                  0.00265                 0.42072   \n",
       "11                   0.83164                  0.00099                 0.33809   \n",
       "10                   0.87979                  0.00198                 0.55803   \n",
       "4                    0.88081                  0.00204                 0.55389   \n",
       "1                    0.87978                  0.00201                 0.55084   \n",
       "8                    0.87698                  0.00132                 0.55002   \n",
       "\n",
       "    f1_score_true_std_com  recall_true_mean_com  recall_true_std_com  \\\n",
       "7                 0.20372               0.29375              0.27969   \n",
       "5                 0.16608               0.13191              0.18601   \n",
       "2                 0.14615               0.14211              0.15175   \n",
       "9                 0.13811               0.17204              0.16078   \n",
       "6                 0.01733               0.17500              0.01322   \n",
       "3                 0.00977               0.31382              0.00933   \n",
       "0                 0.01499               0.31217              0.01384   \n",
       "11                0.00685               0.22533              0.00515   \n",
       "10                0.00837               0.50362              0.00888   \n",
       "4                 0.00821               0.51382              0.00904   \n",
       "1                 0.00786               0.50855              0.00909   \n",
       "8                 0.00676               0.48849              0.00574   \n",
       "\n",
       "    precision_true_mean_com  precision_true_std_com  \n",
       "7                   0.36971                 0.20664  \n",
       "5                   0.32858                 0.14198  \n",
       "2                   0.42774                 0.15913  \n",
       "9                   0.45739                 0.06349  \n",
       "6                   0.66652                 0.01814  \n",
       "3                   0.66668                 0.01003  \n",
       "0                   0.64546                 0.01511  \n",
       "11                  0.67683                 0.00968  \n",
       "10                  0.62571                 0.00964  \n",
       "4                   0.60079                 0.00860  \n",
       "1                   0.60087                 0.00783  \n",
       "8                   0.62941                 0.01183  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.iloc[:, 29:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['model'] = metrics['model'].apply(lambda x : x.replace('Random Forest', 'RF'))\n",
    "metrics['model'] = metrics['model'].apply(lambda x : x.replace('XGBoost', 'XGB'))\n",
    "melted_df = pd.melt(metrics, id_vars=['model', 'params'], var_name='metric', value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95400</td>\n",
       "      <td>upvote_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>upvote_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.94553</td>\n",
       "      <td>upvote_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>comments_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00657</td>\n",
       "      <td>comments_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00713</td>\n",
       "      <td>comments_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00199</td>\n",
       "      <td>comments_popular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>comments_popular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "0     RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "1    XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "2    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "3      MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "4       RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "..             ...                                                ...   \n",
       "247  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "248  MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "249    MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "250     RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "251    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "                     metric    value             class  \n",
       "0          accuracy_mean_up  0.95340    upvote_popular  \n",
       "1          accuracy_mean_up  0.95400    upvote_popular  \n",
       "2          accuracy_mean_up  0.95287    upvote_popular  \n",
       "3          accuracy_mean_up  0.94553    upvote_popular  \n",
       "4          accuracy_mean_up  0.95340    upvote_popular  \n",
       "..                      ...      ...               ...  \n",
       "247  precision_true_std_com  0.00000  comments_popular  \n",
       "248  precision_true_std_com  0.00657  comments_popular  \n",
       "249  precision_true_std_com  0.00713  comments_popular  \n",
       "250  precision_true_std_com  0.00199  comments_popular  \n",
       "251  precision_true_std_com  0.00673  comments_popular  \n",
       "\n",
       "[252 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df['class'] = melted_df['metric'] .apply(lambda x: 'upvote_popular' if 'up' in x else ('downvote_popular' if 'down' in x else 'comments_popular'))\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95400</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.94553</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean_up</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00657</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00713</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00199</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_true_std_com</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "0     RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "1    XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "2    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "3      MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "4       RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "..             ...                                                ...   \n",
       "247  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "248  MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "249    MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "250     RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "251    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "                     metric    value             class label  \n",
       "0          accuracy_mean_up  0.95340    upvote_popular        \n",
       "1          accuracy_mean_up  0.95400    upvote_popular        \n",
       "2          accuracy_mean_up  0.95287    upvote_popular        \n",
       "3          accuracy_mean_up  0.94553    upvote_popular        \n",
       "4          accuracy_mean_up  0.95340    upvote_popular        \n",
       "..                      ...      ...               ...   ...  \n",
       "247  precision_true_std_com  0.00000  comments_popular  True  \n",
       "248  precision_true_std_com  0.00657  comments_popular  True  \n",
       "249  precision_true_std_com  0.00713  comments_popular  True  \n",
       "250  precision_true_std_com  0.00199  comments_popular  True  \n",
       "251  precision_true_std_com  0.00673  comments_popular  True  \n",
       "\n",
       "[252 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df['label'] = melted_df['metric'] .apply(lambda x: 'True' if 'true' in x else ('False' if 'false' in x else ''))\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>accuracy_mean</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>accuracy_mean</td>\n",
       "      <td>0.95400</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>accuracy_mean</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean</td>\n",
       "      <td>0.94553</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>accuracy_mean</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>precision_std</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>precision_std</td>\n",
       "      <td>0.00657</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_std</td>\n",
       "      <td>0.00713</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>precision_std</td>\n",
       "      <td>0.00199</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>precision_std</td>\n",
       "      <td>0.00673</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "0     RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "1    XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "2    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "3      MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "4       RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "..             ...                                                ...   \n",
       "247  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "248  MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "249    MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "250     RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "251    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "            metric    value             class label  \n",
       "0    accuracy_mean  0.95340    upvote_popular        \n",
       "1    accuracy_mean  0.95400    upvote_popular        \n",
       "2    accuracy_mean  0.95287    upvote_popular        \n",
       "3    accuracy_mean  0.94553    upvote_popular        \n",
       "4    accuracy_mean  0.95340    upvote_popular        \n",
       "..             ...      ...               ...   ...  \n",
       "247  precision_std  0.00000  comments_popular  True  \n",
       "248  precision_std  0.00657  comments_popular  True  \n",
       "249  precision_std  0.00713  comments_popular  True  \n",
       "250  precision_std  0.00199  comments_popular  True  \n",
       "251  precision_std  0.00673  comments_popular  True  \n",
       "\n",
       "[252 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df['metric'] =  melted_df['metric'].apply(lambda x: x.replace('_up', ''))\n",
    "melted_df['metric'] =  melted_df['metric'].apply(lambda x: x.replace('_com', ''))\n",
    "melted_df['metric'] =  melted_df['metric'].apply(lambda x: x.replace('_down', ''))\n",
    "melted_df['metric'] =  melted_df['metric'].apply(lambda x: x.replace('_true', ''))\n",
    "melted_df['metric'] =  melted_df['metric'].apply(lambda x: x.replace('_false', ''))\n",
    "\n",
    "melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95400</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.94553</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "0     RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "1    XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "2    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "3      MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "4       RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "..             ...                                                ...   \n",
       "247  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "248  MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "249    MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "250     RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "251    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "                class label metric_name     mean      std  \n",
       "0      upvote_popular          accuracy  0.95340  0.00000  \n",
       "1      upvote_popular          accuracy  0.95400  0.00000  \n",
       "2      upvote_popular          accuracy  0.95287  0.00000  \n",
       "3      upvote_popular          accuracy  0.94553  0.00000  \n",
       "4      upvote_popular          accuracy  0.95340  0.00000  \n",
       "..                ...   ...         ...      ...      ...  \n",
       "247  comments_popular  True   precision  0.00000  0.00000  \n",
       "248  comments_popular  True   precision  0.00000  0.00657  \n",
       "249  comments_popular  True   precision  0.00000  0.00713  \n",
       "250  comments_popular  True   precision  0.00000  0.00199  \n",
       "251  comments_popular  True   precision  0.00000  0.00673  \n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to extract 'metric_name', 'mean', and 'std' from 'metric' and 'value'\n",
    "def extract_metrics(row):\n",
    "    metric = row['metric']\n",
    "    value = row['value']\n",
    "    if 'f1' in metric:\n",
    "        metric_name, mean_std = metric.split('_')[0], metric.split('_')[2]\n",
    "        metric_name = 'f1-score'\n",
    "    else:\n",
    "        \n",
    "        metric_name, mean_std = metric.split('_')[0], metric.split('_')[1]\n",
    "\n",
    "    if 'mean' in mean_std:\n",
    "        std = 0\n",
    "        mean = value\n",
    "    else:\n",
    "        mean = 0\n",
    "        std = value\n",
    "    return pd.Series([metric_name, mean, std])\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "melted_df[['metric_name', 'mean', 'std']] = melted_df.apply(extract_metrics, axis=1)\n",
    "\n",
    "# Dropping the original 'metric' column\n",
    "melted_df.drop(columns=['metric', 'value'], inplace=True)\n",
    "\n",
    "melted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, params, class, label, metric_name, mean, std]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melted_df[(melted_df['model'] == 'XGBoost - TFIDF' )& (melted_df['metric_name'] == 'accuracy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.88947</td>\n",
       "      <td>0.00206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.92918</td>\n",
       "      <td>0.00139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.94981</td>\n",
       "      <td>0.00288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.90945</td>\n",
       "      <td>0.00365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>comments_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.74831</td>\n",
       "      <td>0.00492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.98822</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94954</td>\n",
       "      <td>0.00130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.88496</td>\n",
       "      <td>0.00296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.82487</td>\n",
       "      <td>0.00374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.95452</td>\n",
       "      <td>0.00428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "0    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "1    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "2    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "3    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "4    MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "..             ...                                                ...   \n",
       "121    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "122    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "123    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "124    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "125    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "                class  label metric_name     mean      std  \n",
       "0    comments_popular           accuracy  0.88947  0.00206  \n",
       "1    comments_popular  False    f1-score  0.92918  0.00139  \n",
       "2    comments_popular  False   precision  0.94981  0.00288  \n",
       "3    comments_popular  False      recall  0.90945  0.00365  \n",
       "4    comments_popular   True    f1-score  0.74831  0.00492  \n",
       "..                ...    ...         ...      ...      ...  \n",
       "121    upvote_popular  False   precision  0.98822  0.00110  \n",
       "122    upvote_popular  False      recall  0.94954  0.00130  \n",
       "123    upvote_popular   True    f1-score  0.88496  0.00296  \n",
       "124    upvote_popular   True   precision  0.82487  0.00374  \n",
       "125    upvote_popular   True      recall  0.95452  0.00428  \n",
       "\n",
       "[126 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_rows(group):\n",
    "    merged_row = {}\n",
    "    for col in group.columns:\n",
    "        if col not in ['mean', 'std']:\n",
    "            merged_row[col] = group[col].iloc[0]  # Take the first value since they are the same\n",
    "        else:\n",
    "            merged_row[col] = group[col].sum()  # Sum the values\n",
    "    return pd.Series(merged_row)\n",
    "\n",
    "merged_df = melted_df.groupby(['model', 'params', 'class', 'label', 'metric_name']).apply(merge_rows).reset_index(drop=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>class</th>\n",
       "      <th>label</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95287</td>\n",
       "      <td>0.00169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.96982</td>\n",
       "      <td>0.00109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.99501</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94588</td>\n",
       "      <td>0.00171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.89244</td>\n",
       "      <td>0.00369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.81861</td>\n",
       "      <td>0.00477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>MLP - No Text</td>\n",
       "      <td>{'activation': 'tanh', 'alpha': 0.002993124875...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.98094</td>\n",
       "      <td>0.00424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.94553</td>\n",
       "      <td>0.02055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.96563</td>\n",
       "      <td>0.01170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.98396</td>\n",
       "      <td>0.03042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94871</td>\n",
       "      <td>0.00775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.86659</td>\n",
       "      <td>0.07349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.81932</td>\n",
       "      <td>0.00321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>MLP - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.93278</td>\n",
       "      <td>0.13387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.97018</td>\n",
       "      <td>0.00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.99484</td>\n",
       "      <td>0.00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94671</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.89346</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.82078</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>RF - No Text</td>\n",
       "      <td>{'criterion': 'log_loss', 'max_depth': 7, 'min...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.98027</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95340</td>\n",
       "      <td>0.00020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.97018</td>\n",
       "      <td>0.00012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.99484</td>\n",
       "      <td>0.00026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94671</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.89346</td>\n",
       "      <td>0.00051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.82078</td>\n",
       "      <td>0.00015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>RF - TFIDF</td>\n",
       "      <td>{'max_features_text': 10, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.98027</td>\n",
       "      <td>0.00100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95400</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.97055</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.99562</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94671</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.89498</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.82123</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>XGB - No Text</td>\n",
       "      <td>{'max_depth': 3, 'learning_rate': 0.0097513407...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.98328</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td></td>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.95053</td>\n",
       "      <td>0.00129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.96849</td>\n",
       "      <td>0.00083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.98822</td>\n",
       "      <td>0.00110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>False</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.94954</td>\n",
       "      <td>0.00130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>f1-score</td>\n",
       "      <td>0.88496</td>\n",
       "      <td>0.00296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>precision</td>\n",
       "      <td>0.82487</td>\n",
       "      <td>0.00374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>XGB - TFIDF</td>\n",
       "      <td>{'max_features_text': 50, 'max_features_title'...</td>\n",
       "      <td>upvote_popular</td>\n",
       "      <td>True</td>\n",
       "      <td>recall</td>\n",
       "      <td>0.95452</td>\n",
       "      <td>0.00428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model                                             params  \\\n",
       "14   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "15   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "16   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "17   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "18   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "19   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "20   MLP - No Text  {'activation': 'tanh', 'alpha': 0.002993124875...   \n",
       "35     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "36     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "37     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "38     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "39     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "40     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "41     MLP - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "56    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "57    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "58    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "59    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "60    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "61    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "62    RF - No Text  {'criterion': 'log_loss', 'max_depth': 7, 'min...   \n",
       "77      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "78      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "79      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "80      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "81      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "82      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "83      RF - TFIDF  {'max_features_text': 10, 'max_features_title'...   \n",
       "98   XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "99   XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "100  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "101  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "102  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "103  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "104  XGB - No Text  {'max_depth': 3, 'learning_rate': 0.0097513407...   \n",
       "119    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "120    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "121    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "122    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "123    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "124    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "125    XGB - TFIDF  {'max_features_text': 50, 'max_features_title'...   \n",
       "\n",
       "              class  label metric_name     mean      std  \n",
       "14   upvote_popular           accuracy  0.95287  0.00169  \n",
       "15   upvote_popular  False    f1-score  0.96982  0.00109  \n",
       "16   upvote_popular  False   precision  0.99501  0.00110  \n",
       "17   upvote_popular  False      recall  0.94588  0.00171  \n",
       "18   upvote_popular   True    f1-score  0.89244  0.00369  \n",
       "19   upvote_popular   True   precision  0.81861  0.00477  \n",
       "20   upvote_popular   True      recall  0.98094  0.00424  \n",
       "35   upvote_popular           accuracy  0.94553  0.02055  \n",
       "36   upvote_popular  False    f1-score  0.96563  0.01170  \n",
       "37   upvote_popular  False   precision  0.98396  0.03042  \n",
       "38   upvote_popular  False      recall  0.94871  0.00775  \n",
       "39   upvote_popular   True    f1-score  0.86659  0.07349  \n",
       "40   upvote_popular   True   precision  0.81932  0.00321  \n",
       "41   upvote_popular   True      recall  0.93278  0.13387  \n",
       "56   upvote_popular           accuracy  0.95340  0.00020  \n",
       "57   upvote_popular  False    f1-score  0.97018  0.00012  \n",
       "58   upvote_popular  False   precision  0.99484  0.00026  \n",
       "59   upvote_popular  False      recall  0.94671  0.00000  \n",
       "60   upvote_popular   True    f1-score  0.89346  0.00051  \n",
       "61   upvote_popular   True   precision  0.82078  0.00015  \n",
       "62   upvote_popular   True      recall  0.98027  0.00100  \n",
       "77   upvote_popular           accuracy  0.95340  0.00020  \n",
       "78   upvote_popular  False    f1-score  0.97018  0.00012  \n",
       "79   upvote_popular  False   precision  0.99484  0.00026  \n",
       "80   upvote_popular  False      recall  0.94671  0.00000  \n",
       "81   upvote_popular   True    f1-score  0.89346  0.00051  \n",
       "82   upvote_popular   True   precision  0.82078  0.00015  \n",
       "83   upvote_popular   True      recall  0.98027  0.00100  \n",
       "98   upvote_popular           accuracy  0.95400  0.00000  \n",
       "99   upvote_popular  False    f1-score  0.97055  0.00000  \n",
       "100  upvote_popular  False   precision  0.99562  0.00000  \n",
       "101  upvote_popular  False      recall  0.94671  0.00000  \n",
       "102  upvote_popular   True    f1-score  0.89498  0.00000  \n",
       "103  upvote_popular   True   precision  0.82123  0.00000  \n",
       "104  upvote_popular   True      recall  0.98328  0.00000  \n",
       "119  upvote_popular           accuracy  0.95053  0.00129  \n",
       "120  upvote_popular  False    f1-score  0.96849  0.00083  \n",
       "121  upvote_popular  False   precision  0.98822  0.00110  \n",
       "122  upvote_popular  False      recall  0.94954  0.00130  \n",
       "123  upvote_popular   True    f1-score  0.88496  0.00296  \n",
       "124  upvote_popular   True   precision  0.82487  0.00374  \n",
       "125  upvote_popular   True      recall  0.95452  0.00428  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_up = merged_df[merged_df['class'] == 'upvote_popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = merged_df[merged_df['metric_name'] == 'accuracy']\n",
    "other_metrics_df = merged_df[merged_df['metric_name'] != 'accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLAAAAMgCAYAAAAz4JsCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAB7CAAAewgFu0HU+AAEAAElEQVR4nOzdd3RUdf7/8dekkEqHBEgINTRDSEilKF1pKuiuomBfF8v61dXf9l396lq/u9a1LEoVRMSGShNpKi2hNxMIgUASQkuA9Drz+4MzlwwzQAKTZCY8H+dwTvKZWz4zeXPnc9/3U0wWi8UiAAAAAAAAwEV5NHQFAAAAAAAAgEshgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSzU2tNPPy2TyWT3z8/PT/n5+Q1dPQAAAAAA0MiQwEKtmM1mLViwwOFrpaWl+uqrr+q5RgAAAAAAoLEjgYVaWb16tXJyci76+rx58+qxNgAAAAAA4FpAAgu18sknn1zy9TVr1ujo0aP1VBsAAAAAAHAtIIGFGispKbnsEEGz2axPP/20nmoEAAAAAACuBSSwUGPfffed3STtv/rVr+ThYRtGl+ulZVVVVaWvvvpK999/v3r37q1WrVqpSZMmat++vfr376/HHntM33zzjSoqKi56jCNHjuj555/X8OHD1aFDB/n5+alp06YKDw/X6NGj9frrr+vAgQM2+8yePdtuAvr//d//dXj8C7fr3Lmz3TaXOt6KFSs0ceJEhYSEyMvLy2b/3NxcffHFF/rzn/+sUaNGqU+fPmrfvr18fX3l7++v9u3ba+jQofrrX/+q/fv31+gzlaSkpCQ9+eSTiouLU1BQkJo0aaLWrVvruuuu06RJkzRr1izl5eVJkjZs2GBX96lTp1702MnJyXbb33///TWuGwAAAAAAV8KroSsA9+EoMfX444/rxIkT+umnn4yy7du3KyUlRb17977osZKSknTPPfcoLS3N7rVjx47p2LFj2r59uz744AMdOnTILnFUXl6uP/3pT3r33XdVWVlpd4wDBw7owIED+v7777V7927Nnj275m/USZ566im9/fbbF339008/1RNPPHHR10tKSnTs2DH9+OOPeu211/TnP/9ZL7300kW3z8nJ0QMPPKDvv//e7rW8vDzl5eXpl19+0WeffaZZs2bp/vvv18CBAxUVFaUdO3YY2y5YsEBvvvmm/P397Y7z+eef25Xdd999F60TAAAAAADOQA8s1EheXp6WLVtmUxYcHKwbbrhBt99+u932l5rMffny5br++usdJq9qoqKiQmPHjtVbb73lMHnlCmbMmHHJ5FVtmc1mvfzyy/r3v//t8PXMzEzFxcU5TF5dzuOPP27ze35+vr744guH23755Zc2v3fq1ElDhw6t9TkBAAAAAKgNemChRhYuXGg3lG/ixIny8PDQ7bffrqeeekoWi8V4bf78+XrxxRdlMpls9jl69KjuuOMOh8MCo6KiFBMTIy8vLx04cEDr169XaWmp3XZ//vOftWrVKrvyZs2aGUMJz5w5o02bNungwYNX+pavSlZWllGnG2+8Ua1bt1ZGRoYOHTpkt22rVq3Ut29ftW7dWq1bt1aTJk2Ul5enLVu22CX5XnzxRf32t79Vs2bNjDKLxaLbbrtN2dnZdsfu2LGjbrjhBjVv3lxHjx7Vzz//rNzcXJtt7r77bv3hD3/QmTNnjLIZM2bo3nvvtdlu69atdvWfMmWK3d8YAAAAAABnI4GFGnE0fPBXv/qVJCkkJESJiYnauHGj8VpGRobWr1+vwYMH2+zz8ssvq6CgwKYsICBAn376qW6++Wab8ry8PL355ptq0qSJUXb06FG9++67dnW55ZZbNGfOHLVo0cKmfM2aNdq1a1fN3qSTxcfHa/HixWrbtq1RVlhYaPw8fPhwJScnKyYmxm4eMelcYur3v/+9TU+us2fPas2aNbr11luNsi+//FJbtmyx2/+FF17QX/7yF3l5nf9vXl5erk8++UTBwcFGmb+/vx544AG9+eabRtlPP/2ktLQ0hYeHG2WOemVdmOQCAAAAAKAukMDCZR0+fFjr16+3KWvTpo3N0LFf/epXNgks6VzSq3oCy2Kx2A1Bk6R//etfdskr6VzPpH/+8582Zd9++63Ky8ttyjp37qwFCxbIz8/P7hjDhg3TsGHDLv7m6oinp6c+/fRTm+SVJAUGBho/9+nTR9K5yew3bNigvXv3Kjs7W0VFRUYPtczMTLtjb9u2zSaB5SixdMcdd+gf//iHXXmTJk30wAMP2JU/9thjeuutt2x60c2cOVOvvPKK8fuFf7sBAwaoR48edscCAAAAAMDZSGDhsubPn2+T2JCkCRMmyNPT0/j99ttv1zPPPGOzzeeff6533nlH3t7eks6tGHjs2DGbbXx8fGq1il1SUpJd2b333uswedWQrr/+enXt2vWS21RWVuq1117Tm2++aTes71JOnTpl87ujz+SRRx6p8fEkqXv37rrpppu0fPlyo2zOnDl68cUX5enpqZ07d9oNZ6T3FQAAAACgvjCJOy7L0fDBX//61za/d+rUSbGxsTZlubm5NhO/Hz9+3O44nTp1qlXyydExevXqVeP960tkZORlt5k0aZL+/ve/1yp5JUnFxcU2vzvrM7lwMvecnBzj73dhLy8fHx/deeedtT4HAAAAAABXgh5YuKQdO3Zo7969duXffvutli5dalNWVVVlt928efN0yy231Fn9nMVsNtuV1TaxVN2Fc3FdaMmSJQ6HU9bEhb3hnGXs2LHq3LmzMjIyjLIZM2Zo/PjxdnW95ZZb1LJlyzqpBwAAAAAAFyKBhUty1PtKkt57770a7f/dd98pPz9fzZo1U1BQkN3rhw8fVmlpqXx9fWt0PEfHSE1NrdG+Vo4mTC8pKbErO3LkSK2OW93lVub75ptv7MoSExP12muvqV+/fmrevLkk6fvvv9fo0aMveaygoCAdPnzYpiw1NVXt27evVZ09PDz06KOP6k9/+pNRtnjxYq1evVopKSk22zJ8EAAAAABQnxhCiIsym8369NNPr+oYpaWl+uqrrySdGy7Yrl07m9fLyso0e/bsGh8vISHBruzjjz9WaWlpjY9RfSJ1q+zsbLuyBQsW1PiYteUoOfbuu+/qhhtuMJJXkuP5rS7k6DOZNm3aFdXroYceskkmVlZW6r777rPZJigo6LJJNQAAAAAAnIkEFi5q7dq1DhM7tTVv3jxJ53ol3X777Xav/7//9/+0ePFiu/L8/Hy98MILNnW45ZZb1KRJE5vtMjIyNGnSJJ09e9buGBs3btQ777xjU9alSxe77ZYvX66DBw8av+/atUv//e9/L/POrtyF70GSdu7cafP7jz/+qNdee+2yx7pwPjJJ+uyzz/Tiiy/aDeusqqrS/PnzbeYmq65169aaNGmSTVlWVpbN75MnT5aXF503AQAAAAD1hwQWLsrR8MF//OMfslgsF/1XUFBgNxxwzZo1Onr0qCTpL3/5i10PqKKiIt18883q37+/Hn74YT366KMaPXq0OnTooOeee04VFRXGtiEhIXaTjUvnhuSFhYXptttu0+9+9zvde++96t27twYOHKht27bZbBsREaFWrVrZlJ0+fVoDBgzQb3/7W/36179WQkKC8vPza/eB1UJMTIxd2W9/+1uNGzdOjzzyiIYPH65hw4bZTdjuyO23367+/fvblf/jH/9Qly5ddM899+h3v/udfvWrXyk0NFSTJ092OPG7laPPtzqGDwIAAAAA6hvdKOBQWVmZw0nGHfWgqi4wMFCjRo3Sd999Z5RZhyI+88wzCgkJ0YIFCzRhwgRVVlba7Lt9+3Zt3779snV79dVXtW3bNv3444825fn5+fr6668vu7+3t7emTp2qV155xab8xIkT+uijj4zf27Rpo1OnTl32eFfioYce0muvvWYz91ZVVZXdxPg33XSTvv/++0sey2Qy6auvvtKAAQOUk5Nj81pmZqbRA66mYmNjFR8fr+TkZLvXIiMjFRUVVavjAQAAAABwteiBBYcWL15sNySvW7du6tev32X3nThxol1Z9d5c48aN048//qhu3bpdUd2aNGmi77//Xk888YQ8PT2v6Bj/+Mc/NGDAgIu+3qVLF/30009XdOyaCA0N1bx58+Tj43PRbR599FH9+c9/rtHxOnXqpM2bN2vkyJFOqd/FemFdOB8WAAAAAAD1gQQWHHI0fPByva+sbr75ZrvE0vbt221Wshs4cKD27dunzz//XPfcc4969uyp5s2by8vLS8HBwYqOjtYjjzyir7/+WiEhIXbn8PHx0TvvvKMDBw7o2Wef1ZAhQ9SuXTv5+PgoICBA3bp100033aR//etf+vvf/263v5+fn1atWqVXXnlFkZGR8vPzU2BgoPr3769XX31Ve/fuVe/evWv0fq/Ubbfdpi1btmjy5Mlq3769vL29FRwcrDFjxuibb77R+++/X6vjhYSE6IcfftCGDRv0xBNPqH///mrTpo28vb3VsmVL9e7dW3fccYdmzJihW2655ZLHuvPOO9WmTRubMk9PT02ePLnW7xMAAAAAgKtlslgsloauBADXExsbq61btxq/jx07VkuWLGnAGgEAAAAArlX0wAJgZ+XKlTbJK0l64IEHGqg2AAAAAIBrHT2wAEiSnnrqKZnNZmVlZWnp0qUqKyszXuvYsaMOHjwoLy/WfQAAAAAA1D8SWAAknVvN8GLmzJmje++9tx5rAwAAAADAeQwhBHBJDzzwAMkrAAAAAECDYjwQADsBAQGKjIzUww8/rPvvv7+hqwMAAAAAuMaRwAIgSWI0MQAAAADAVTGEEAAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKV5NXQF4HqmTZumwsLChq4GAAAAAABoxAIDAzV16tQabUsCC3YKCwtVUFDQ0NVANQEBAZo8ebIk6ZNPPlFRUVED1wiwR5zCHRCncAfEKdwBcQp3QJw2LiSwcFEmk0mBgYENXQ1I8vPzM34OCAiQhwejf+F6iFO4A+IU7oA4hTsgTuEOiFPXVVhYKIvFUqt9TJba7oF6V1hYqP379xv/9u3bZ9dDKiIiQi+//LJTzvf666+roKBATZs21TPPPOOUY+LqVFVV6fjx45Kk4OBgeXp6NnCNAHvEKdwBcQp3QJzCHRCncAfEqeu6krwDPbDcwFNPPaUTJ040dDUAAAAAAAAaBP3nAAAAAAAA4NJIYLkZT09PdezYsaGrAQAAAAAAUG8YQugG4uLiFBQUpJ49e6pbt246c+aMHn744YauFgAAAAAAQL0ggeUGpk6d2tBVAAAAAAAAaDAMIQQAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDRWIYRDAQEB8vPzU1VVVUNXBZLN34G/CVwVcQp3QJzCHRCncAfEKdwBceq6/Pz8VFBQUKt9SGBdo+bNm6f58+c7fK2goEB33XWXJkyYoOPHj9dzzXA5p06daugqAJdFnMIdEKdwB8Qp3AFxCndAnLqWCRMm6MMPP6zVPiSwrlFFRUU6ceLERV8vLi6ux9oAAAAAAABcHAmsa1RAQICCgoIcvlZQUKDU1FQtWrRIU6dOreeawZGqqirjiUGbNm3k6enZwDUC7BGncAfEKdwBcQp3QJzCHRCnrmvatGm13ocE1jVqypQpmjJlisPXXn/9dRUUFKikpIT/4C7I09OTvwtcHnEKd0Ccwh0Qp3AHxCncAXHqWkpKSmq9D6sQAgAAAAAAwKXRA8sNLF68WDk5OcbvjuanysnJ0UcffWRTNmTIEPXo0aPO6wcAAAAAAFCXSGC5gQ0bNmjPnj2X3CY3N1ffffedTVnXrl1JYAEAAAAAALfHEEIAAAAAAAC4NBJYAAAAAAAAcGkMIXQDL7/8ckNXAQAAAAAAoMGQwAIAuLX169c3dBWuyKBBgxq6CgAAAIDbIIEFAHBrgwcPbugqXBGLxdLQVQAAAADcBnNgAQAAAAAAwKXRAwsA4NbWrVvn9GPu2rVLjz32mCTp/fffV2RkpNPPAQAAAKDmSGABuCjmFoI7qOu/d2RkJDGFq/bBBx80dBWuyKOPPtrQVUA9Ik7hDohTuAPuo+oGCSwAF8XcQgDgHNYefe6GG65rC3EKd0Ccwh1wH1U3SGABsHP/pAMNXYWrUr3+sxd0b8CaALjWcT2FOyBO4Q6IUwAksAAAAOpYYv/nG7oKwGURp3AHxClcGYnWukUCC8BFjRm+oKGrAFxUXTYQTpzMMn5+6dksBbWt28aIKzYQ4Fw9w+9u6CoAl0Wcwh0Qp3AH3EfVDRJYAC4qqG1MQ1cBAAAAANwK91F1w6OhKwAAAAAAAABcCgksAAAAAAAAuDQSWAAAAAAAAHBpzIEFAHBrJ05udfoxT5/Z5/BnZ2JuBAAAAKDmSGABANzastWT6vT4m7Y9VyfHve/OtDo5LgAAANAYMYQQAAAAAAAALo0eWAAAtzZm+IKGrgIAAACAOkYCCwDg1phLCgAAAGj8GEIIAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGnNgAUANrV+/vqGrcEUGDRrU0FUAAAAAgKtCAgsAamjw4MENXYUrYrFYGroKAAC4nQ8++KChq3BFHn300YauAuoRD1hxLSGBBZfAhRcAnIPrKdwBcQp38NhjjzV0Fa4ICaxrCw9YcS0hgQWXwIUX7mDdunVOP+auXbuMBvL777+vyMhIp58D1xaup3AHxClc2YH7JzV0Fa5K9fp3n72gAWsCAM5FAgsAaqiun7xHRkbydB8AABfxfGL/hq4CcFEkWnEtIoGFBsWFFwCcg+sp3AFxCndyd8/whq4CcFkLxgxv6CoA9YYEFlwCF164srq84co6cfL8zy89qwNBbevsXBI3XNcCrqdwB8QpADhHTB23HQFXQgILLoELLwA4B9dTuAPiFAAA1BYJLDgUHx+vAQMGqKKioqGr4jbq+rNq1aqVJMlsNstsNtfpuRoTYtgWceqaiFNbxKlrIk5tEaeuiTi1RZy6JuLUFnHqmur67/Lggw/q7bffrtU+JLDgkI+PjwIDAxu6GqjG09Ozoatwzdtabbifs+w7fcbhz85Unz0diFO4A+IU7oA4hTsgTuEOiFPXdCX5BhJYcKisrEyFhYUksVxIVVWVJC7ADWnSstV1evznNm2rk+Om3XdnnRzXEeIU7oA4hTsgTuEOiFO4A+LUNRUWFtZ6HxJYcCg5OVkpKSl65plnGroqbsPb27vOjl1VVaWTJ8/1/gkODubiWwt1+XdxR8SpayJObRGnrok4tUWcuibi1BZx6pqIU1vEqWuq6zidOXNmrfchgQUANcSqWQAAAADQMEhgAUANsWoWAAAAADQMj4auAAAAAAAAAHApJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpXg1dAQAAAAAAcG1bv369049ZVVWl06dPS5JatmwpT09Pp59j0KBBTj8mHCOBBQAAAAAAGtTgwYMbugpXxGKxNHQVrhkksAAAgEviSSzcAXEKAED9IIEFAABcEk9i4Q6IU7gDEq1wB++//77Tj5mWlqY333xTkvT73/9e4eHhTj8H6s81kcAqKyvTqlWrtGnTJmVlZens2bPy9fVVmzZtFB0drZEjRyo0NPSqznH8+HE9/PDDV7z/Rx99pODgYJuyt956S6tXr67VcR599FGNGTPmiusBAACAy5t04P6GrsJVqV5/X/294SqCekGiFa7Mej367LE5dXoeayLL2X4clWT8zPW0bjX6BNbu3bv11ltv6eTJkzblFRUVKigo0KFDh/Ttt9/qtttu0+TJk2UymRqopmgseMIFd0CcwpWRGAAA5+B6CqAxadQJrF27dumFF15QeXn5JberrKzUwoULVVhYqEceeaSeamerLm700DB4wgV3QJwCgHMMX0DPdwBwBq6nuJxGm8AqLi7WG2+8YZe8CgkJUUREhE6ePKkdO3bIbDYbry1dulTR0dFKSEio9fn8/f118803X3a7AwcOKCUlxaase/fuatOmzWX37dGjh3r27HnJbTp37nzZ4wAA4A5oyMIdtI0JaugqAJfF9RTugOspLqfRJrAWLVqkvLw8m7J+/frpueeek5fXube9cuVKvfPOOzbbzJgxQ/Hx8bUeSti0adMazYH1hz/8wa6sJokvSerfv7/uvvvuWtUL9W/dunVOP+aOHTv0u9/9TpL07rvvKioqyunnwLXB2hW///OJTj924ZEC7Z+xV5LU46HrFBjW1OnnYCjBtYWGLAA4B9dTAI1Bo0xgWSwWrVq1yq78vvvuM5JXkjRy5EgtWrRIR44cMcqOHTumPXv2qG/fvk6vV1pamvbt22dT1rJlS7cdygNb1hvrk1knnH7sM7mnjZ9n5s5Ti6wlTj9H2wPnGzYkBhq/bc9tqtPjWxNZzhZ+96V7oQIAAABonBplAuvw4cN2k7YHBgaqe/fudtv269fPJoElSZs3b66TBNbixYvtykaPHi1vb+8a7X/8+HEtWbJEubm5ks71+uratat69eolHx8fp9YVV271pGV1evy6SjzcmXZfnRwXAAAAAICr1SgTWOnp6XZlISEhDrft2LGjXdnBgwedXqczZ87o559/tinz8vLS6NGja3yMNWvWaM2aNXblAQEBGjt2rCZNmlTjZBiAaxtzYQAAAABwJ40ygZWTk2NX1qJFC4fbNm/evEb7X63ly5ersrLSpuz6669Xy5Ytr/rYRUVF+vzzz7Vt2zb985//VGBg4FUfE1eOxADcAXNhAAAAAHAnjTKBVVxcbFfm6+vrcFtHQ+8c7X81KisrtXz5crvymk7eXlPp6el644039Oyzzzr1uKgdEgMAAAAAADhXo0xglZWV2ZV5eHg43NbT09OurKSkxKn1Wb9+vd2KiL1793Y4J9eFwsLCNHDgQEVGRiosLEx+fn46efKkduzYofnz5ys/P99m+y1btmj37t11MocXAAAAAABAQ2iUCSxHvaqqqqocbuuo3M/Pz6n1cTR5e016X917771q1aqVXXmHDh3UoUMHxcbG6sknn1RRUZHN6xs2bLjqBFZAQID8/Pwu+rkB7oIYhjsgTuEOiFO4A+IU7oA4hTuo6zj18/NTQUFBrfZplAksf39/u7LS0lKH2zoqd7T/lUpLS9O+fftsytq0aaMBAwZcdl9HyavqgoKCNGLECH377bc25Y4msb/QvHnzNH/+fIevFRQU6K677tKECRN0/Pjxyx4LcGXEMNwBcQp3QJzCHRCncAfEKdxBXcfphAkT9OGHH9Zqn0aZwGrfvr1d2ZkzZxxue/bs2Rrtf6Uc9b4aM2aMw6GLVyI0NNSurCZZzKKiIp04ceKirzt7HjAAAAAAAIAr1SgTWI7mlsrOzna4bVZWll1Zt27dnFKPM2fO6Oeff7Ypa9KkiW666SanHF+S3fBByfEQygsFBAQoKMjxZOMFBQVKTU3VokWLNHXq1Kuu46UcqtOjA1JwcPBVH4M4RV0jTuEOiFO4A+IU7oA4hTtwRpxeyrRp02q9T6NMYIWFhSkoKMimh1FRUZHS0tIUHh5us+2OHTvs9o+NjXVKPZYvX67KykqbsqFDh6pZs2aX3Xf//v1q2bKl2rZte9FtLBaLNm7caFferl27yx5/ypQpmjJlisPXXn/9dRUUFKikpMRpPcWAhkIMwx0Qp3AHxCncAXEKd0Ccwh3UdZxeyeJ5jpfmc3Mmk0nDhw+3K587d65NQmnlypXKzMy02aZdu3aKiIiwKfvrX/+qW265xebf5caDVlZWavny5Xbl48ePr9F7SElJ0aOPPqoPP/zQYS+x4uJivffee9q/f7/da85KwAEAAAAAALiCRtkDSzo3IdiKFSuUl5dnlO3YsUNPPPGEIiIidOrUKW3fvt1uv4ceekgmk+mqz79+/Xqbc0tS37591blz5xofo7y8XIsXL9bixYvVvn17de/eXQEBATp16pT279+v/Px8u33atWunIUOGXG31AQAAAAAAXEajTWD5+/vr6aef1gsvvKDy8nKjPDs7+6LzYY0dO1YJCQlOOf+SJUvsym6++eYrPl5OTo5ycnIuuY2fn5/++Mc/ytvb+4rPAwAAAAAA4Goa5RBCq8jISD377LNq06bNJbfz8vLSHXfc4bQJy9PS0pSammpTFhwcrPj4+Bofo23btvLz86vx9l27dtX//d//OZzAHgAAAAAAwJ012h5YVpGRkfrggw+0cuVKbdq0SZmZmcrPz5efn59at26t6OhojRo1SqGhoU475+LFi+3Kxo4dKw+PmucLBw4cqJiYGG3fvl27d+/WwYMHdezYMRUUFKiyslL+/v5q3bq1evTooUGDBik6OtopQx8BAAAAAABcTaNPYEmSj4+Pxo0bp3Hjxl3R/i+//HKttv/973+v3//+91d0rup8fHyUmJioxMTEqz4WAAAAAACAu2rUQwgBAAAAAADg/khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl0YCCwAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0khgAQAAAAAAwKWRwAIAAAAAAIBLI4EFAAAAAAAAl+ZVFwctLCzUypUrtXHjRh07dkyFhYUaP368Hnjggbo4HQAAAAAAABoxpyawLBaLXn31Vf3rX//S2bNnbV7r1KmTzGazOnXqpKNHj0qSRo0apeXLlzuzCnCS+Ph4DRgwQBUVFQ1dFeCqEMNwB8Qp3AFxCndAnMIdEKdwB3Udpw8++KDefvvtWu3jtCGE5eXlGjNmjP7+97/rzJkzslgsxj/jZB4eevDBB43yVatW6dixY86qApzIx8dHgYGBDV0NAAAAAADQyFxJvsFpCazHHntMK1askMVikclkMv5d6Pbbb5ckmUwmmc1mLVu2zFlVgBOVlZWpsLCwoasBAAAAAAAamSvJNzhlCOG2bds0c+ZMm4SVNZF1ocjISAUFBenkyZOSpLVr1zI3lgtKTk5WSkqKnnnmmYauCnBVvL29G7oKwGURp3AHxCncAXEKd0Ccwh3UdZzOnDmz1vs4pQdW9RNbLBZ16NBBL7300kWTWP369TOGFu7du9cZVQAAAAAAAEAj5ZQE1tq1a2UymWSxWBQQEKBNmzbpL3/5y0W3Dw0NlXQu2XXkyBFnVAEAAAAAAACNlFMSWNnZ2ZLOzWs1evRohYSEXHJ7Pz8/4+f8/HxnVAEAAAAAAACNlFMSWCUlJcbPbdq0uez2J06cMH728nLKNFwAAAAAAABopJySwGrVqpXx8/79+y+5rcVi0ZYtW4y5sarvCwAAAAAAAFzIKQmsHj16yGKxyGKx6Mcff1RycvJFt50xY4YyMjIknRty2KdPH2dUAQAAAAAAAI2UUxJYQ4YMkXQuIWU2m3XjjTfaTeKekpKiqVOn6tFHHzUmfJekoUOHOqMKAAAAAAAAaKScksD6zW9+Y8xlZTKZlJ+fr//7v/+TJKNn1ooVKzR9+nRVVVUZ+3l7e+uBBx5wRhUAAAAAAADQSDklgdWxY0c9/fTTRq+q6j2srKyJLOvcVyaTSX/4wx8UHBzsjCoAAAAAAACgkXJKAkuSXnrpJU2YMMEmieXon3QumTVhwgQ9//zzzjo9AAAAAAAAGimnJbA8PT311Vdf6V//+pdatWpl9Li68F+LFi302muv6YsvvpCHh9NODwAAAAAAgEbKy9kHfOaZZ/TYY49p7dq1SkpK0smTJyVJQUFBiouL09ChQ+Xv7+/s0wIAAAAAAKCRcnoCS5L8/Pw0ZswYjRkzpi4ODwAAAAAAgGsIY/gAAAAAAADg0khgAQAAAAAAwKU5ZQjh8OHDr3hfk8mkVatWOaMaAAAAAAAAaIScksBau3atTCZTrfezWCxXtB8AAAAAAACuHU6dxN1isdR4WxJXAAAAAAAAqAmnJrBISgEAAAAAAMDZnJbAqmnvK2uSqza9tQAAAAAAAHDtckoC69ChQ5d8PTc3V0eOHNGnn36qr7/+WpWVlWrbtq2++eYbtW/f3hlVAAAAAAAAQCPllARWp06dLvt6//79NWHCBK1cuVJjxozRqVOn9MQTTygpKckZVQAAAAAAAEAj5dQ5sGpi5MiRGjNmjBYvXqxt27ZpxowZevjhh+v0nGVlZVq1apU2bdqkrKwsnT17Vr6+vmrTpo2io6M1cuRIhYaGOuVct9xyS633+eKLL9SkSZNLbpOWlqbVq1dr165dysvLU0VFhVq0aKEuXbpowIABGjJkiDw9Pa+02gAAAAAAAC6r3hNYkhQSEmL8PHfu3DpNYO3evVtvvfWWTp48aVNeUVGhgoICHTp0SN9++61uu+02TZ482eUmoq+oqNB///tfrVy50m7esBMnTujEiRNKSkrS119/rf/3//7fZXvDAQAAAAAAuBuP+j5hXl6evv/+e5lMJlksFu3du7fOzrVr1y49//zzdsmrC1VWVmrhwoWaNm1andXlSlRVVemVV17RDz/8cNlJ7w8fPqy///3vyszMrKfaAQAAAAAA1A+n9MD6+OOPL/m62WxWSUmJsrOztXDhQmVkZBg9nUpKSpxRBTvFxcV64403VF5eblMeEhKiiIgInTx5Ujt27JDZbDZeW7p0qaKjo5WQkOC0eowYMUL+/v6X3OZiQ/++/fZbbdmyxabMy8tLsbGxatasmbZt26ZTp04Zr509e1Zvvvmm/v3vf8vDo95zkwAAAAAAAHXCKQms+++/v8ZD7y7sSdS5c2dnVMHOokWLlJeXZ1PWr18/Pffcc/LyOve2V65cqXfeecdmmxkzZig+Pt5pQwknTZqk4ODgWu9XXFysBQsW2JR5eHjoueeeU79+/Yxt/vSnP+nw4cPGNgcOHNCPP/6oYcOGXV3FAQAAAAAAXIRTu+lYLJbL/jOZTMbwQZPJpIkTJzqzCkY9Vq1aZVd+3333Gckr6dyE8mFhYTbbHDt2THv27HF6nWpr3bp1dr3TYmJijOSVJPn7+2vSpEl2+/7www91Xj8AAAAAAID64tRJ3Gvba6lHjx7605/+5MwqSDo3H9SF814FBgaqe/fudtv269dPR44csSnbvHmz+vbt65S67Ny5U/n5+SooKJCPj49atWqlXr16Xbbn2datW+3KoqKi7MqqJ7Ss9u7dq+Li4ssOXQQAAAAAAHAHTktgXW6S8eo8PT11xx136K233lKzZs2cVQVDenq6XVn1lQ+r69ixo13ZwYMHnVaXd99912F5586ddc899yguLs7h6zV9D4GBgWrZsqVOnz5tlFksFh06dEjXXXfdFdYaAAAAAADAdTglgfXcc89ddhsPDw8FBgaqS5cuGjRokNq2beuMUzuUk5NjV9aiRQuH2zZv3rxG+ztbRkaG/vnPf+ruu++2GwZYVVWlEydO2O1zqfdQPYElnXsPJLAAAAAAAEBjUG8JrPpUXFxsV+br6+twWx8fnxrtX1fmz5+v0NBQDR48+LLnr817KCoqck4FAQAAAAAAGphT58ByFWVlZXZlHh6O56v39PS0K7tw8vTa8PT0VExMjOLj49WrVy8FBQXJbDbr6NGjWrJkiVavXm033PLjjz/WwIEDjTo6qn9t30NpaekVvwcAAAAAAABX0igTWI56JFVVVTnc1lG5n5/fFZ97xowZatWqlV159+7d9eSTT6pz586aMWOGzWvHjh3TwYMHjUnmHdX/YnW9WPnFemvVVEBAgPz8/C56TsBdEMNwB8Qp3AFxCndAnMIdEKdwB3Udp35+fiooKKjVPo0ygeVo9b2L9UhyVH41q/c5Sl5VN378eC1cuNDuD5Wenm4ksC52/tq8h4CAgEvWY968eZo/f77D1woKCnTXXXdpwoQJOn78+CWPA7g6YhjugDiFOyBO4Q6IU7gD4hTuoK7jdMKECfrwww9rtU+tEliOhqpdLZPJpMrKSqces3379nZlZ86ccbjt2bNna7S/s3h6eiokJESpqak25fn5+TbbBAUF2U3k7sz3UFRU5HCieKv6nAcMAAAAAADgUmqVwLpw7iZXZe3JVF12drbDbbOysuzKunXr5vQ6VVdYWGhXduGQv+7du9slmLKyshQTE2N3rAsTWyaTSV26dLlkHQICAhQUFOTwtYKCAqWmpmrRokWaOnXqJY9ztQ7V6dEBKTg4+KqPQZyirhGncAfEKdwBcQp3QJzCHTgjTi9l2rRptd6n1kMITSZTrU9yMXWVEAsLC7PrwVRUVKS0tDSFh4fbbLtjxw67/WNjY6/ovJs2bVJsbKy8vC7+sR45csRhMq1du3Y2v8fExGjDhg02ZTt37tStt95qU+ao/tddd91lh0FOmTJFU6ZMcfja66+/roKCApWUlNRJrzugPhHDcAfEKdwBcQp3QJzCHRCncAd1HadXsnie42Xt3JzJZNLw4cPtyufOnWszXHHlypXKzMy02aZdu3aKiIiwKfvrX/+qW265xeafo/Ggn376qR5//HEtXbrU4R/j4MGDevXVV+0Sd02aNFHfvn1tygYNGmQ3mfy2bdu0c+dO4/fi4mJ99tlnducZNWqUXRkAAAAAAIC7qlUPrLCwMKf2wKpLEyZM0IoVK5SXl2eU7dixQ0888YQiIiJ06tQpbd++3W6/hx566KreY05Ojv773/9q+vTp6tGjh9q3by+TyaSjR48qNTVVZrPZYV0vHELo7++vSZMmadasWUaZ2WzW888/r7i4ODVt2lTbtm3TqVOnbPbr3r27brjhhiuuPwAAAAAAgKupVQIrIyOjjqrhfP7+/nr66af1wgsvqLy83CjPzs6+6HxYY8eOVUJCglPOX1lZqV9++UW//PLLJbeLjIzUnXfe6fC1W265Rbt379aWLVtsjrtx40aH2zdv3ly///3v6ZIKAAAAAAAalUY5hNAqMjJSzz77rNq0aXPJ7by8vHTHHXdc9YTlHTt2rHHvLQ8PD40bN07/+Mc/5O3t7XAbT09P/eUvf9HIkSMve9xOnTrpxRdfVMeOHWtdbwAAAAAAAFdW60nc3U1kZKQ++OADrVy5Ups2bVJmZqby8/Pl5+en1q1bKzo6WqNGjVJoaOhVn+v//b//p3vvvVdbt27V3r17lZmZqVOnTqmkpEQmk0mBgYEKDQ1Vnz59NGLECLuJ2x3x9vbW//zP/2j06NFavXq1du/erby8PJWXl6t58+bq2rWrBgwYoKFDh9LzCgAAAAAANEqNPoElST4+Pho3bpzGjRt3Rfu//PLLNd42KChIY8aM0ZgxY67oXBfTo0cP9ejRw6nHBAAAAAAAcAdOT2AVFhZqzZo1xgTjxcXFdqvuVWcymTRjxgxnVwMAAAAAAACNhFMTWG+88YZefPFFnT17tkbbWywWElgAAAAAAAC4JKclsP7+97/rlVdeuWRvKwAAAAAAAKC2nJLASktL06uvvipJNV6FTxLJLgAAAAAAAFyWUxJYs2fPltlsNpJXl5vz6nLbAAAAAAAAAFYezjjIhg0bJJ1PSj355JO67bbbjNdNJpOWL1+u//3f/5Wfn58kafTo0VqzZo1Wr17tjCoAAAAAAACgkXJKAmvfvn1Gz6rw8HC9+eab6tixo802N954o5599lktWbJEkvT999/riy++0JAhQ5xRBQAAAAAAADRSTklgWVcdNJlMio2NveS2Q4YM0dChQ2WxWPT+++8bCS0AAAAAAADAEacksMxms/Fzq1atJEne3t422xQUFBg/9+jRw/j5gw8+cEYVAAAAAAAA0Eg5JYHVsmVL4+fy8nJJUkBAgM02O3fuNH7OzMyUdG7OrG3btjmjCgAAAAAAAGiknJbAsk7gnpubK0kKCQmRdH7VwT/96U/avn27Zs+ere+//94oz8vLc0YVAAAAAAAA0Eh5OeMgISEhSklJkXQ+gXXdddcZr1ssFm3atMmYH8tisRgJrDZt2jijCgAAAAAAAGiknNIDKyYmxvh59+7dkqSEhAS1bt1a0rleWBaLxfhn/d1kMmn48OHOqAIAAAAAAAAaKacksKr3rMrLy9PWrVvl4eGhp556yhhaaDKZjH9WPj4++vOf/+yMKgAAAAAAAKCRqvUQwtTUVPXq1cumbNiwYfrPf/5j/G6dwP3Pf/6ztm/frq+++sruOH5+fpo3b5769OlT2yoAAAAAAADgGlLrBFafPn0UExOjyZMna9KkSWrXrp1atWqlxx9/3G5bT09PffHFF1q6dKm++eYbHTlyRE2aNFFMTIweeughY6J3AAAAAAAA4GKuaBL3bdu2adu2bfrDH/6gYcOG6Z577tHEiRMVGBjocPuxY8dq7NixV1VRAAAAAAAAXJuueA4si8WiqqoqrVq1Svfff7+Cg4M1adIkLV68WFVVVc6sIwAAAAAAAK5hV5zAsk7Ibl1ZsKSkRJ9//rluvfVWtWvXTr/73e+0YcMGZ9YVAAAAAAAA16BaJ7A8PDyMpJVku7qgtTw3N1cffPCBrr/+enXr1k3PPvusUlNTnV55AAAAAAAANH61TmAdPXpUb7/9tgYOHChJl01mHTp0SC+99JKuu+46xcbG6q233tKxY8ec+y4AAAAAAADQaNU6gRUUFKQnnnhC69atU0ZGhl599VVFR0fbJLIkx8msbdu26ZlnnlHHjh114403au7cuSosLHTqGwIAAAAAAEDjcsVzYElSx44d9cc//lFbt27Vvn379L//+7/q1avXRZNZkv3k7+3atbu6dwAAAAAAAIBG7aoSWNWFh4fr2Wef1d69e7Vjxw796U9/UufOnY1klsVisemVJcmY/B0AAAAAAAC4GKclsKqLjIzUK6+8ovT0dG3YsEH/8z//ow4dOtj0ygIAAAAAAABqwquuT5CYmKiePXuqd+/eeu6553Ty5Mm6PiUAAAAAAAAakTpLYBUXF2vRokX69NNP9cMPP6iiokKSbFYsBAAAAAAAAC7HqQms8vJyLV26VAsWLNDixYuN+a0unNAdAAAAAAAAqKmrTmCZzWatWrVKn376qb7++mvl5+dLunTSyvpa+/btNWnSpKutAgAAAAAAABqxK05grV+/Xp9++qm++OILY16rmiStmjVrpttuu02TJ0/W8OHD6ZHlouLj4zVgwABj6CfgrohhuAPiFO6AOIU7IE7hDohTuIO6jtMHH3xQb7/9dq32qXUC649//KMWLlyozMxMSTVLWnl7e2vMmDGaPHmybr75Zvn6+tb2tKhnPj4+CgwMbOhqAAAAAACARuZK8g21TmD9+9//lslkuuRk7BaLRSaTSYMHD9bkyZN1xx13qGXLlrWuHBpOWVmZCgsLSWIBAAAAAACnKiwsrPU+VzyE8GK9rfr06aMpU6bo7rvvVlhY2JUeHg0sOTlZKSkpeuaZZxq6KsBV8fb2bugqAJdFnMIdEKdwB8Qp3AFxCndQ13E6c+bMWu9zVZO4W5NWISEhuuuuuzR58mT169fvag4JAAAAAAAA2LiiBJbFYlHz5s11++23a/LkyRo6dCiTsQMAAAAAAKBO1DqBNXHiRE2ePFnjx49XkyZN6qJOAAAAAAAAgKHWCawvv/yyLuoBAAAAAAAAOOTR0BUAAAAAAAAALoUEFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NK+GrkB9KCsr06pVq7Rp0yZlZWXp7Nmz8vX1VZs2bRQdHa2RI0cqNDTUKefKysrSrl27lJaWpszMTJ04cULFxcUym83y9/dXhw4d1KdPnxqd86233tLq1atrdf5HH31UY8aMuZq3AAAAAAAA4FIafQJr9+7deuutt3Ty5Emb8oqKChUUFOjQoUP69ttvddttt2ny5MkymUxXfK7LJZzy8/OVn5+v1NRULVq0SOPHj9cDDzwgT0/PKz4nAAAAAABAY9eohxDu2rVLzz//vF3y6kKVlZVauHChpk2bdlXnKyoqqvG2ZrNZ3377rf7zn/9c1TkBAAAAAAAau0bbA6u4uFhvvPGGysvLbcpDQkIUERGhkydPaseOHTKbzcZrS5cuVXR0tBISEq76/CaTSd26dVPHjh1lMpmUnp6uw4cP2223evVqDRs2TP369bvsMXv06KGePXtecpvOnTtfaZUBAAAAAABcUqNNYC1atEh5eXk2Zf369dNzzz0nL69zb3vlypV65513bLaZMWOG4uPjr3goob+/v8aMGaPx48erdevWRrnFYtGSJUv04Ycf2u2zdu3aGiWw+vfvr7vvvvuK6gUAAAAAAOCuGuUQQovFolWrVtmV33fffUbySpJGjhypsLAwm22OHTumPXv2XNF5Bw4cqA8++ED33XefTfJKOtcja/z48YqPj7fbLyMj44rOBwAAAAAAcC1olD2wDh8+bDfvVWBgoLp37263bb9+/XTkyBGbss2bN6tv3761Pu+wYcMuu01ERISSk5Ntyi4c5ngxx48f15IlS5SbmytJatq0qbp27apevXrJx8en1vUFAAAAAABwB40ygZWenm5XFhIS4nDbjh072pUdPHjQ6XWyqj7nllXbtm1rtO+aNWu0Zs0au/KAgACNHTtWkyZNkre391XXEQAAAAAAwJU0yiGEOTk5dmUtWrRwuG3z5s1rtL+zJCUl2ZX179//qo5ZVFSkzz//XH/84x9VWFh4VccCAAAAAABwNY0ygVVcXGxX5uvr63BbR0PvHO3vDKtWrVJKSopNWdOmTTV8+HCnHD89PV1vvPGGU44FAAAAAADgKhrlEMKysjK7Mg8Px7k6T09Pu7KSkhKn12nr1q167733bMpMJpN+97vfKTAw8KL7hYWFaeDAgYqMjFRYWJj8/Px08uRJ7dixQ/Pnz1d+fr7N9lu2bNHu3buvaA4vAAAAAAAAV9QoE1iOelVVVVU53NZRuZ+fn1Prs27dOr3xxhuqrKy0KX/ggQc0YMCAi+537733qlWrVnblHTp0UIcOHRQbG6snn3xSRUVFNq9v2LDhqhNYAQEB8vPzu+jnBrgLYhjugDiFOyBO4Q6IU7gD4hTuoK7j1M/PTwUFBbXap1EmsPz9/e3KSktLHW7rqNzR/ldq2bJlmjZtmt3k7ffcc48mTJhwyX0dJa+qCwoK0ogRI/Ttt9/alDuaxP5C8+bN0/z58x2+VlBQoLvuuksTJkzQ8ePHL3sswJURw3AHxCncAXEKd0Ccwh0Qp3AHdR2nEyZM0IcfflirfRplAqt9+/Z2ZWfOnHG47dmzZ2u0/5VYuHCh5s2bZ1NmMpn08MMPa/z48U45R2hoqF1ZTbKYRUVFOnHixEVfr6t5wAAAAAAAAGqrUSawunfvbleWnZ3tcNusrCy7sm7dul3V+S0Wi2bMmGHXM8rLy0u///3vdf3111/V8au7cPig5HgI5YUCAgIUFBTk8LWCggKlpqZq0aJFmjp16lXX8VIO1enRASk4OPiqj0Gcoq4Rp3AHxCncAXEKd0Ccwh04I04vZdq0abXep1EmsMLCwhQUFGTTw6ioqEhpaWkKDw+32XbHjh12+8fGxl7xuauqqvTOO+9ozZo1NuV+fn76y1/+oqioqBodZ//+/WrZsqXatm170W0sFos2btxoV96uXbvLHn/KlCmaMmWKw9def/11FRQUqKSkxOEk94A7IYbhDohTuAPiFO6AOIU7IE7hDuo6Tq9k8TzHS/O5OZPJpOHDh9uVz50712Yi9ZUrVyozM9Nmm3bt2ikiIsKm7K9//atuueUWm3+OxoOWlZXplVdesUteNW/eXC+//HKNk1eSlJKSokcffVQffvihw15ixcXFeu+997R//367164mAQcAAAAAAOBqGmUPLOnchGArVqxQXl6eUbZjxw498cQTioiI0KlTp7R9+3a7/R566CGZTKYrOue7776r5ORku/IePXpo9erVWr169UX3ffjhh+3KysvLtXjxYi1evFjt27dX9+7dFRAQoFOnTmn//v3Kz8+326ddu3YaMmTIFdUfAAAAAADAFTXaBJa/v7+efvppvfDCCyovLzfKs7OzLzof1tixY5WQkHDF58zNzXVYvnnz5svu6yiBVV1OTo5ycnIuuY2fn5/++Mc/ytvb+7LnAwAAAAAAcBeNcgihVWRkpJ599lm1adPmktt5eXnpjjvuqPMJy2ujbdu28vPzq/H2Xbt21f/93/85nMAeAAAAAADAnTXaHlhWkZGR+uCDD7Ry5Upt2rRJmZmZys/Pl5+fn1q3bq3o6GiNGjVKoaGhDV1VGwMHDlRMTIy2b9+u3bt36+DBgzp27JgKCgpUWVkpf39/tW7dWj169NCgQYMUHR19xUMfAQAAAAAAXFmjT2BJko+Pj8aNG6dx48Zd0f4vv/yyU7erKR8fHyUmJioxMdGpxwUAAAAAAHAnjXoIIQAAAAAAANwfCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApXk1dAXgmuLj4zVgwABVVFQ0dFWAq0IMwx0Qp3AHxCncAXEKd0Ccwh3UdZw++OCDevvtt2u1Dz2w4JCPj48CAwMbuhoAAAAAAKCRuZJ8AwksOFRWVqbCwsKGrgYAAAAAAGhkriTfwBBCOJScnKyUlBQ988wzDV0V4Kp4e3s3dBWAyyJO4Q6IU7gD4hTugDiFO6jrOJ05c2at96EHFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEsjgQUAAAAAAACXRgILAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAAAAAAODSSGABAAAAAADApZHAAgAAAAAAgEvzaugKoObKysq0atUqbdq0SVlZWTp79qx8fX3Vpk0bRUdHa+TIkQoNDW3oagIAAAAAADgVCSw3sXv3br311ls6efKkTXlFRYUKCgp06NAhffvtt7rttts0efJkmUymBqopAAAAAACAc5HAcgO7du3SCy+8oPLy8ktuV1lZqYULF6qwsFCPPPJIPdUOAAAAAACgbjEHlosrLi7WG2+8YZe8CgkJ0U033aT+/fvLw8P2z7h06VIlJSXVZzUBAAAAAADqDD2wXNyiRYuUl5dnU9avXz8999xz8vI69+dbuXKl3nnnHZttZsyYofj4eIYSAgAAAAAAt0cPLBdmsVi0atUqu/L77rvPSF5J0siRIxUWFmazzbFjx7Rnz546ryMAAAAAAEBdI4Hlwg4fPmw3aXtgYKC6d+9ut22/fv3syjZv3lxndQMAAAAAAKgvJLBcWHp6ul1ZSEiIw207duxoV3bw4EGn1wkAAAAAAKC+kcByYTk5OXZlLVq0cLht8+bNa7Q/AAAAAACAuyGB5cKKi4vtynx9fR1u6+PjU6P9AQAAAAAA3A0JLBdWVlZmV+bh4fhP5unpaVdWUlLi9DoBAAAAAADUN6/Lb4KG4qhXVVVVlcNtHZX7+fld8bkDAgLk5+d30fMB7oIYhjsgTuEOiFO4A+IU7oA4hTuo6zj18/NTQUFBrfYhgeXC/P397cpKS0sdbuuo3NH+VvPmzdP8+fMdvlZQUKC77rpLEyZM0PHjx2tYW8A1EcNwB8Qp3AFxCndAnMIdEKdwB3UdpxMmTNCHH35Yq31IYLmw9u3b25WdOXPG4bZnz56t0f5WRUVFOnHixEVfr6/5s/xffqNezlMfGs87USN7M1ePOHVRjerNXD3i1EU1qjdz9YhTF9Wo3szVI05dVKN6M1ePOHVRjerNuB4SWC6se/fudmXZ2dkOt83KyrIr69at20WPHRAQoKCgIIevFRQUKDU1VYsWLdLUqVNrWFvUpaqqKp06dUqS1KZNG4dzngENjTiFOyBO4Q6IU7gD4hTugDh1XdOmTav1PiSwXFhYWJiCgoJsekoVFRUpLS1N4eHhNtvu2LHDbv/Y2NiLHnvKlCmaMmWKw9def/11FRQUqKSkhP/gLsjT05O/C1wecQp3QJzCHRCncAfEKdwBceparmTROVYhdGEmk0nDhw+3K587d64qKyuN31euXKnMzEybbdq1a6eIiIg6ryMAAAAAAEBdoweWi5swYYJWrFihvLw8o2zHjh164oknFBERoVOnTmn79u12+z300EMymUz1WVUAAAAAAIA6QQ8sF+fv76+nn35aTZo0sSnPzs7W999/r61bt8psNtu8NnbsWCUkJNRnNQEAAAAAAOoMCSw3EBkZqWeffVZt2rS55HZeXl664447mHgdAAAAAAA0KgwhdBORkZH64IMPtHLlSm3atEmZmZnKz8+Xn5+fWrdurejoaI0aNUqhoaENXVUAAAAAAACnIoHlRnx8fDRu3DiNGzeuoasCAAAAAABQbxhCCAAAAAAAAJdGAgsAAAAAAAAujQQWAAAAAAAAXBoJLAAAAAAAALg0ElgAAAAAAABwaSSwAAAAAAAA4NK8GroCcF2FhYV6/fXXG7oakOTn56cJEyZIkqZNm6aSkpKGrRDgAHEKd0Ccwh0Qp3AHxCncAXHqugoLC2u9j8lisVjqoC5wY6+//roKCgoauhq4wMGDB1VRUSFvb2917dq1oasDOEScwh0Qp3AHxCncAXEKd0CcuramTZvqmWeeqdG29MCCncDAwIauAhw4dOiQSkpK5Ofnp379+jV0dQCHiFO4A+IU7oA4hTsgTuEOiFPXVpv8Az2wADcxduxYnThxQkFBQVq6dGlDVwdwiDiFOyBO4Q6IU7gD4hTugDhtPJjEHQAAAAAAAC6NBBYAAAAAAABcGgksAAAAAAAAuDQSWAAAAAAAAHBpJLAAAAAAAADg0rwaugIAaubuu+9WUVGRAgICGroqwEURp3AHxCncAXEKd0Ccwh0Qp42HyWKxWBq6EgAAAAAAAMDFMIQQAAAAAAAALo0EFgAAAAAAAFwaCSwAAAAAAAC4NBJYAAAAAAAAcGkksAAAqCdms7mhqwAAAAC4JRJYAADUsbKyMs2YMUNz585t6KoAgMvKyspq6CoAwDWlrKxMn3/+udatW9fQVakRr4auAADUB4vFIkkymUwNXBNca5YvX645c+aouLhYrVq10g033KAuXbo0dLUAG2azWR4ePNdEw9i6das++ugjtWnTRo8//rjat2/f0FUCgEZv+fLlmjVrlkpLS3X99derd+/eat26dUNX65JIYAFo9KqqquTp6Snp3FMGHx8fm7LqPwPOsnv3bn300Uc6fPiwUZaXl6fly5fr0UcfbcCaAedZk/vW5FVaWpo6deqkJk2ayGKxyGw2c31EnTl69KimT5+urVu3SpJKSkq0adMmTZw4sYFrBtQ9R21Ri8ViPGzlwQLqiqM26q5du7R9+3aNHDmyAWt2eSSwADRa1i9+T09PVVVV6ZNPPlFaWpr+9re/ydfXVxUVFfL29panpyeNBDjNyZMnNWPGDG3cuFHSuV5/FotFPj4+6tatmzp27GgkUoGGVP1Gac+ePXr33XdVWVmpRx99VDExMTbJq8OHD6tJkyb0jIFTVFRUaO7cufrmm28knb9OnjlzRtnZ2Tp16pTatGnTwLUE6saF7dPPPvtMXl5euu222+Tl5WW0T2mXwtku1UZt166d/Pz8VFlZKS8v100TuW7NAOAqWb/4V61apTlz5ujs2bPy9fXVypUrNX78eHl7exuvf/fdd7rnnnsUExPTkFWGGzObzfrkk0/0xRdfSDrfKJCkTp06KTo6WgMGDFCvXr0aspqAwWQy6cyZM3r77be1bds2SZKnp6c2b96sXr16KSAgQMePH9dHH32kzZs366677tKvf/1remThqqxYsUKzZ89WUVGRUWaxWBQcHKzrrrtOgwYNUqtWrRqwhkDdqt4+/fjjj3XmzBmFh4erd+/e6tu3r9E+/eGHH7Rv3z5NmDBBoaGhDVlluLnG1EYlgQWgUZs7d65xsfb09FR5ebnWrl2rkSNHKicnR++++64OHDgg6VxDITo6mideqLU1a9Zo1qxZOnv2rFFmsVjUtm1bRUREaMCAAYqOjlaTJk0kMWwVrsFsNis5OdlIXvn4+KisrEw7d+5Ujx49dPToUX3++efG9tu2bVPfvn113XXXNVSV4cb27Nmjjz76SBkZGZLO30A1a9ZMvXr1UmJiouLi4tSsWbOGrShQx0pLSzVt2jStXr1a0rn/C4cOHTKusWlpaZo2bZrS0tLk6+urzp07k8DCFattG9XVR6WQwALQqA0cOFCLFy9WaWmppHMX5YyMDD3zzDPGakfWRvT+/fu1fft2emGhxvbv36///ve/Sk9PlyR5eXmpqqpKAQEB6tmzpxISEhQfH6+WLVtKOpe4kkTyCi7Bw8NDffv21dChQ7V27VpVVFTIZDLp6NGjev/991VRUSHJdnhXXl4eCVjUysmTJzVr1iytX79e0rm4M5vN8vb2Vrdu3RQXF6cBAwaoQ4cOksTca2j0TCaTOnbsKG9vb1VUVMjDw0OVlZXauHGjUlJSlJKSIunc/5XS0lJt3bpV/fr1U8eOHRu45nAn+/fv17Rp04wH9dYhq4GBgW7dRiWBBaBR69atm26++WabXgQVFRV2S3WHhoaqf//+DFtAjZWWluqbb75Renq6vL29ZTabVVVVJYvFopiYGD300ENq3ry5JPsbsrKyMh06dEjBwcFGwwFoCEFBQRo6dKj27t2rkydPymQyyWQyGcksi8WigIAA9e7dWzExMerXr5/LN27hOlauXKn//Oc/ks7dtFuTV0FBQRoyZIiio6NtevRZk6PEGBozHx8fxcbGKjU1VUlJSUZ5Tk6OcnJyjN8tFos6dOigsLAw+fr6NkRV4aYyMzP1n//8R0eOHJGXl5csFouqqqrk7e2tQYMG6c477zRWG3TURj169Ki6dOliM1emqyCBBaDRsl5077jjDv3www86c+aM3TatW7dW3759lZCQoP79+9NAQI35+vrq5ptvVmpqqk6dOmVzw5Wammokr6ovFmB9bf369dq2bZvGjBmjUaNGMaE7Goynp6dat24tX19fm0aq9eewsDANGzZMsbGxCgsLa6hqwg2ZzWadOnVKvr6+Ki0tlYeHh6qqqmQymdSyZUvdfvvt8vPzk3QucWWd1Fo6d/P1888/KzExUV27dnXJmyjgSlhjOSwsTKNGjdLu3btVXFwsyXZeotatWysiIkKJiYm0T1Fr7dq10w033KBFixapsLDQGBJoNptVVlZmJK8u1kbdunWrnnnmGXXr1q3B3sPFuO7gRgC4AtYnDNK5hkBmZqb+9re/6cyZM0bPAisvLy9FR0frscce08CBA+Xr62s0HICa6NKli7HcsNlslnQuIXD8+HF9+eWXkmRMxnr06FF9+eWXmj17tn744QdlZWVp5cqVxnwwQH2wXh+t1qxZo6eeekqZmZlGmcViMW6yunTpotGjRyssLMwoBy7HYrHIw8ND119/vTEs32w2GzfoOTk5Sk5OlnTuBsrT01Mmk0mnT5/WihUr9PHHH+uzzz7T+++/L0kkr+D2rO1TayxnZmbqs88+U3FxsdE+tV5fmzVrpqFDh+q3v/0t7VPUmsVikbe3t+Li4oxJ2a3f6VVVVdq5c6fWrVsnyXEbdeXKlcrOztbcuXMb7D1cCgksAI2GtWHg6empkpIS5efna+nSpdq/f7+xTfWnuFVVVTpx4oTxVKJ6wwKoCR8fH11//fVGN+vqk15+8sknqqioUHl5uX744QfNmjVLixYtUkpKijEn26FDh7R//367pALgbFVVVTZDBH7++WcdOnRI0vkGbPUbJOtQryNHjhjzsVz4EAC4GGuchISEKD4+XsHBwTbXyIKCAi1btkwFBQXy9vZWaWmpkpOTNWfOHC1cuFDJyckymUxKS0vTkiVLGvKtAFetevu0rKxMZWVlevXVV5WWlmZsU719WlpaKl9fXwUGBtrsD9SENVY6d+6shIQEtW7d2ub6e+bMGf3www+yWCwqKSnRypUrbdqoJSUlkqTt27dr586dDfY+LoYEFgC3Z73pst6YffPNN5o0aZKOHj2qoUOHqkePHpJkM9bb09NTFotFBw4c0LfffitJLr3iBlxXu3btNHbsWEnnkwTWydz/+c9/atasWcYNWX5+vqRzMdi0aVNNnTpVN998M/O9oM55enrKw8NDGRkZ+sc//qF///vfOnPmjIYOHarIyEhjUlfrSlfWBvChQ4e0ceNG5ebmShK9AGCwxoK19+nFXu/Xr5+io6MlnR8qaLFYdOjQIS1btkyHDx/Wp59+qrlz52rt2rXGXGzW/a03U4C7qd4zWzrXPn3kkUdUUVGhcePGGUNomzZtKul8+7S8vFzJycnavn27zf5ATVmvn9HR0erXr58k2+vv/v379f7772vOnDn67LPP7NqozZo1029/+1tjX1fCHFgA3J71Rmvz5s2aOXOmjh49qsDAQHXo0EEBAQEaMmSIQkJCFBUVpW3btunHH3809ikpKdFPP/2kxMREhYSEuPzSsagfp0+flre3t9LT09WsWTN5eHiobdu28vf3l2T7pNTLy0sxMTGKi4vT5s2bjXleJGnnzp3avXu3zdAZSbrlllt07733Gj1fiDvUtcrKSv3nP//R2rVrjbKmTZsa8wSmpKSoT58+ys7O1rRp01RUVCQvLy9VVlZq+/bt6tWrl0aOHEkvABissXDhtct6PbO+3rJlSyUkJCgtLU3p6ek2vUwWL16sTZs2KSsrS2VlZcZrFotFAwYM0EMPPaS2bdvW47sCnMf6f8PauzArK0uhoaEKDAzUiBEjlJKSIovForCwMG3dulWpqanGvgcOHNDmzZvVrVs3NWvWjHngYCguLpafn58OHDig4OBgSeeGnVpVj5W2bdsqISFBBw4c0JEjR2zuf9auXavKysrLtlFdLfZIYAFoFH755Re9+OKLks49qfLx8THm1RgyZIgGDx6sFi1aqHfv3tq8ebOKi4uN5WSzs7O1dOlSPfzww8aTiQuHg+HakJaWpg0bNigrK0tpaWk6c+aMvLzOfVUGBQUpJiZGw4cPV8eOHY1eVp6enmrVqpVGjx6tnTt3qry83CZ2rA0C6+qEv/nNb4zl4q37E2uoaydPntTPP/8s6dxNVcuWLY2FBsLDw9W1a1d5enqqefPmSkhI0OrVq41hK6dOnVJSUpJ69OihsLAwI25xbTpz5ozS09NVVlamXbt2KSAgQJLUp08f9ejRw6Y3ifW79LrrrlNsbKwyMzONa6TZbFZ+fr7Onj1rk7gKCwvTww8/rMjISEnnh09xnYQ7+vHHH/XGG29IOnftDQ4OVnFxsfz9/TV58mT5+PioZcuWatWqlQ4ePKjy8nLj4cG2bdvUq1cv3XDDDcb/ER54XbvS09OVnJyso0ePKiUlxXjQ5Ovrq6CgII0aNUqxsbE2Q089PT0VGRmp1NRUZWdnG72wzGazysvLba69F2ujulLySiKBBcDNXOyLOz8/X97e3rJYLKqsrFTTpk2NMd/VG9PBwcGaOHGiPvnkE2PfiooKJSUlKT4+3ugqa+1Fk5+fr+bNm9NYaOSOHTumBQsWaMuWLSotLVVFRYWkc8lQ6wot2dnZys7O1vr16xUfH6+pU6caN/Emk0m9e/fWiBEjtGzZMknnE1ceHh5q0aKF/vrXvxrDWavPhwE4i3W4iqPrVUFBgdEYtVgsRoLfyvpz27ZtNXjwYO3du1fHjx83bqT27NmjLVu2qH379jaTvh49elSxsbH18O7Q0MrKyrR06VJt2bJFJ0+e1PHjx+22adOmjfr27avJkyerbdu2xqTBPj4+iouLU2pqqnbu3Gn3RN9iscjf31/33HOPMST7wqXdAVd2sV4qhw8flnRuzsyysjL5+voavbnbtWtnbNenTx/Fx8dr3bp1xrU8JydHSUlJ6t69u5FUsLZPzWazcS1G43bs2DEtXLhQ27dvV1FRkcrKyiSdn6uyqKhIJ06c0J49e9S7d2/ddNNNGjZsmPF97+/vr/j4eO3bt0+//PKL0T619rpq06aNnnnmGfXp00eS67dRSWABcAvWJwYX3phZb8hKSkpUUVFhDFvw9/dXWVmZfHx8jG2tDYsxY8boxx9/VFZWlnFzlpeXpyVLlqhfv34ymUxKT0/XTz/9pJKSEo0fP57l4xupiooKzZs3T998843R+LTGifXGy9PTU5WVlcYX/ZkzZ7Rs2TJ5enpq3LhxRqMyICBAI0aMMG7uPD09ZTabja7Z1uSVNSEGOFP1XlGFhYXGE1hr0r+wsFC+vr4qLy+X2WxWixYt1KpVK5tjWP8PhIeHa/Dgwfryyy9VWVkpT09PFRcXa8OGDercubP69u2r9evXa9OmTdq+fbv++Mc/KiYmxuWGGcB5li9fri+++MJufipvb29VVlYaN9WnT5/WmjVrlJGRoYkTJ2rIkCHG93Z4eLgSEhKUkZGhs2fPGj2eJSk4OFhvvPGGXc8BV72BAqys7dMLr32VlZVGD25JRm8X63ysF/Zk7dChgwYPHqxffvlFeXl5Rvt0165d6tmzp2655RZJ0sGDB7V69Wo1bdpUd955J9fdRqyiokKffPKJvv32WyOpZP1bW0eRWNua1vKUlBTt27dPZWVlGj16tFHes2dPxcfH6/DhwyoqKjKSXx4eHurTp4+RvLowbl2Ra9cOwDXvwiew27dv144dOxQZGamYmBijvKioSNK5pxGVlZVq3ry5TfKqOuuX/uuvv258IVRVVSkpKUnvvfeefH19dfjwYe3bt0+lpaVq166d2rRpYzwxQ+OwYsUKzZ4924gd6fyQF0lq3769jh49arNCYPUE6g8//KCKigrde++9xk1Xp06dNHr0aM2dO9d4gurl5aW8vDzNmzdPU6ZMoTcfnMraALX2FpwzZ46+++473XjjjRo9erQ6deokDw8PlZaWqrS01LjRatasmbHSlZW1odusWTPFx8dr165dSktLM8oPHDigd999V82aNVNBQYHy8vJkNpv1ySefKCoqimRDI7R9+3bNnDlTR44ccfi6tbdq9cmBpXOT/7/33ntq1aqVevfubdwQxcTEKDU1VT/99JPNTVdxcbGSkpI0YsQIhqjCLZjNZmPSdenc/5Vjx44pLi5Obdq0MWLe2lPRy8tLFRUVRtvUUYz36tVLgwYN0nfffWf8n8rPz9cPP/yg8vJyVVZWKiUlRampqSotLVVkZKR69+5dT+8Y9WnFihWaM2eOCgsLjbLqbdSAgADl5+cbbdQLE1uzZ8+Wv7+/BgwYIG9vb3l6eio2NlapqanatGmTkfg0m83auXOn1qxZo2HDhrlFG5UEFgCXZu3CevLkSc2cOVMbNmyQJC1atEhDhw7V8OHD1a9fP+OCa72Qt2jRQtLFhxwOGjRIa9eu1datW425jCwWi1asWGFsY93v66+/VmRkpLp3716XbxX1ZM+ePZo+fboOHTok6XwX7KZNm6pz587q06ePbrjhBvn5+amoqEgpKSlas2aNMdmqdU6XsrIyJScnKyQkRLfeeqskqUmTJho0aJCSkpK0f/9+48mYJH3++ecaM2aMWrduzQ0anMZ6nbqwsbtixQpt2bJFkZGRuuuuu3T69Gn5+PgYE7b6+fnZJK8u1KVLFw0dOlQHDx40emGZzWbl5uYqNzfX5ol/enq6UlJSFBERUbdvFvXm2LFjmj59ujZv3izp/FATb29vBQYGqkWLFgoPD9fx48d1+vRpHTlyxLg+WuerKisr07x583Tvvffquuuuk3RuyJR1QuGjR48aPbcKCwu1bNkyRUVFqXXr1szzA5dnjc+TJ09qxowZ2rhxoyTps88+0x133KHIyEiFhoYaKw1WVlZKkjp27CjJ8ZDDli1bKjExUbt371ZGRoZx3T1y5Ijmzp1r9Mqyth8++OADvfPOO/XyflE/LmyjWq+9TZs2VceOHdWjRw9df/31xuiRPXv2aM2aNSouLjYSUtaRKUuXLlWrVq2M7+aOHTsqISFB6enpxkgB63QpK1euVGxsrJo2bery118SWABc3tdff63Zs2dLOtdgaNKkiUpLS7V27VolJycrMjLSGJZlTURZh2hd7ALs6empyZMna+vWrcbwMOvTY+uQMbPZLC8vL40bN47kVSNQVVWlTz/9VJ9//rmk808/q6qqFB4erqFDh6pv377q1KmTsU/r1q0VFhamQYMG6YMPPtC6desknX/Sdfr0aa1evVrR0dHGMNOgoCCNHTtW+/fvN3r4WRudM2fO1B/+8AeXbhjA/VS/Rnp5eRlJ09OnT2vt2rU6cOCASkpKbFZ5CwoKkmQ/jMXKx8dHsbGxSktL09q1a22G1lZfaTMyMlIPPPCAunbtWtdvE/WgqqpKs2bN0nfffSfp/N/bbDara9eu6tOnj2JjYxUdHW3sU1BQoG+++Ubr1683klLWGExNTdWPP/6o4OBgtWnTRtK5mElJSVFOTo7NhMJHjhzR8uXLNXnyZK6RcAuzZs3SokWLJMkY8nr69Gl9+OGHatu2rcaOHatffvnF5rppTWRdbNhft27dNHz4cM2cOdMYnmvtdWNVVVUlLy8vJSYm1un7Q/0pKyvTrFmzjHlUrd/LZrNZPXr00ODBgxUVFWXTRpWkgQMHKjIyUosWLVJKSorNg9OUlBStX79eYWFhxiqFUVFRSklJ0YoVK2yuv+np6VqxYoVuv/12l7/+unbtAEBSWFiYJkyYoPbt28tsNhuTF1rnZdm0aZO2bdumiooK40u+esLJeiGvzmKxqFu3bsaQLutQRen8U7Jhw4Zp1qxZmjRpUl2/RdQT63LD1ida1r+5j4+Pxo8fbzQMrOXWHgWBgYGaOnWq+vbtazQOrF/wJ06c0Pr1641zeHp6KioqSgMHDpR0fsJVk8mkdevWGY3Z6kMTgasxbtw4ffTRRxozZoyCgoKMudesQ1iys7OVm5sr6fxN05kzZyTZDmO58CapXbt2mjJlikJDQ1VZWWn0PqyqqlJwcLD+9Kc/6Z///CfJq0aioqJCn376qZG8sj4IMpvNCgsL0wMPPKD777/fSF5ZY8I6LP93v/ud8SCp+gTAW7ZsUWpqqnGeZs2aKSEhQeHh4TbnLysr07p165SWliZJXCPh8lq0aKG4uDgj7q0PRD09PXXixAktXLhQR48elXQunps0aWJzvXQU435+frr++us1fPhwSefbK5J9+/Tuu++u67eIelJcXGwzHYrZbDYSTJ07d9att95q10a1xk9iYqIee+wxY0SJdP67fceOHTp48KBxnlatWikhIUFdunSRdL5NUFJSorVr1yozM9PmHK7IZLmwtQIALuLC7tWnT5/WkiVLtG3bNqWnp0uSzZMG6XyyoHPnzho0aJDGjRtnLPFd/ZjVj71kyRItXrxYhYWF8vPzU6dOnXTnnXfS66oROnv2rD788EOtW7fOJnYsFoteeuklRUREOOyRYk1Ybd68WXPmzFFmZqbN0sNxcXGaOnWq2rZta5Tt3LlTr732moqLi40FCCorK9W1a1e9+eab9fvG4dYu7M5/qe79ubm5+v7777Vy5Url5eUZSacLm3sWi0U9e/bU4MGDNWDAACN2L9zGZDIpJydHycnJOnDggDw8PNS7d2+NHj3auW8SLmHnzp1asGCBfvnlF5ueVMHBwfrLX/6iLl26OJzk1xorixcv1kcffWS3wuCoUaN0//33G/MFVlRU6Ouvv9aXX36p0tJSmwmJhwwZoieffLL+3jRwlTZv3qzvv/9ev/zyi4qKiuzi3zr5ttlsVlxcnMaNG2fTi9GRsrIyvfXWW9q2bZukc1MUdOvWTZMnT7ZL/qJx2L59u+bOnav09HSb629YWJj+8Ic/KCws7JJt1EWLFmnWrFk23/kWi0V333237rzzTmPf0tJSffHFF1q0aJGxInFVVZW8vb01cuRIPfLII/X+3muDBBaAenGxVVIuNxdQ9UuUdf+ysjKtXr1ay5cvV2ZmpvG019F+HTp0UGJiooYNG2azkmD1uYwkGRMhFhQUsOJgI5CWlqZFixbp1ltvVY8ePWzibPfu3XrllVdUVFRkM4wwLCxM//nPfyRdPF4ladq0acYE7tYGRsuWLfXaa68pODjY2Le4uFiffvqpvv32W6MxYZ3E9ZFHHtGYMWOYCwuXZJ3k2hqL1qHS1RMI1ZNZ1ePp6NGj2rlzp77++mvl5eUZ8Vq9F5V07rratm1b3XLLLYqOjlZoaKjdcasjZhuPwsJCVVVVqXnz5jYrri5atEhffvmliouLbRJLgwcP1tNPP+3wWNWvmX/729+0Z88eYwl3s9msZs2a6T//+Y9atGhhHO/QoUOaO3eutm7davNAoFWrVnrkkUeUkJBAvKHBXCz2qj8Ird6OrKqq0tGjR/XFF19o586dOn36tCT7B63W9mliYqISExM1ePBgeXt7G9fc6osXlZWV6eTJkyopKZEkEleNRFpamnbs2KGhQ4eqbdu2RqyVlZXpyy+/1Ndff63y8nLj+uvl5aXhw4fr8ccfd3i86tff//mf/9Hhw4dt2rehoaF67733JJ3/bt+/f7/mzp2rXbt22bR3g4KCNHXqVMXExLjsXFgksADUqR07digqKsouIXBhw2D79u0qKipSbm6u2rZtq+DgYHXr1s3ueNX3y8vL0/79+zV//nwdPnzYuNG78AZNOjdkYfTo0YqNjVXPnj0lXTpJAfeUn5+v2bNna9WqVZKkkSNH6oknnpB0/u9dUlKiBQsWaNGiRXY39JdKLFm/yPfs2aN//vOfxlBW675PP/20hgwZYhNXhw4d0muvvaacnBz5+PgY+3To0EHvv/8+8XeNs84vddNNNyksLMzhDYwkZWRkaM2aNSosLFRmZqYCAwMVFBSkESNGKCQkRP7+/hdtaBYVFWnJkiX65JNPjNetN1MXzmfVtGlTDRkyROPHj1e7du2Iz0Zs3rx5WrFihSZOnKiJEydKOn+Ny8jI0Lx587R582abxFKLFi00depUDRw40GG8Wa+bO3fu1LPPPmvsa42xxx57TDfddJPNvsuXL9eCBQt0+vRpmzlfunXrppdeeumSCw0AV+Pw4cN28wlJ9u1Ta2/EEydOKCQkRC1atFDr1q0vup/FYlFGRoY2bNigr7/+WhUVFZdsn1533XUaO3asrrvuOrVs2bJu3ixcQn5+vubMmaOVK1fKx8dHv/3tbzVy5EhJ56+/aWlpmjt3rnbu3Glz/W3durUef/zxiyaWrPG3Zs0avfXWWzbtW29vb73wwgvq2bOnTRv1u+++08KFC5Wfn2/TRh02bJieeuqp+vtgaolJ3AHUiQ0bNmjmzJk6efKk3njjDXXr1s0Yyy2dH5udlJSkzz77TMePH1d5ebnKy8uNL/moqCj96le/UkREhFFWvVHRqlUrJSYmKiMjQ4cPH5ZkP2bbun1+fr4WLlyob7/9VgMGDNCgQYPUu3dvYzgD3N+6dev0r3/9y/jd09NT27ZtU1JSkhISEoyEgJ+fn4YNG6bNmzcrOzvbJqY+/vhjDR8+XD4+PnYNBOvPERER6tSpk/bt22f0wLLOfyHZTswaGhqqcePGafr06UbD4I477tCkSZNIDlzDLky0BgYGKiwszIgnDw8PeXp6qqCgQHPmzNGqVasczkexYcMG9enTR08++aSx0pWVtZEaEBCg9u3b2/SG8fb2VkBAgM6cOWNs5+npqcLCQi1evFgbN25USEiInnrqKYc3anBfP//8s2bNmmXMibZ7925FRkbaPDDq3LmzEhISdPDgQeXm5hq9AM6ePWusFOgoaWq9lvbr1099+vSxGYbo4eGh/Px8STJurEwmk2JiYrRv3z6tXr3aZl7Czp07c41EnbC2T1u3bq1nnnnGmDfQGm/WOE5OTtZXX32lkydPKj8/X+Xl5fL19VVAQIBGjx6t66+/Xu3btzeOW70t0aVLF3Xp0kXHjx/Xjz/+KMl2bk3p3BxzFRUV2rt3r/bt26d27dppwoQJ6tGjh8PEGtzb4sWLNX36dOPvX1ZWpqSkJIWHh6tTp05GeXh4uBISEpSRkaGzZ88a19/Tp09r2bJlioyMlLe3t91DeGv8RUVFqX379srJyTHmWw0ICDCmU6k+jUpMTIz279+vn376yWij3n777ZoyZUp9fjS15np9wgC4tYyMDP3tb3/Ta6+9ppMnT0qS5s+fL+ncxdX6FConJ0cvv/yyXn75ZaWnp6uoqMjoLuvh4SGTyaQdO3bozTff1JIlS4xhgtU7jVobA9bJiK2ioqL061//2kgqWBNnvr6+Ki0t1Zo1a/Tdd9+poqKifj4U1ItTp04ZCUnrRJZnzpzR8uXLVVZWZjQCJCkkJERjx46VdO6plXXC6+LiYn388ccXPYc15qyTsFqfblnnELiQt7e34uLi1KVLF0VFRWn69OmaPHkyQ2KuYV9++aXuv/9+I3klnZs/5cCBA5LOJ0rXr1+vp59+Wj/88IPNUELrddTDw0MFBQXauHGj3nvvPWPia+s1snrD9tSpU8YKrSaTSSNGjNBTTz2lxMREBQYGGkNhrPvk5uYqJCRErVq1qpfPBPXjyy+/1L///W/l5uaqSZMmks4t2Z6cnKyysjKbniHR0dGKioqSJOM71GKx6MCBA/rhhx8kOV5FzWKxqLKyUj169JB0fhhs9QVYqu/btm1bJSQkqG3btjKbzQoPD9err76qJ554Qj4+PnX2WeDac2H7NCcnRz/99JMkGe1Ok8mko0eP6sUXX9RLL72klJQU5ebmqry8XF5eXiorK1Nubq4++eQTvfrqqzp06JDdeayxXVlZqZycHOP4Pj4+io6OVmRkpCQZbVDr6oVZWVl699139emnn9I+bYS2bt1qPIi3TgGwZ88ebdmyxeg9Zb3+xsTEGHFivf8xm81KSUnR6tWrJdkvumJlsVjUpEkTm4Rsfn6+iouLjW2sr3Xo0EFRUVFq2rSpYmNjNX36dN17770uOWywOnpgAXCK4uJizZ49W99//72k8xl+Ly8vnT17VpmZmerYsaMkae/evZo3b56xGlv1i3D1RJWnp6dyc3P1xRdfyGQyady4cQ6H/V3YKI6Li9P48eMVGxurlStXavPmzcrNzVVpaakCAwN133336cYbb6yHTwV1yRoL1i/+QYMG6fDhw1q9erWxEpDZbNa+ffu0cuVKjRs3zogdb29vJSQkKCkpSbt27bKZLHPx4sUaM2aMQkND7YYSVL/Bt7I2ODp06OCwnkFBQXr++efVvHnzOvkc4B42bdqkGTNm6MSJEzblzZo1U8eOHW2eiu7YsUMLFizQiRMnjHI/Pz+VlJTIx8dHxcXFxvXVbDZr/fr18vT01H333adWrVrZ9Yw5duyYpPNDuby9vRUdHa3rrrtO2dnZWrBggXbv3q2ioiINGDBADz30kMNJ3eGerPEQFRWltWvXKjs7W+Xl5fLw8FBZWZk2b96sXr16KTo62rjetWnTRgkJCUpLS9ORI0dsVqpavXq14uLi1KFDB7tYM5lM8vLyMr7XrYkBSXYJUWu8R0REaPjw4WrXrp2x8hrgLBdrn+bn5ysnJ0e5ublGT9PNmzdr3rx5ysjIsJnnSjqXkLImcj09PZWRkaE5c+botttuU2RkpE371DpvkXVf63X37rvvVo8ePbRs2TJt2rRJO3fuNB60BgYG6v7779eoUaMa4FNCXbHOWXnnnXfq2LFjxqqUHh4eKi4uVnJysnr27KmIiAjjWtquXTslJiYqPT1dR48eNb67i4qKtHLlSsXGxqp169Z211/rHILV2xPWHlgX9uqzvp6QkKDevXtftA3rikhgAbhq3377rebOnavy8nKjzGKxqEOHDoqMjFRiYqJCQkKM15YvX659+/YZF9i2bduqe/fu6tKli06fPq09e/YYy7hK53pYLVy4ULGxsQoODjYu2NaeL9aeXtYJsq0NiPDwcHXt2lW33HKLFi5cqFatWumuu+4ynjzDPZWUlOjzzz9XmzZtNHbsWOPLu23btoqLi9P+/fuVlZVlfOEXFxdr1apViouLU1BQkJGUat26tcaMGaM9e/YYPQy8vLxUWVmpGTNm6LnnnrPrKWWdQ+v06dM2q7x07tzZZihBdR4eHiSvrmFHjhzRRx99pF27dkmSTUKqR48eSkhIUHx8vJEwys3N1ccff6wjR44YidXw8HANGDBAUVFRat68ub766istX75clZWVxlPbLVu2qH379po0aZLdpO7VVyOSZNyseXp6qkuXLvrLX/6ijIwMFRYWKiIior4/ItQR6425NR66deumQYMG6euvv7aZgyc9PV1JSUnq0qWLzSTrffv2VWxsrLKzs216CGRnZ2vZsmV66KGHLjoPS/WYs34nV28HSOcfCAQGBuruu++us88B165LtU8jIiJ0ww03GInV0tJSLV++3EjYWiwWBQUFqWPHjmrfvr0yMjJ06NAhFRUVGcfas2ePAgIC1LlzZzVr1sxmSHZRUZFOnTol6dxDs9LSUpWWlkqSxowZo+HDhyspKUlfffWV+vTpowceeEDe3t71+OmgLhQXF2vlypXq0aOHevXqZbQje/XqpcTERC1evNiYLkU6NxdmcnKyOnfurMDAQOMaGhkZqZSUFOXk5BhtVLPZrMOHD2v58uWaPHmy3fXXZDIpKytL+fn5NgvA9OnTx65Ha/Xrr7tNp0ICC8AV27p1q6ZPn248TbA2blu0aKHevXtrwIABiomJsbkwLl682OiyLUm+vr4aP368rr/+euOm6ssvv9TcuXON4Vlms1lnzpzRl19+qccee8xmIuLqcw9Zb+aCgoKM1z08PBQSEqL/+Z//YdhWI7B48WJ9/PHHKisrU58+fRQTE6Pg4GCbL/zU1FTjC98ak5mZmVq2bJnuu+8+Iw48PDzUt29fDR06VKtXrzaeVJlMJm3btk2bN29WXFyc0XCwftnv2LFDBw4csLkxi42NdbsGAOpWSUmJ5syZo2XLlkmy7YnStWtXRUVFaeDAgcZwVKt169YpPT3d5np61113KSYmRtK5BIF11dTqQw6KioqUlZVl05vA+nr1a3T1ubSs/xcsFos6d+5cdx8G6lVycrKmT5+uqKgoTZkyRc2aNTPiZcyYMdqxY4dSU1ONeQGrqqq0bds29e7dW0OGDDESUP7+/oqPj9e+ffu0d+9eIylVUVGhTZs2KSEhQREREcaxrd+5Z8+e1Y4dO4z6VFVVKSgoyFhABahrF7ZPrW3J5s2bq0+fPnbt06qqKs2aNUtbtmwxjtG6dWtNnDhRAwYMUMuWLWU2m/XWW2/pp59+Mv7vVFRUaNu2bVq/fr3GjBljM0KgoKBAXl5eMplMxgqy1l4uVVVV8vHx0Q033KABAwaQuGokFi9erNmzZ6uiokJ33HGHOnbsqICAAJvr786dO3Xw4EGb7/CtW7eqV69eGjhwoHH9bdasmRISErR//37t37/fOEdZWZnWrVun+Ph4hYeHG+1W673Rpk2blJeXJ+n8Qytr+6GxIIEFoNaOHj2q6dOna+vWrZLO3wSZzWaFhYVpxIgRGjhwoJFIst7ol5eXa+PGjZJkLAV/6623asKECZKkrKwsTZ8+Xdu3bzf2k84lubp27arQ0FCjESDJGP6QlZVl1KOystK4oXM0uSzc0/bt2zV9+nTjby1JBw8e1Pr163XbbbcZf9/AwEDjhis1NdWIofLycm3YsEEJCQnq1auX0XBo2rSpRo0apa1btxqTZUrnGpczZ85UXFycTeykpaVpzpw5xhCCqqoqdejQwVhFBpDONWLnzp1rPG23zslmNpsVHBysqVOnqkuXLsY8GNZhKuXl5fr5558lnZ9v7bbbbjMan998843mz59vHNd6rWvXrp369u2r0aNH26xidWFvGGuCwdrby3ptZrLsxiErK0sfffSRkTzavXu39u7dqwEDBhgJpubNm2vUqFHKyspSQUGBMUfV8ePHtWnTJnXv3l0hISHGDXrPnj0VFxenw4cPq7Cw0Lju5ebmaunSpYqIiDDKrNfKbdu26cSJEzYPC26++WZWFESdu1T7tHfv3rrhhhsUFRVlJJKs18SCggLt3r1b0rn2qdls1j333KNhw4ZJOvd/a+bMmcZxpfNJqE6dOikwMNCmfWplHSHg6empwMBAYx6i6u0Kklfuz1EbdcOGDYqIiFC/fv2M72Hr6sHHjh1TUVGR8d2clZWlpKQkde/e3VhUwNPTU71791ZcXJyOHDmi0tJS41p7/PhxLV26VE8++aRNLO3du1dLliyRdP4BVp8+fTR06NB6/TzqGgksADVWUVGhuXPn6ptvvpF0fgn26iur+Pr62vSmqj4++/jx4zp48KDx1EqSBg4cKEn68MMPjYtu9aWGw8LC1L9/f40aNUrt2rVTaWmp8WVvncPA399fxcXFqqyslHRu5Tc0DseOHdP06dO1efNmSedjw5rUbNmypV2jsXfv3oqPj9eRI0dUXFxsfImfPHlSS5YsMbp0W2OzW7duuummm7Rw4UIjlj09PXX06FF98803uvXWW5Wfn6+kpCQtXrzYZmWXZs2a6b777rvo8EFcWy5sxFobrdVXqDx+/Lik80Oevb29jSSS2Wy2WalNkgICArR792598MEHys7ONo5jsVhsehMkJiYqKytLe/bsUWRkpJFQqKio0LFjx4yYrX5sEleNQ1lZmebMmWP3HXr06FEdOnRIffv2VWBgoPH3HjZsmJKTk7V582abXgC7d+/W1q1b1aFDB5ukVFxcnPbt26eNGzcaCS/r9qtXr9bw4cONB0grVqzQRx99ZNPTLyQkRP3792+QzwbXhpq0T5s1a2Ys3mIts14L9+/fb6xKXFFRIR8fH6N9+tFHH2nx4sXGcau3T2NiYjRq1CgFBQXZPSg9duyYPD09jeGDZWVltBUamYu1UX18fIwFUi6cp2rkyJHasmWLduzYYdMLdufOnerdu7dGjx5ttFGtCwGlpqZq69atNtff7du3Kzk5WfHx8crPz9eGDRv01VdfKS8vz9imZcuWuvvuu+1WKXZ3JLAA1MiKFSs0e/Zsm7H/1huz6nMB7d+/X0lJSRoxYoR8fHzsJnctLy+36e46b948paSkqLCw0Oa4QUFBioyMVEJCgqKjo1VRUaHnn39e4eHh+vWvfy0fHx/ji+LUqVNG8uqGG25QcHBwPX0qqCuVlZWaO3euFi1aJOl8o0A6N99U//79NWDAAGOlKytrz6jY2FilpqYqOTnZ5gt/165dWrdunQYPHmwcz9qNPzk5WRkZGTaN0Llz5yo4ONhYqcuafJDO9Xq5+eablZiYWMefBlxdTk6OZs2apaSkJEm2Nzlms9lm3r7Kykq9++67evvttx0+ra/ey0WSFixYYDzFr9447t69u2JjYxUXF6fQ0FCdPXtWH3zwgXJycvTJJ58YybP8/HyZzWZZLBa1b99eDz30kOLi4ur3A0KdWbp0qT7++GOVlJQYZda/de/evRUREWGzfLo1KTV27Filp6fr5MmTxndyQUGBkpKS1LNnT/Xs2dNIeIWGhiohIUHp6elGz6qqqioVFBRo1apVSkhI0OnTp7VixQpt2LDBuOZaLBb16NFDTz75JA+WUGcu1T6tnqRPSkrSoUOH1KVLF7sFWgoKCoyHCNK5BQc+/vhjrV279pLt0/79+6ugoEBPPPGE7rvvPiUkJNg8VDObzUaP2RtuuMFoL/PwwL3VpI1q7fFvZY0vHx8fjR49WhkZGcrLyzOul6dPn1ZSUpJ69OhhM7VAly5dlJCQoIMHD+r06dPG9ffs2bNasmSJsfDL5s2bdfz4cSO2OnbsqNtvv119+/atvw+mnpDAAnBJhw4d0ttvv20sFWy9SDdr1kzh4eHatWuXKioqbIZeWYcVhIWF2RwrPz/fpkdCVVWV3VOLZs2aqVevXkpISFBMTIwxHGbZsmXauXOnMjMzFRcXZ8ylceTIEVVWVioqKkoPPPAA87g0Aj/88INmz57tsNHYt29fYzLr6g3EC3uUdOrUyfjCP3XqlM0X/vLlyxUdHW0zL0H79u01duxYvf/++0aj1/ok9r333jN6+FWf2HXixIkaMWJE/X9AcCl79uzR3/72N0n2jdi+ffvq7Nmzxrx/1njLyMjQihUrdOONN9rcSHXq1EnNmzdXUVGR8QT25MmTl20cV1RUaObMmcbcbJ999pnuvPNOmUwm5eXlqbi4WPfdd59uu+22+v54UEd27typ6dOn68iRI5Jsv0P79OmjxMRExcbGqmnTpjb7WWOtX79+io+P14oVK1RRUWEserFv3z4lJycrLCxMfn5+RnxGRUUpJSVF33//vc2EwgcOHNAbb7xhLPFePZEWHh6uW2+9Ve3bt+emHU63Z88e/fe//zUW/bH+H2jatKnat2+v9PR0I1atPQQ//PBDvfLKK3a9pazDuK0LueTk5Nj1urpU+zQ7O1sff/yxEhISjLZJSkqKLBaLoqKidP/996tLly71+OmgrtS2jVr92mdtq1pXwf7xxx9tFmP55ZdflJycrI4dOxpz/Hp6eqp///5KTU3V6tWrba6/e/fuVXZ2tk6fPm3TRg0NDdXEiRN1/fXX1/8HVA9IYAG4pPT0dKOBLJ0bq9+tWzfFxMRo7Nix+umnn/Tf//5XkoynCJmZmVq7dq3uuOMO+fr6GhfvPn36qHnz5sYTXGuvAOnccBprj4LExETjaW1+fr7mz59vTIScm5trzNNhnbD9D3/4gwYPHlzPnwyc7ZdfftG0adOUkZEhSUZDslmzZurZs6exWpt1RT9rD5ULG6LWhFZ0dLRSUlK0cuVKmy/89PR0rVixQhMnTjQaE15eXoqNjVVMTIy2bt1qM/SgqKjIZhhCXFyc7r//fnoUQJIUERGh4OBgo3deUFCQIiIiNGjQIPXr108nT57UgQMHdPToUZtYnT17toYNG2bMt2Kd0y8qKkpHjx41Ys7aIG3Tpo2ioqIUFxen/v37G6up7tmzx+jJam0kb9myRRMnTlSTJk0UGhqqOXPmMP9QI3H8+HHNmDHD6O1nvVY1adJE3bt3V1xcnBITE42hStbv2eqxZ423sWPHas+ePTpy5IjNkNMtW7aoV69eNnMAtmzZ0phQ+NChQ0aslZaWavv27TarGkpSVFSUJk+erPDw8Pr4WHAN2rx5szIzM422grV9Gh8fr759+2rVqlVG29HaBvjll1+MXtjVHx706NFDzZo1M1Zvk87Pw2o9rqP26axZs7R69WpJ5+YxWrNmjTFvVufOnfX0009ryJAh9fq5oG788ssv+uijj3Tw4EFJ5+eYatq0qZHYrEkb1Rp348aNU2pqqrHKq4eHh0pLS7Vlyxb17t1b/fr1s1llOz4+XmlpacrMzDRitLKy0ljpUjoXswkJCbr33nsbdRuVBBYAh6xJJ+tY7eTkZLVv314xMTEaMGCAevfuLencUsCLFy9WVlaWTS+sFStWqH///oqIiLAZtjBs2DB99tlnNvNj+Pn5qX///rrnnnvUrl0748K8fft2/fDDD0pOTrabd8B6rrCwMLueXnA/J06c0Mcff6yMjAzjht7aE2rAgAG6++671aJFC0ky5hSwxkBJSYmys7PVuXNneXl5GV/4rVu3VkJCgtLS0nT48GEjrkpKSrR27VrFx8crJCTEiM1WrVppzJgxNr0KTSaTMTy1TZs2euihh4x5MQBr7PzmN7/RSy+9pLi4OCUkJCguLs5oxLZu3VpjxozRjBkzjJj28vJSUVGR5s6dqwcffNBmOGvnzp3VsmVLnT592khOWCdzvfvuu435BY8dO6bNmzfr559/Vnp6uqRz/zfCwsL0yCOPGAmuxjb3xbXI+n28aNEizZo1S5LtHD8dOnTQoEGDFBMTY3w3S7K5Qa9+LOswptDQUA0dOlQLFy5USUmJEYcZGRlKSkpS165d1bp1a+M4ERERiouLU1ZWltFr68KeVb6+vrrnnns0fvz4evhkcC2yJmAfeOABrV69Wvn5+ercubMiIyM1cOBA4/+AxWJRcnKycnNzbf4fzJw5U4MHD7aZC7NJkyYKDw+3maRdkpo1a6bExESNHTvWpgfVtm3btGzZMpv2qbe3t/z9/Y1tBgwYUMefBOrLnj179Morr6iwsFBeXl7GSJKAgAANHz5ct9xyi9q0aSPJvo1aWlqq3Nxcm8UxJKlbt24aPHiwvvnmG2OYqSQdOHBASUlJ6tKli80KstZVto8ePWqUVW+jtm3bVg8++OA10Ub1uPwmAK5F1Sf8nThxosaNG6d77rlHkydPNhoH1onYf/Ob30iSMdeLp6enCgoKtGLFCmNCYusFOyoqSqGhoTaTZ5aWlio7O1tFRUUqLi7Wrl27NGvWLM2bN09JSUmqqKgw5vR45plnjKdbcH+bNm2SJLVo0UI33nijmjZtavy9pXMNgePHjxvJq4qKCmN4n3TuidjcuXP15ptv2izbbk2QRkREKCYmxlgFzjqENTs7W0uXLpV0fqJtk8mk3r17a/jw4ca5rV2y77rrLs2YMeOaaBig5qxxGB8frz/84Q+65557NHLkSDVv3twYkuLj46PExERFRERIkk3vvm+++cbomWW9nvbv398YIm29ubIOLfj444+1atUqzZgxQ++8846+/PJL7du3T5WVlfLw8FDPnj31m9/8Rt26dTP+D8H9mUwmVVRU6MCBA8ZNi/U72sPDQ506ddKdd95pfDdXVVXZ9Lo6fPiwFi5caAxHtb4uSTfeeKO6detmzM9i3Wf79u3GNdV6o+/j46P4+HjjPB4eHjY3UCNGjND06dNJXqFOWa+JkvT4448rLi5Od999t6ZMmWKTwO3UqZNGjx4tybZ9mpubqwULFkg638uqXbt26tOnj5o2bWqTlC0rK1OTJk2MNoj1Ojx//nwj2WV9cPCPf/xDCQkJ9fIZoH6kpqZKOtebLj4+Xr6+vqqsrLTple/v728kry7VRs3NzTVi17r/6NGj1alTJ5vrr8Vi0bZt27Rz505J59uoAQEBSkhIMOZ+NZvNNm3U6dOnXzNtVHpgAbgo6wW4Z8+e6tixo/Fkyfrlbh3fHR0drbi4OG3evFleXl7Ghfmnn35SfHy8sYS3dO5LYMSIEZozZ47NBPAZGRn629/+Jl9fX505c0be3t7GDZ10bkLN0aNHs5JRI7Fu3TpNnz5dp0+f1r///W+Fh4cbywWvXr3aaFRaJ6dctWqVRowYYcRcVlaWNm7cqC1btigjI0OlpaVatGiRwsPD1bx5cyOu/P39jWEve/bsMWK3oqJCSUlJSkhIUGRkpNFwCAwM1KhRo/Tjjz+qtLRUQ4YM0QMPPGDMdQFcyBpT1YcxX9grpU2bNho7dqz27t1rXPesw16mT5+uZ599Vt7e3rJYLGrbtq2GDBmikydPKj093ThOXl6e1q5dq7Vr1zqsR1hYmH71q1+pX79+klhhsDFITU3Vd999p4kTJ6p79+4aPXq0jh07prS0NEnnHzQdP35c+/btU0REhM0E0nl5ecZqg1u3blVaWpr+9re/Gd/HZrNZgYGBGj16tDIzM3X27FljKoBTp04pKSlJ4eHhCgsLM67J4eHhSkxMVGZmpk6fPi3p3MqvDz/8sLp169YAnxKuRdYYTkxMVJ8+fdSsWTNJsmk7+Pj4aPDgwUpOTlZaWpqRCJDOLY5x4403qlWrVsb/mYEDBxpzwFnbBGVlZVq8eLE2btwoPz8/Y0GNsrIyoy7t2rXTuHHj1Lt3b+Z6ayR+/vlno7frP//5T4WEhGjw4ME6ePCgMc2FyWRScXGxtmzZooiICEVERFyyjTpv3jw9+eSTRuLfbDarVatWGjlypLKzs1VYWGjcP+Xk5GjTpk3q1q2bOnToYMSjdToN6/xqN9xwgx588MFrro1KDywANXJh8srK+hTM2gvL+iTW2lBYunSpzfhsf39/DR8+XDfccIMkGU+STSaTysrKdObMGZveX5LUoUMHPfbYY5owYYJN92y4n/T0dP35z3/Wv/71L+Pm58MPP5QktW/fXoMGDVJwcLBNDz1JxkovxcXFxopDixYtUmpqqtGQ3L17t/bu3WvsY43THj16/H/27jsqqmv9G/h3GDqIICJFxIaKioggRUWwF+xdo1E0xdQ3dV2Te5Pc5KaY+4u5Kabae++xd1FBQFSaoIIUEUWkSmeGef9gnc0cBxUjSvH7Wcslc2bmzBnc7rP3s/d+Nry9vWFubi5L+J6Tk4P9+/eLmQpSw8HJyQkvv/wyvvnmG7z//vvPXcOAHk9NnZX7j+np6aFHjx4iF4pUxykUCkRGRoqRfKn+9PLywqRJk2Bubi7qQj09PRgYGMjqTMmgQYPwn//8B97e3k/lO9KzlZubi++//x4LFizAmTNncPjwYQDVM0ql7dmluiwtLQ3Hjh1DWVkZDAwMUFJSgtDQUKxatQpbtmzB+fPnAQDh4eEidxZQXU779+8PNzc3cd+WzhsTE4Pz58+L2atSWezVqxfatm0LCwsLfPDBB/j2228ZvKJ6ox28ur9utLW1RWBgIIDqmYnSEjApQCEFwxwcHDBs2DCxOYYUaACq/k+mp6ejvLwc5eXl4vxt2rTBq6++ihEjRsDQ0JDBq0ZOaqMuWrQI2dnZuHfvnpit7+npCQ8PD5iamsrqydTUVISEhAAACgsLH9hGPX78uCx/oPT3oEGD0K1bNzHwKpXH6OhoXLhwAUB1vi1pQ41x48Zh4cKF+OCDD57LNipnYBHRY7n/5ixVqnZ2dhg/fjx27dol24o4Li4O586dQ2BgoBiZsLS0xBtvvIGysjJcuHBBzLSSziUtvTE2NsbYsWMxY8YMWTCDGp979+5h1apVOHr0KIDqxNSGhoYoKChASkoK2rVrhy5dusDPzw/bt28XHXyFQoG0tDQsWrQIZmZmiIyMFKOgQFWj1dLSEi+88ILO9GmpkeHp6Yn4+HiEhoaKGQZqtRpxcXE4deoUBgwYIMtDNHTo0Gf3y6HngoWFBYYNG4YLFy6IHVmBqk7V8uXL4enpKfK/GRgYoF+/fjA0NMTOnTvFzC0pgGBsbCw2vpgxY4Zsq25q3DZv3oyNGzfKloCeP38esbGxcHV1hb+/PxISEnDp0iUxm0+lUiEuLg4hISFo27YtTpw4gUuXLuHGjRuiUy+dLycnR5xXOz9lYGAgrl69iszMTHHe4uJihIeHw8XFRXSwgKqO/rx589C2bdtn+8sheoiagkdKpRK9evWCr68vzp07J1t+GBwcjMDAQHTt2lXMwvLw8ICVlRW++eYbZGdn65xPO7H7hAkT2D5tIh7URlWr1cjKysLt27dhZ2eHgIAAMaNfqifLy8tx8eJFrF69GoWFhbh48eID26jaedSkvpK+vj4CAwPFrtlSG7WgoADh4eHo0qULOnXqJMp3+/btn/sdLRUaJkkgoickzWpRqVSYM2cOCgsLZZ0zOzs7LFiwAB06dBDHlEoliouLERcXh+DgYKSkpCAzMxOtW7eGnZ0d2rRpg+HDh4uExdR47dixA+vXrxezSyQODg7w8PCAl5cXevbsKW7OCQkJWLZsmZjyr72LlvSzdods4sSJmDVrlihzD5rCf+LECWzYsEHsgglUld3WrVvjm2++EUm3iZ6W0tJSbNu2DVu3bpVtq61WqzFv3jyMGzdOJ/F2WVkZwsPDcevWLdy4cQP29vYwNzdHhw4dRF4tavzOnj2L5cuX63Sarays0LNnT4wcOVIEKg8dOoTNmzeL5NRSsL9169YwNjZGWloaysvLZfWkn58f5s2b99B76sqVK7F//36Ul5eL8xoYGGD06NGYPHmyzixWosZAo9Hg0qVL+Pbbb1FaWgo9PT3RZu3YsSP+97//idcBVYGF5ORkXLhwAUeOHEF5eTmys7Ph6OiIFi1aoGPHjhgzZgzbp03Eg9qorVu3hpubG/z8/NC9e3dxz96zZw927NiB3NxcWf1rYmIiNsN4WBv1QX7//XccO3ZMbCKkVqthaGiI4cOHY9asWdxJWAtnYBHRE5M6YPr6+ggKCsIvv/wCoHrb4tu3b+PEiROws7ODqampqMRNTU3h5eUFT09PlJWVoaioCC1atEBhYaGYEk6NV0REBJYtW4bbt28DqJ5hZ2VlhW7duqFPnz7w9PQUy0KljlH79u0REBCA69evizIkPS/RaDTw9vbGSy+9BDs7OwDVgdH7g1dSY8Ld3R0JCQk4fPiwCIQBELsXEj1txsbG8Pf3R1hYGNLS0mQN2nXr1mHo0KEwNTUV/xekJPD9+/evx6umpyk5ORlLly4Vy5+ljo+pqanId+Lt7S3rLPft2xexsbE4e/asqCMrKytx8+ZNWQBfo9GgQ4cOeOWVV9CtWzcA1fdl7XpSKm/SLqzJycmorKyEoaEhysvLcezYMXh6eqJHjx4MXlGjo1Ao0KVLFwwZMgR79+4FULVcW6lUIikpSeTY1N7EQJrlMmLECBQUFKC0tBQtW7ZEeXk5A1dNxP1tVKkefVAbVWpj+vn5IS4uDmFhYSJ4pdFoUFxcLKt7H9RGvZ9U/44aNQqXL1/GzZs3xXPSclXWu3KcgUVEde69997D9evXRT6NyspKmJqaYsGCBbKZNtQ0paenY+nSpbIdrACI7d6HDh0Kf39/0QisacZUeno6Vq1ahYiICNGRl25XNjY2WLBgATp16gQAogFRmxv8xYsXsWzZMqSnp6Nt27aYN28e3N3d6+ibEz2aSqXC0aNH8fvvvwOA2LFIpVJh+PDheOONN2qc5aId1GId2vgVFRVh1apVIr+VdlCpQ4cO8PDwQJ8+fXSWikj//uHh4VizZg1u3LghOlDao/7m5uaYPXs2hg8fLt6n3UG/n/T+vXv3YvPmzWIHYTMzM8yZM0ech6ixSkpKwn//+19kZmbKVglYWFhg1apVYpY369mm7WFt1E6dOmHAgAHw8vJCq1atAEBnUEChUODMmTNYt24dbt26JSsnGo0GdnZ2ePfdd2W7wta2jbp9+3asXbsWGo2GbdSH4JAzEdUZaXThlVdewccffyxmzEjLBQ8ePIi2bds+lwkHnwdlZWVYvXo19u3bBwDihq09QmVgYAA3N7eHBq+AquWF/v7+uHLlCgoKCsSNX6PRwNbWVgSvVCpVrWZPSZ/TrVs3BAQEwMLCQmyvTfQs6evrw8vLC+fOncPFixdlOVkOHTqEwMBAtGvXTme0Vvo/wE5V47d7926sXbtW5H/U19cXW6u3atUKM2fOFEnVgZrryd69eyM6OhpZWVkoLS2VPd+xY0csXLgQRkZGAKrvzY9awgIAQ4cOxcmTJ1FQUIBx48bhxRdfFPkriRozJycnBAYGYuXKlbL2aUFBAdauXYugoCDZMkJqWmrTRm3WrBlGjRol3vOgNqq3tzeio6Nx9+5dVFRUyPK6DhgwQASvattGlQKnQ4cORUxMDLy9vcXmA6SL89GIniPSjflpTbyUZlx169YNfn5+Oru7nTt3DlFRUbIdBqlpOHDgAObMmSMaBgBEAkztm39KSgrCwsLE6P6D6OnpoXv37ujTp484F1DV4IiNjRW7wtS2kSk1LoyMjDBlyhQGr6hG2stUnyYrKyuxsYX0f0Rq5P75558AUKtgAzUu58+fx+uvv44VK1aIPCfaAUwAuHPnjs6uf9r1nJT4V09PDwEBATUmUler1YiPjxc/16YsSQndjYyM8Morr2Dp0qWYN28eg1f0TD3NhUEGBgbw9fVFt27dxAYv0uft3LlT5MdkG7Xp2b9/f63aqJGRkbh48aJ4fH8bU6p/DQ0N4e/vDycnJ9lz5eXlCA8PF+eo7dI/afmihYUFPv30UwavHoEBLKLnhHYlrT0Vtq5JHcCXXnoJQNXog56eHoyMjKDRaLBhw4Yad3ahxiktLQ3vvPMO/vjjD5SUlIiy1bx5c/Tu3RuWlpainEmdqNOnTyMpKQnAwwNQ1tbW6Nu3LxwcHGRbFgNVMxikzQJqG3S4v/wTSaQOi1TG8vLyoFarkZub+1Q+T09PD926dcPAgQMBQFaOL1++jOvXrz+Vz6X6kZ6eji+++AJffvklMjIyRB2kr68vW/4n1ZFr166VbYZyP6mcdurUCT4+PrCwsJDVkTdu3MCxY8fEOWp7r5c+r0uXLmL5DNHTph0wetr3ZxsbGxEckNrFUnJsKZk7Bw+ajsuXL+Ott97Cn3/+KWujWlhYiJx+Ut0p/btv2rQJZWVlDzynVM+6urqid+/eMDMz06l/z549i7KyMlmQ9FGk97P8PRqXEBI1cdpLB9RqNUJDQ5GVlYU7d+7A0NAQbdq0QceOHetsS1bpc1q0aIHp06dj06ZN0Gg0KCsrg4GBAYYNG8aGcROSnp6Oe/fuiceGhoZwdnaGp6cnRowYgbCwMPz0008AILYGzsjIQEhICNq0aYOWLVvWeF6pQ+fs7Ax/f39s2rRJNDYVCgVu376N7du3Y86cOQxI0d92f16ghIQEHDt2DLm5uUhNTYVKpYKFhQX8/f3Rt29f2Nvbi/c9abkzNzfH0KFDZVtue3h4ICgoqMZZNdQ4xcfH46OPPgIAWbCqXbt26N27NwoKCnDo0CHZzKqUlBQcOXIEEyZMeOB5pdf2798fcXFxiIyMFAna1Wo1Ll++jJCQEAwbNox1JDVI97dPL168iMLCQmRkZMDW1hZ2dnbo2LEjjI2N6ywnlVKphJubG/z9/REcHIzKykqUlpbCwMAAHh4edfCtqCGJiorCjRs3xAwnqY3q5eWFvn374vDhw9i2bRuA6jZqQkICTpw48dCZ+lL96+/vj/j4eERHR4v6t6KiAtHR0Th37hwCAgJY/z4FDGARNVH3j+geP34c27ZtQ05ODioqKmTbxerr62PKlCno378/WrduXeslBw8ijSLMmDED27dvR0VFBYYPH47Zs2fD3Nz8yb4YNQhS+fL19UVcXByOHj2Kli1bwsvLC3369EGXLl0AAIMGDcLRo0cRFxcnyzcQEhICV1dX+Pn51VjWpBu+ubk5vLy8EBUVhfj4eNEIAaryBQ0YMABt27Z94jJLzx/tOjIrKwsrVqxASEiI7DX6+vrIyclBSkoKjh49igkTJoiAQF10qNq2bQs/Pz8cO3YMb7/9Nry9vZ/ofNTwdO3aFV26dMGVK1dEDj83Nzf06dMH7u7uqKysRFxcHNLT02V12Pbt29GnTx+xg9X9pPtsq1at0K9fP6SlpYlAKADcvXsXISEh6NGjB+zt7WvcGICoPtw/K/vEiRPYtWsXcnJyUFxcLGuf2tnZ4eWXX0bXrl1hbm5eJ+W4efPmGD58OIKDg6HRaNg+bYKk+/PUqVMRGhqK1NRUtGvXDu7u7ujTpw9cXFwAAIGBgThx4gSys7Nlif23bt0KLy+vB+44KZVBR0dH9O3bFzdv3kR2drZoE9y5cwdnz56Fq6srrK2tWf/WMe5CSNTEJScnY9myZYiNjQVQvc5a2vFIGi1QKpXo1KkTvv7661olHHwUKaAQFRUFU1NTkXSbmg6pHCUnJyMiIgJt27ZFr169YGhoCACoqKiAgYEBoqKi8OWXX4pGqTTTwMvLC3PmzEGbNm0e+jkVFRU4evQoli9fjoqKCtmuhH379sWCBQue+nelxulRjUaNRoMjR45gzZo1uHfvnpjhJ83K0l6eqtFoYGVlhalTpyIwMLDOgqaFhYXsODVRUhmJjIzE4sWL0blzZ/j6+qJ3796wsLAQrzt16hT+97//ic6PFOgfMWIEXn/99QeeX+qkFRcXY+nSpQgODoZKpRIzWszNzTFu3DhMnTr1qX9XoseVlJSEpUuXinxtUvtUWvaqVCpRUVEBa2tr+Pv7IygoCEDdzIAtLi5GcHAwOnbsyPZpE6Vd/545cwZ9+/ZFz549RRtVev7gwYP4/fffderfCRMmiDJXE6kcFhQU4M8//0RISIgov2q1Gs2bN8fEiRMxfvz4Z/Btny8MBRI1Mjk5OSI/yoMSTUqdrvDwcPz444+IjY0VFbP0XGVlJSorK6FSqcRzCQkJWLFiBQoLC5/4OqWOXc+ePdk4aKKkwED79u0xbtw4+Pj4wNDQUIyuSsl/e/bsKZL6a4uMjMSFCxcemmtAOk/Pnj11ZqcolUpERESIWTPPKgE3NWwnT57EmjVrxHT+h43TBQcHY8uWLSJ4pdFoYGJiAoVCAQMDAxEolQL+ubm5WL58uUj2WxdljsGrpku6D3p6euLNN9/EnDlzMGjQIJGzSiqb/fr1Q69evXQSSx88eBCXL19+4PmlMmtqaipmUAPVS2EKCwsRERGBhIQEcZzoaSkoKBCzAB9U1iorK6HRaHDixAn897//RXx8vE77VGrbSu3T7Oxs7Ny5U2zeUhfl2NTUFCNGjGD7tAnTrn9ffvlleHl5ydqo0vMDBw6Ei4uLTv27Z88eka+1JlL9a2Fhgf79+8PBwQFAdfnMz89HREQErl27BuDpblDwvGEAi6gR2bhxI+bOnYvvvvsOQPWuf/fT09NDaWkp9u3bh9TUVNlUV2kUq1+/fiLBtvYMhUOHDuHSpUvchYUei7Rd+/0jo1I5mjZtGpo3by46bVLn//Tp00hOTn7k+e3t7eHn5wcrKytUVlaK3duk2VlA7Xd7oaYpISEBH374IX744QecOnUKkZGRNb5OqjNTU1Oxfv16ZGVliYZo165dMWvWLPz222/497//jf79+wOoTvarr68PlUqFlStXAuCGAPRoUnnz8vISASapntRO5i7NkpLqTKlztXnzZtmSqvtJ5/Dw8ICHhwdMTExkQdeUlBTs379fbKhC9DSsX78er776qsgn9KCypqenh7t37+LgwYO4c+eOrH3q5eWFcePGoW3btmIAQU9PT7xm5cqVyMvLq7PBA3p+mJmZAah59p6RkRGmTZsGoKr+ldqo0lLC2gSevLy80LNnT7FhldQmjo2NxYULFwCwvVCXeCcjagROnDiBefPmYdOmTQCAW7duYe/evQAePBK1ceNGXLp0SSQp1tfXx7hx4/Daa69hwoQJ+PDDD7FgwQIYGxtDrVaLqbQqlQpHjx5FSUnJM/t+1HTcf4OWGpr29vaybYGlcnvt2jWEhYWhoKAAwINHqBQKBbp27Qo/Pz8AQHl5OYCqwNhnn31W59+DGg+1Wo1169ZhwYIFuHbtGhQKBfLz83Hq1CkUFBSI4JREKqNHjhxBZmamGAiwtrbGvHnzEBgYCAcHB3Tr1g2vvfYahg8fLt4nBbJCQkLEZ7EjRQ9TU6elpmPdunXDyJEjxfPSLKpLly7hzJkzD/0MqQwGBASgffv2osxrNBqUl5fD2tqao//0VJw6dQrz5s3Dli1bUFJSgmvXruHixYsAdO/n0oY+f/75JxISEkT71MrKClOmTMH777+PefPm4dNPP8XMmTMBVAcU9PX1UV5ejg0bNjzz70hNx4OCSB4eHhgwYAAAyHKthoaGIjw8/KHnk5YN+vv7iw2xysrKoFAoMGXKFBEco7rDABZRA6dSqbBnzx5kZ2fDyMhIdLZ27dqF/Pz8GkeiCgoKcP78eQDVo7iDBg3CsGHDxHav0myD6dOnw8TEBEB1Y+PixYtIT08HwCUHVHcmTJgAJycnnenbp0+fxpUrVwA8fITKysoKnp6esLGxgZeXF1asWIEXXnjh6V84NWgVFRXQ19cXM1qkvH7x8fE4e/YsAHm50mg0yM3NRVhYGIDqOs7DwwOdO3dGZWWlmAVjbm6OefPmoVmzZmJJojQbIDg4GABn/lHdGT9+PCwsLHTK2ubNm0WQvybay7n79OkDIyMjVFZWwsXFBd999x3mzJkjlnQT1ZWkpCQsX75ctE+BqpmtwcHBKC4urnHw4M6dOyL4r1Qqoaenh3HjxmHAgAEwNTVFZWUlbGxsMG7cOAwePBhA9e7WQFW9e/v27UcuDyd6XJMmTRKD+np6erJZsMXFxQ98n1T/uri4oEePHtDT00OfPn2wfPlyzJo165lc+/OGrS6iBk5fXx8vv/wyFAoFysrKxPTXrKwsbNmyBYBupz8tLQ3p6enQ19cXN/127doBqM4pIL1n5MiR6NixIwCIabNA1a6FADtn9OSkhJhGRkaYNGmSOC7NMMjKykJISAju3LnzwHNIDdWuXbvi22+/xSeffPLA3WHo+WJsbIx+/frB09NT1qHJzs5GSEiITjBeqkulBqlUx0l1pfburWq1GsbGxiJQKs0YAKryW5SXl7MTRXXGzs4OEydOBFBd1pRKJTIyMkT+nweRymWfPn3Qt29fvPfee/jvf/+Lzp07P/XrpueTra0txowZI+pUaRZ/bGwsQkNDAei2T6Ojo5GXlyfaBQDErsXSMlepTp07dy4MDQ3FcantKuWF45IsqktOTk4YN26ceCxtApOUlCRSVTyIVP+OGDECP/30Ez766CO2UZ8i9kyJGpiaOkPdu3fH0KFDAVTfsBUKBY4dO4akpCSxtOX+c0i7EQEQnTjpsTRF1tjYGIMGDZId034dO2dUF6TyNGDAAJGsGKgOHpw7dw4xMTEPzL0mlXtjY2O0bNnyGVwxNWT310sODg7o378/zM3NZbsDXr9+vcaZUpmZmSguLoa+vr6o8+7evSvLhwVUl9vevXvDxsZGtqvhjRs3YGhoyE4U1anhw4ejffv2Ojto7tq1Czdu3Hjg+6SOv42NDd555x2xHIboaTE3N4eHhwdcXV1lx+/cuSMblJIStwMQmwRJ5buyshKpqakAIHbAltq0zZo1w5gxYwBArBwoLi4W5+IKAapro0aNgr29vZiFJd3ft23b9tBBVqmubtmyJZycnJ7JtT7PGMAiaiCkfABSZXl/B23ChAmicyZVlCUlJdi4cSMAeUJ3aUct7eWF0dHRuH37tixni/RZvXv3FktkpA6blAOLnTOqK1Jwavr06WK5gTTDQNrS+mEdNHp+FRYWYtOmTThx4gSAmnOtde/eHX369AEg34Xt/PnziI2NBVBdBjt16gQzMzNZYuvbt2/j6tWrNZ5f6mxp1592dnYAGOSnumVqaipLKCzVkSUlJdixY8dD38v7NT0NGRkZD3zOyckJ/v7+YumV1Ia8evUqTp8+DUA+eKA9eCXV06mpqbh3757seek8zs7OIjG2FODKy8vTOS9RXWjevLnYUEParV2pVCI/Px+7du2q34sjgf/ziepZeno6/v3vf2PBggX4v//7P5H88v6GqIODg2xpgfSa8+fPIyQkBEB158zd3R1GRkayYFd2drZstzbtLbyzs7NRUVEhm8nl7Ows+yyiJyU1SF1cXDBw4EBRtqS/Y2JicPHixYfmGqDnz+7duzFnzhxs3LgRP/74IzZt2oS0tDTxvFR+bGxs0LdvX9jb28t2V01LS0NwcDDKy8tFUL+yshLu7u4AqoNdd+7cwblz50QQtaKiQnyGubk5SktLZcusO3ToAIBBA6p7Pj4+8PHxASCfGX38+HFERUXV56XRcyQ9PR3/+te/8MEHH2DdunVi9pREo9HAwMAAbm5u8Pb2FscUCgUKCgoQFhaGa9euAahun7q6usLQ0FAEuzQaDWJjY5GQkAAAYgas9PqKigqUlZUBgNiNU1pySPQ09O/fH25ubqIdIbUxDh48KMop1S8GsIjqSWlpKZYuXYo333wTly5dAlC128XChQtx4MABZGVlAYBsSdXw4cPh5OQkRgSAqsbCli1boFKpxBbvJiYmYvmAVPEWFxfjxIkTYmt5KdmxWq3GuXPnUFpaKjpiZmZmcHNzE68jqitSeZ4yZQpatGghyqfUoN2zZ89DR3vp+REREYHXXnsNK1asgEqlgrGxMQBgy5YtWLhwIS5duiQC71IHv3PnzmKnSimAX1FRgejoaLGTkJ6eHszNzdGhQweYm5vLgl3nz5/Hrl27RMcMqJp98PvvvyM/Px96enpiabYUYCCqa3p6epg6daq4RysUCjH7ZMOGDaJDT/Q0aLdPY2NjUVxcjK1bt+Kbb75BdHS0eJ32bNR+/frB2tpa1j5NTU3FqVOnxE7YQFX7UjvvKlA1yHDq1CmxlFBaQSDl0wIgZsA6OjqKAVaip8HAwEA2C1ahUIhJAcuXL39gqgt6dhjAIqoHycnJmD59Ovbu3QsAYgctPT09lJWVYcWKFVi4cCFSU1NFA0GtVsPc3FxUqtJIlEKhQHJyMnbv3g2gapaLQqGAt7c3WrRoIWtM3L17F4sXL8aJEydQWFiI+Ph4bNy4Efv27QNQnafAz88Pbdq0eaa/E3o+SDNgWrZsibFjx4rgQ3l5OYyMjDBu3Dg2TgmXL1/G4sWLcevWLRgYGECpVIptqYGqoNJ3332HP//8Ezk5OSIAZW5uDi8vLzFCL73+9u3bOHPmDHJycsRneHp6irImBbuKi4tx9OhRfP7559i9ezc2bNiAFStW4MKFC2ImjKmpKYKCgpjngp4qZ2dnjB07FgBkudoSEhKQnJxcn5dGTdjFixcxa9YsWftUalfGxcXhu+++w/Lly1FUVCTalkDVzOp+/foBqE7GXlpaikuXLomBU6BqyWHXrl1hbGwsa5+eO3cOS5YsQVpaGjQaDa5du4a1a9eKZeNqtRoajQZ9+/aFsbExVwfQU+Xq6ophw4YBqOoblZWVwcjICH379pWVe6ofCg1rAKJnLiMjA99//z0SExNhYGAgglESadTVzs4O3t7eCAoKEhWmSqXCt99+i4iICLHLoEajgZWVFb777jvY2NgAAAoKCrBr1y5s375ddOKk8yqVSpibm4tR3NLSUtEB7N27N1577TXunkEy2vnZ6upcarUar732Gu7cuYORI0di9uzZMDU1rZPPoMbt3r172LZtG/bt2ydmWWkvnQaqR+87duyICRMmiIZlSUkJjh07hpUrV4rZUmq1GlZWVpg4caIICgDA0aNHsWPHDty8eVMEVx/WLFIoFBg8eDBeeOEF1pH01GVnZ2PBggViRraXlxeCgoLg6OhYz1dGTdXhw4exYcMG5ObmyvKoApDNdnV3d8fQoUPFjFcAiI2NxbJly5CcnCzqXaVSiX79+uHVV19Fs2bNAACJiYlYvXo1oqOjZRsTVVZWwtraGkZGRigrK0NRUZFonyqVSvj7++PVV18Vs3GJJHXZRpXcunULr732GgAgMDAQL774ItuoDQQDWET1QK1WIzQ0FN999x0AeY4L7c6Z1Gnz8fHB4MGDxZKVhIQEfPTRRyIfi9TpGj58ON544w3xOXfv3sXPP/+MqKgonQTE2mu7pWrAzc0Nc+fOFbldiAB5w6CuGglSw/by5cswNzfnbBbSERsbi7Vr1yIhIUFWV0n1orSlemVlJQwMDODv74/p06ejVatWuH37NpYtW4aIiAjRkQKqRlVfeeUVtGvXDkDV0uqzZ8/it99+E3Wj1FnSHliQ6sjJkydj6tSpYhMCIsn9uwbWlZ07d2LHjh1499134enpWefnJwKq7+15eXnYvHkz9u/fDwCy+lZ6rB3UmjlzJnx9feHk5ISioiIcOnQIa9asEe1TtVqNli1bYtq0aWJGCwAEBwdj69atSEtLEzMMpXyENW1o5OnpyZmvVKOnEbySBAcHo02bNmjfvv1TOT/9PQxgEdWT/Px8LFu2DMHBwbIglLSUUKFQyDpQhoaGmDJlCgYPHgxra2ssXboUe/fulY2QGRoa4vPPP0e3bt1EgEBaNijl2QLkDRKNRiN2PRo/fvwz/i1QQ6bdISspKcG5c+dgbGwsdnojeprUajV27dqFbdu2obi4WBbEsrW1RWZmpqjDpEGA9u3bIzAwEMOGDcOZM2ewdOlS5OXliY6UqakpRowYgTlz5sg+69ixYzh48KDYhVBiaGgIPT099OjRA7Nnz2bniXRod54qKyuRm5srcgHVRUBLe2c3omfh4sWLWLt2LZKSknQGOrVnVqnVaujr66NVq1Z466234OLigrt37+LXX39FVFSUbPDA3d0d8+fPh4ODA4Cq5OyXL1/Gf//7XxQVFQHQnV0LAEZGRpg5cybGjRv3LH8F1Ajc30a9cOECrK2t4eLiwnqziWMAi6gexcbGYuHChSgsLBQ3emNjYzg7O8PW1hbBwcGoqKiQNQK6dOmCF154AW3atME//vEP3L17V9bAcHd3xxdffAGguuFbWFiIyMhInDlzBunp6bh16xbat28PS0tLdOnSBaNHj4a5uXl9/iroCUgdqLrqMN0/mhUbG4vQ0FCEh4ejoKAAixYtYo40eqqkMpiamop169YhPDxc1rnx8/NDq1atcOfOHZw5c0ankzV+/Hh07doV165dw7Zt22TvbdeuHebOnQt3d3dZI7e4uBinT5/GnTt3cOPGDTg5OcHQ0BCurq7o1q1b/fwi6KmoixH7+8+RnJyM0NBQpKam4u2334a5uflTm5VF9DRIZbqsrAw7duzAzp07UVZWJgYIzM3N4eHhgcTERGRmZkKtVsPAwAAVFRWwtLRE7969MXv2bFy4cAHLly/HvXv3RPvV3NwcY8aMwfTp0wFUt0/j4+Nx/vx5nDhxAhUVFSgoKICjoyPMzMzQo0cPTJgwge1TknlQGzUyMhKGhob4/vvvYWBgwPq3CWMAi6gelZSUYMuWLdixY4esA+bg4IA5c+ZAT08Pq1evRnp6umy5jLGxMcaNG4eSkhLs2bNHVpFrNBq8++67GDhwoM4IhFqtRlFREcrKymBubo6KigpYWFjUx1enOlTTTbouOmhpaWmiUXD9+nWoVCpoNBr06dMHH3300ROdm6i2jhw5go0bNyI7O1s2k2r8+PEYO3Ys1q9fj7CwMNy5c0d0ppRKJSwsLODr64uQkBDk5+eL2Vr6+voil4qJiYls6SA1XXFxcViyZAlef/31Jx6h165fs7OzERYWhrCwMFy7dg1FRUWYOHEi5syZww4UNTpS2U5MTMTatWtx6dIl2QDAoEGD0Lt3b9y8eRPr168HIN/VumfPnnBxcUFubi4OHz4se2+nTp0wb9482SoBSWFhocj1ZmZmBgMDA1hZWT3jb0+Nyf1t1IqKCgBVu1zPmjXrqS4tpPrFABZRPUtNTcX//d//IT09Hfr6+lCpVFAoFOjduzc++eQT5ObmYvny5YiMjERxcbFsNpajoyNu3rwpO59Go0GbNm3w/fffw8jISDSg76/IWbE3ftLNu7CwEElJSbCysoK9vT2GDh0KW1tbAH/v3zk3NxdhYWGIiIhAQkICCgsLZUm0O3bsiK+++orJLOmpkspudnY2NmzYgKNHjwKoXsLSoUMHzJ8/Hy4uLkhKSsLPP/+MjIwMlJeXi7pUu07VXjZta2uLGTNmYODAgTqf+6A6kxqn7OxsLF++HGfPngVQlevxyy+/fOLzlpSU4OLFiwgNDUVsbKxsh0sA+PXXX5lsnRq1/fv3Y9OmTcjPz5fNpJoyZQrGjx+PkJAQ7Ny5Uyy91m4ntGnTBunp6eJcGo0GhoaGGDx4MF566SUYGBiI10rLwKlpyc3NRUlJCYqKimBoaAgHBwfx7/4k99bc3FyEh4cjPDy8xjaqp6cnPv74YxgYGNTVV6EGhgEsonpWUVGBw4cPY8mSJQCqE7qbmprixRdfRGBgIEpLSxETE4OlS5fizp07spwv2rRHuqZPn44ZM2ZwBLgJys/Px7p16xAcHAyVSiVypWl3zsePH4/AwMDHPndxcTEOHDiA7du3i7wUEktLS8ybNw8BAQF18j2Iais8PBzr1q1Damqq6EgpFAoEBgZiypQpsLKywu3btxEeHo41a9aIkVgpEKVNajx7eXnh1VdfhY2NDevJJmr9+vXYsmULAHnn+rPPPnuihOjXrl3DsWPHEBsbixs3bsieUyqVmDp1qlgqRdTYSHXk7du3sX79egQHBwOobp+6uLhg7ty5cHFxQXZ2Nnbv3o1Dhw6htLS0xjxWQPX/PwcHB7z44ovo27cvBwmaqIKCAuzduxeXL19Gbm4u0tPTYWFhAVtbW7z88stwcXEB8PcGWLOysrBnzx4EBwcjLy9P9pyVlRXmzp3LNupzQL++L4DoeWdgYABvb2+EhYUhKipKVOYlJSU4fvw4vLy8YGNjI/4+c+YMtm7dKhoH9+d+kY4dPHgQ/fv3h6OjIxsJTcjWrVuxYcMGnWVPUjJ/hUKBzMxMLFmyBM2bN4e3t/djjUIZGRmhefPmsg0EgKop2TNnzhTliAky6VmQ6i5XV1d4enri5s2bUKlUIoh14cIFuLi4wN/fH3Z2dhg7diwcHR1x4MABhIeHy2ZTSaT3xsXF4fDhw5g5cyaDV03MqVOnsHLlSuTm5opjGo0GNjY26NKlyxPVXWq1GtHR0Th16pTYXECqjwMCAjB37lwufaJGTbrP29nZwcfHB4mJicjIyBDHr127hnPnzsHR0RHW1taYN28eunfvjr/++gsxMTGyHQvvD2RlZWXh7Nmz6Nq1K6ysrNg+bWK2b9+OHTt2yGZFKRQK3Lt3DwUFBVi8eDGCgoLg5eX1t/7dzc3NUVBQgPz8fNlxtlGfLwxgETUA1tbWGDlyJGJjY2W7u6SlpeHQoUOYNWsWAKBdu3Zo164dWrVqheDgYMTExIjGgZQjS09PDyqVCnl5eTh16pSsQqfGKyQkBCtWrBA5IgD5dtbau1hK5Wf37t2wtbWFs7NzrT9HqVSiR48e6NWrF86dO4d+/fph3rx5aNmyJYDqRgEbBvQsaO+U6uPjg6tXryI2NlbUe7du3UJYWBg6deoEe3t7AICHhwe6d++O3bt348CBA2Jpl7S7q/R/pqioSJRrahoSExOxdOlSJCQkAKguP2ZmZnBxcYG3tzd8fHz+doBJo9FAqVSic+fOsLe3R1JSEiorK9GpUyfZzAK1Wi3KG1FjJLUnevbsiYSEBNy6dUvWPo2MjISLiwt8fX0BAD4+PnBzc8O6desQGhqK7OxsWXBXoVBAX18fFRUVCAkJQZ8+feDn58f/I03E/W1UaVBVe6d0hUKB9PR0HDlyBHZ2do+9GZBGo4GJiQn8/f0RFxeHrKwstlGfU1xCSNRA3Lt3DytXrsSxY8dkea4cHBzw/vvvo1OnTqioqBCzaYqKirBs2TJcuHABeXl5InkxUDVCMWfOHAwbNqzevg/VjdTUVCxduhQxMTEA5B16Ozs7tGjRAhqNBomJiSJPhRTMAoDZs2dj/Pjxj30zj42Nhb6+vqxDJgVIiepDZWUl/vrrL2zZsgWFhYUiv5WFhQWmTZuG0aNHA5CPvCYkJGDHjh0ICwuTnatfv36YO3cubGxsnvn3oLpXUFCAlStX4vjx4wCq60l9fX106NABnp6e6Nu3L5ycnABAzFp+kvps+fLliIiIwJQpUzB48GBxXqnTRtRUREdHY926dbhy5YqsfTpo0CC88MILsLGxEfVuRUUFUlJSsHjxYmRkZKCiogLGxsYoLS0FwPZpU/OgNur9+SeVSqWoH83NzfH//t//g4+Pz9/+3N27d6NTp05ih2C2UZ8vDGARNSDx8fH49ttvkZeXJxoJSqUS/v7+ePfddwHIG8jl5eWIj4/HsmXLkJaWBgCYMGECZs6cyeSFjVxZWRl+++03nDx5EgBko/kdOnRAr1694OPjA2dnZ2g0GsTHx+OLL75AaWmp6Dyp1Wq0a9cOP/3009++DnbIqCGQZgOkp6eLEX7tnVnd3Nwwe/ZsdOrUSWdJikqlwu7du/HXX3/BxMQEb775JlxdXevx21Bd2rBhA3bu3Iny8nIAEB0nR0dHuLm5oW/fvnB1da2zpSXSstT8/Hw0a9ZMdJi4ZIWaGqkuraiowK5du7B9+3aUlJSI9qm1tTWmT58uglHaKwJu3bqFiIgIrFixQhwfP348Zs2axfZpE1BcXIw1a9bgwIEDACALVLVv3x7du3eHs7MzcnNzsXr1ahH0lMqOlKf3cZeQ3p+vkm3U5xOXEBI1IB06dMDw4cOxefNmkc9IrVbj0qVLCA0NRZ8+fcQNAqjKn9WzZ0989tlnuHjxIrp3747WrVvX87egJ6XRaHDu3DkRvDIwMIBKpUJlZSXs7e3x2muvoV27dtDXr6rCKysr0a1bN7zwwguisSiVk5SUFFy7dg2dOnX6W9ciLUkkqk9SA9fR0RE+Pj5ISkrCnTt3REM2Pj4e4eHhcHJygpGRkWzHQX19fUycOBF9+/YVywypacjNzcWWLVug0WhgZGSE8vJyqFQqGBsbY8yYMQgICICJiQmA6hF6qT7LyMhATk4OOnToAFNT01p3pKQy17x5c3FeLlmhpkiqQw0MDNC7d28kJCTg/Pnzon2anZ0tlnC3b99evAcA7O3tMXbsWLRq1QqJiYkYNGgQHBwc6vPrUB2JiYnBJ598Ih7r6+tDrVbDwsICAwYMQK9eveDu7i7qypycHOzevVt2Du1lhY/j/uAV26jPJ86zI2pAjIyM4O/vj3bt2skCVXl5eTh48CBKSkpkCYmlit/GxgbDhg1j8KqJUCgUcHNzg5+fHwDIdpu8deuWLJcEUH1Dl3K7SK9Xq9UwMzODkZHRM/4GRHVPqvfc3d3h4eEBALJlKxEREYiNjQVQXTdq/83gVdNjZWWF+fPnA4CYgaWnp4fS0lLk5OTAxMQEarValBM9PT0UFBTgxIkTWLp0KVatWoWoqCgAj9+RkrDzRE2Z9P+iffv28PHxEWkLpHZHXFwczp8/j4qKCtn/Iam+9vX1xaxZsxi8akLKy8tha2sLoDp4Jf17jxw5Eh4eHtDT00NZWRkAYPjw4QCqy4SVlRX69ev3xNfB/GnPLwawiBoYOzs7jBo1CkDV0hcpYHX16lUcO3asnq+OnhVLS0sMHToUpqamYuaANOPq119/BQCdafj37t0Tr5UYGxvD1NT02V040VMilWsrKyv4+PigQ4cOsudTU1MREREhkrbT82HgwIHo3Lmz6FRL5WTHjh1ITEwUs6NUKhUuXLiANWvWYOPGjbhw4YLYTS0jI6OevwVRwyUFHqSZNUD14EFJSQkiIiJw5coV2XsYXGh6pHLQtWtXDB06FADEjtUKhQIFBQU4dOgQAKCiokIMnpaUlMDMzAx6enpQq9VQqVTYsmULtmzZgpiYGJSUlACAWGZI9CgMYBE9BU+SWk5fXx+enp7w9PQEUL2Eq7S0FLt27RIzcKhpUygU6Ny5s0gODFQ3GJOSknD06FEAVbmypPKQmJiIwsJCANWzsvr168ed1qjB0Z5V+DikurV79+7w8vKCoaEh1Go1jIyMUFlZiaNHj+Lq1at1eanUwBkbG2P69OkAqupIKR9KRUWFWLaSmJiITZs2Ye3atTh+/DgyMzPFzKlz586hoKCg3q6f6Fn6O+1T7dn+Pj4+st3jFAoFEhMTcfLkSf4/auKkcmBqagoPDw+RS1J7Wd/hw4eRlJQEAwMDlJeX4+LFi1i+fDkKCwvFkr/CwkKcPXsW69evx1dffYWffvoJFRUVnM1KtcYAFlEd086j8XcDWS1atEBgYCCMjY3F8ofKykq0adMGxsbGdXm51ICZmppi0KBBsLOz00lcuXLlSpSXl4sRrv3792P16tWy5YPm5ubo27cvgCcLqhLVFakcSturZ2dnA6h9QEvKyWJkZAQvLy/RgC4rK4OhoSFmzpwptnWnxi8vLw+lpaW4ffs2iouLxfH76zNPT0/0798fQHXZAoDg4GD8+uuvWLNmDf766y9cv35dVkd6eHhg0aJFYrdVoqbscRNm3/9eAOjRowd69+4NAwMDseOxNKtGmiVOjV9WVhbS09Nx6tQphIWFieWAUv3p5OSEgIAA0U+RZr8WFxdj8+bNyMjIwPbt27Fu3TokJCTIglNSOZQ2CAgNDcWyZctEWgyiR2FNQ1RHpACDQqFASUkJzp07B2NjY/Tp0+exz6VQKNC1a1f0798fR44cga2tLebOnfu3zkWNW5s2bRAYGIgVK1aIhoNSqURhYSF27NiBfv364fvvv0dycrJ4j0ajQevWrfHqq6+ia9euADidn/6+J+n03H8O6TzJyckIDQ1Famoq3n77bZibm+sEaR9EOkenTp3Qo0cPxMXFoV+/fnj55ZdhZmb2RNdJDUNWVhYOHjyIlJQU3L59G3fv3kXbtm3Rr18/DB8+HMbGxjrlcvLkyQgLC0N5eblsy/Zjx47JlqZoNBrY29vjpZdegpeXFwDdna2ImpL726cXLlyAtbU1XFxcar17pvR/zczMDN7e3oiLi8PVq1fZPm1irl+/jpMnTyI9PR0pKSnIzs4Wg+c9e/YUaU0MDAzg5uYGLy8vnD59WpabNzw8HJmZmcjKykJRUZEYeAKqdyGU8vxKxw8dOoRevXpxAIpqRaHhsDw9Z6RGb101WO9vRMfGxiI0NBTh4eEoKCjAokWLZNOtH8e1a9dw7do1BAYGPvF1Uv3KyspCREQEPD09RfLL2srMzMSPP/6Iy5cvyzpm2rRnpri6umLEiBGicyY9T1RbcXFxWLJkCV5//fXH6uTURLuOlHatCgsLw7Vr11BUVISJEydizpw5j1UnS+fMzs5GZWUlbGxs/ta1UcOiUqmwceNG7N27F6WlpeK41OkBgICAALzzzjs1lsd169Zh69atUCqVYnaIdufJwMAAM2fOxIQJE8R7nqRsEzVkD2qfRkZGwtDQEN9//72YSfU47WG1Wo2//voLCoUC48aNexqXTs9YdnY21q5di5CQEKhUKlHfSvXnyJEjMXXqVJHEXypX586dw5IlS5CdnS2rd+9nbW2N8ePHw9jYGNevX8eBAwcAQARW1Wo13N3d8cUXXzy7L02NFgNY9Nyp6UZdFzMM0tLSRMPg+vXrUKlU0Gg06NOnDz766KMnOjc1bhs2bMDmzZsBAO+++y78/f0fq8OkVqsRGhqKRYsWiUTFUjnWaDSi/LZp0wYeHh7w8/NDp06dntbXoSYsOzsby5cvx9mzZwEAbm5u+PLLL5/4vCUlJbh48SJCQ0MRGxurk2j9119/haOj4xN/DjVeR48exapVq3Dv3j0AEJ0h7QCU9Pfrr7+OYcOG6dzL8/Ly8I9//EPkuJIC/RqNBn5+fnjjjTfELD0Gruh5cX/7VFqqNWXKFMyaNeux2sB1PQhM9W/79u1Yv3692ARIOzSgVCqhr68PCwsLvP/+++jWrZvsvXl5edi+fTv27NkjS58inUdPTw/Tp0/HtGnTZO9bsWIFdu/erVNPL168GE5OTk/5G1NjxyWE9NyQbuCFhYVISkqClZUV7O3tMXToUDEj5u8EsnJzcxEWFoaIiAgkJCSgsLBQdgO4c+cOiouLuRPcc+jMmTNYtmwZcnNzxbHTp0+jQ4cOaNu2ba3Po1Qq4ebmhv79+yM4OFgsxZLKmIWFBXx8fODh4QFPT0+RF6suArP0/Fi/fj22bNkCoHrUNTo6GpGRkWJTib/j2rVrOHbsGGJjY3Hjxg3Zc0qlElOnTmXw6jl2+fJlLFu2DElJSQCqy540A0A7b5q0i9W+ffvg7++vc1+1tLTE1KlTsXjxYp1Zqvb29jAzM4NKpRI7ExI1BLm5uSgpKUFRUREMDQ3h4OAAAwODJ76H5+bmIjw8HOHh4bL2qUQKZt2/o/HDSO9n8KrxS05Oxpo1a3DhwgVxTKPRoFmzZmjevDm6du2KPn36wNDQEHZ2dmjRooXOOSwtLeHr64uYmBgkJyfLAlJGRkZYuHAhOnbsCACyXFljx47F4cOHUVJSIlueSlQbDGBRk5efn49169YhODgYKpVKtuWrRqNBcHAwxo8fj8DAwMduKBQXF+P48ePYvn07ioqKxHGNRgNLS0vMmzcPAQEBdfp9qPEIDQ1Fbm6uaIhK27j36NEDtra2j5WQ38LCAsOHD8fFixdx79492VRtpVKJoKAgmJubA9DNN0T0MKdOncLKlStlgVaNRgMbGxt06dLliTr6arUa0dHROHXqFIqLi2UJtgMCAjB37lxYWVk98XegxicnJwcrV65EcHAwgOrk6/r6+nBycoK9vT1sbGyQkJCA+Ph42ayPtLQ0REREICAgQKeTHxAQgBMnTiA2NlYsudZoNNi2bRt69+7NhO3UYBQUFGDv3r24fPkycnNzkZ6eDgsLC9ja2uLll18WZfXvBLKysrKwZ88eBAcHIy8vTxzXaDSwsrLC3Llz2T59DkllKTMzEzt27EBMTIyYzW9kZIQuXbrAw8MDPj4+sLe3l71X2lRKahNI5+rQoQP8/f2RkpIiZnEBVZurHDt2DB07dhTBK+k9xcXFMDExkQWw9PX1GcSiWmEAi5q0rVu3YsOGDaLDJI0YaS9NyMzMxJIlS9C8eXN4e3s/1kiUkZERmjdvLoJikilTpmDmzJmiUuZSheeHlJ9KCiolJCTg7t27UCgUIo9LcHAwunTpojMV+1GcnZ0xbNgwbN++XZbQPTc3F2fOnMGIESMeezSVnl+JiYlYunQpEhISAFQH9c3MzODi4gJvb2/4+Pj87QCTFFzt3Lkz7O3tkZSUhMrKSnTq1EnWOdMelaXnw4kTJ/Djjz8CgKxutLGxgb+/P9zd3eHm5gagahDqk08+QVpamuw+mpycjICAAJ1yY2BggGnTpiE2NlaULen827Ztw8cff8z7MdW77du3Y8eOHbJZ+wqFAvfu3UNBQQEWL16MoKAgeHl5/a260dzcHAUFBcjPz5cdZ/v0+aZQKFBeXi4G9rX5+Phg/Pjx6NChAwCI4L+enp6sbVlUVAQjIyOx66SJiQk8PDwQHR2NixcvisEIhUKBo0ePws/PD926dZOd68SJE8jJyREDEwDg7u4OS0vLZ/fLoEaL8z+pSQoJCcHLL7+MdevWiYpRu5LUTjIojdDu3r0bqampj/U5SqUSPXr0QK9evQAA/fr1w/LlyzFr1iyRlFB6HTVt586dEz9LAVIbGxtZAn6p/F2/fh1hYWGiYVnbVITGxsYYMGAA2rRpIxoCkpUrV6KkpEQkZCV6kIKCAvz000/44IMPkJCQIDoyUrBp7NixCAoKwogRI2BlZVXjpgG1IZ23R48e6N69O+zt7fH//t//w6JFi+Di4iKWiSmVSgavnjN3794Vy//09PTEIJClpSUmTZokglcVFRVo3rw55s6dC0B+735YYNXNzQ1DhgwBIL/3R0REyOpqomdNap+uWbMGhYWFon2o3U5UKBRIT0/HkSNHdJZd14ZGo4GJiQn8/f3RsmVLAGyfUrWTJ0+KdBTSv//o0aPx3nvvieDV/fkHpeDVjh078MILL2Dfvn0Aqtuvbdq0Ecu6pYEDoGoWlpQDVqFQICEhAStXrsTBgwdl1+Tq6oqgoCCWR6oVzsCiJiU1NRVLly5FTEwMgOoZBaampmL9tkajQWJiIvLz82XLWa5cuYKoqCi0b9/+sSpQW1tbjBkzBhMmTJDNKNC+MVDTFRYWhuXLlyMzMxP/+c9/0LNnT9nNe9y4cTh9+jSSk5NlMw1Onz6Nbt26wcfH57E67/b29ggMDMSff/4pypm+vj5KS0uxevVqvPbaa0/rq1ITsGHDBuzcuRPl5eUAqqbsq1QqODo6ws3NDX379oWrq6vO6PzfDTBJS74mT56MuXPniv8X0nlZRz5fpFkmfn5+SE1NxenTp2WJg2/fvo2wsDAMGjQIarUa+vr6UKvVaNu2LWxtbZGZmQmgKlfKozaqmDBhAkJCQlBcXCzKmVqtxubNm+Hu7s6lKvRMPah9Kv2tnRJAmvkSFxeHjIyMx97JWqqvPT09MWbMGHTq1EnM+Gb79PmlVqtRVFQkgk/SMWtra4wZM0YENrV3u5bKyblz57BixQpRB+/btw/9+/cX/SqlUglXV1f4+vri+PHjsskDly5dws6dO2FsbIzg4GAkJSWhrKxM9MEcHR0RGBgoBsw4oEWPwgAWNQllZWX47bffcPLkSQCQLUfp0KEDevXqBR8fHzg7O0Oj0SA+Ph5ffPEFSktLZQ3b4OBgTJo06bE/39XVFQB0KnxqutLS0rBs2TJERUWJY6tWrcIPP/wgK1P6+vqYMmUKvvvuOwDVHfrs7GycOXMGbdu2hZ2dXa0/18DAAF5eXggLC8OlS5dkQdgDBw4gMDAQTk5OXBZAOnJzc7FlyxaR66K8vBwqlQrGxsYYM2YMAgICYGJiAkC3k5ORkYGcnBx06NABpqamtW5kSgGr5s2bi/MycPX8ksqMvb09vLy8cO3aNdy+fVskZy8sLMTBgwfRq1cv0ZlRKpXIz88XuVI0Gg06duz4yACWo6MjJkyYgPXr1wOorntTU1Nx6NAhTJw48al/X6Li4mKsWbMGBw4cAFAduNLT00P79u3RvXt3ODs7Izc3F6tXr5bNjCosLMT169fh4+Pz2B17qbyPGzcOANunz5uioiKYmJjIZuorlUrcuHEDGRkZ4rhGo4Gnp6doh94/IzAtLQ1Lly5FdHS0OK6np4fMzEz89ddfmDNnjjh/q1at0K9fP8TFxensBLt582YYGBigoKBAlOPKykp0794dc+fO5c7Z9Fi4hJAaPY1Gg3PnzonglZQwW61Wo1WrVnjttdcwbdo0ODs7A6iqMLt164YXXnhBvF+6qaekpODatWt/+1o4qtW0SeVq5cqVePvttxEVFSX+zZVKJa5fvy6mRWsHkPr164fevXuLqdbSzTs8PBzR0dFiS+vasra2xsiRI8VsLgAiF8GSJUsAcFkA6bKyssL8+fMBQMzA0tPTQ2lpKXJycmBiYiJL0qqnp4eCggKcOHECS5cuxapVq0TA9u+OkLJcklQPuru7w8PDQ4z6Swl+U1NTRT2qUCgQGxuLJUuWoKCgQBwzMTHBzp07ERISIo4D0FnqOnLkSDg6OurkWdu+fTvS09Ofxdel51hMTAxmzJghglfSfdrCwgKjRo3CzJkzERQUBH9/f4wbN07MgtGuX7VnsjwO7cCFFPxi/dv0lZaW4ocffsA333xT49L/qKgocf+XyoUUPJLak0BVufvll1/w9ttvIzo6WpSfyspKcd49e/bg+vXrsmXanTt3hp+fn+x8CoUCpaWluHfvnvhcExMTvPbaa/jmm28YvKLHxgAWNXoKhQJubm6iwtSusG/duiWWWElBAummLiUnll6vVqthZmYGIyOjZ/wNqLGQdk6RGqNSTgDtm/7q1atRXl4ubvTSc9OmTROzW6SAaWlpKU6fPv3IHBf358jS09NDt27dMHDgQACQjXLFxMQgJSXlyb8sNUkDBw5E586dxQwAqVO0Y8cOJCYmimCstGPmmjVrsHHjRly4cAHXrl3DuXPnkJGRUc/fghozqcw1b94cPj4+Yot1SWlpKc6cOYOwsDDs378f69atw9WrV8X7NBoNLly4gPXr1+O///0vPv30U+zZswcAZLMKAKBZs2aYPn26zjUUFhaKGQVET0t5eTlsbW0BQCyHlcrmyJEj4eHhAT09PZSVlQEAhg8fDqC6/FpZWaFfv35PfB1ckvV82L9/P4KCgnDy5EmYmJiIgClQnZBdWgIotQE0Gg0KCwsByAeY9PT0xM7E0uukP9JusSqVSszqlupeCwsLeHt7i6CUnp6eTr08evRorF69GiNHjnzKvxFqqhjAoibB0tISQ4cOFckDpaAVAPz6668AoLMz271792TbvQJVSbKlxLJE96usrESzZs1EgmHt3SelG3pxcTHWrl0LoHoXNo1Gg06dOmHw4MHiBi79HR0djfPnz6O4uFh2XKK9bCA3NxdZWVkAqjpmQ4cOhbW1NSoqKlBZWQlPT08sXrwY7dq1e3q/BGrUjI2NRYderVaLYGpFRQV2794NoGp3wk2bNmHt2rU4fvy4WAoAVOXB0J7xQvQkunXrBi8vLxgbG8uWN926dQtLly7Fpk2bEB8fL+v4KxQKqFQqKBQKsSRw+fLl+O2333D58mWdz+jbty+8vLzE7EInJyd8/vnnsg02iOqSVFa7du2KoUOHAqhuLygUChQUFODQoUMAqjYqkAZOS0pKYGZmJpbUSgGCLVu2ICYmBiUlJQDkg2ZE5eXl+OCDD/Dnn3+KMnJ/jj9pZp8UTNLeKTA/P18EUYHqiQBSTlVjY2MEBATA29tb5zWhoaEIDw+XHWvfvj0CAgJgYGAg6l2NRoNevXrh119/xSuvvMLJAvREmAOLmgSFQoHOnTtj8ODB+OuvvwBUL+FKSkrC0aNHMWTIEJSVlYlKMzExUYw6SI2Ffv36iR1biO6np6eH8vJy0QCoKQkrUDWteuTIkXBwcBBBUoVCgSlTpiAsLEwEobQTuru4uMDNzU02y0B6X3FxMSIiIhAWFgZbW1uMHTsWVlZWcHJygp+fH06dOoW33noLXl5e9fOLoQYhLy8PxsbGyMvLg4WFhQjG3587xdPTE/3798fp06dlOdSCg4NhbGyMzMxMXLlyBaWlpbJk7h4eHpg3b95jJxQmqolGo4GhoSG8vLyQkJCAixcvig6VWq0W9aTEwMAA+vr6KCsrE68Dqu/fR44cQWhoKN588010794dzZo1E+2A8ePHIyYmBrNmzcKYMWPq4+vSc0Qqm6ampvDw8MClS5cQGxsrq28PHz4Mf39/dOzYEeXl5YiLi8PWrVtRWFgo7v2FhYU4e/Yszp49C2NjY/Tq1QsffPCBzoAsPd/u3buH7OxssQu1Wq2GpaUlAPn9X6PRwMbGRtSZUnlMSkpCSUmJ6B9Jx21sbPDOO+8gMzMTw4YNg7W1Nd5++22kpaUBqN4EZvPmzejRo4docxgZGaFXr16IjIzExYsX4eDggHnz5rGNSnWGASxqMkxNTTFo0CBERETg9u3bsqmwK1euhL+/v6ic9+/fjzVr1sgay+bm5ujbty8A3Q4fPZ/uLwdSh8vR0RFhYWHimDTKpD2tetmyZfjss89kCd0tLS0xfvx4LF26FED1NsVpaWkIDQ2Fo6MjWrRoAaA6OBYTE4OQkBDExMQgPT0drVq1Qtu2bTFgwACYmppi6tSpmDdv3jP+zVBDkpWVhYMHDyIlJQW3b9/G3bt30bZtW/Tr1w/Dhw+HsbGxTlmePHkywsLCxHJXKQB77Ngx2ei+RqOBvb09XnrpJdH4lJIDEz0JqTx27NgRPj4+SElJQW5urk7uHgAIDAzEiBEjUFJSgvT0dOzduxfJycmyQL80s+WXX36Bq6sr3nrrLdGhcnV1xbp169jxpzqXlZWFsrIyJCUlwdjYGO7u7jAyMhL1pJOTEwICApCYmCg2DqqsrERxcTE2b96MoKAgnDp1CufPn0dycrIsJYB2vV1RUYHQ0FAsW7YML7/8MssyCUZGRigrK0NFRYVoc96fc1X62cLCQsx21U49ER8fjz59+ojXSvXwoEGDZJ81fvx4/PzzzwAggmBJSUk4ePAgJk6cKMq9vb09Bg8eDC8vL4waNerpfXl6LjGARU1KmzZtEBgYiBUrVoiKWdrJZceOHejXrx++//57JCcni/doNBq0bt0ar776Krp27QqA+QKeZ2fOnIGxsTF69+4tm2Glzd7eXgSqjIyM4ODggMzMTBQXF4sZV5GRkTh//jx69+4tbvJA1dr/U6dOiZwu0khYSEgIXF1dRb6L5ORkhIWFiUattGTmzp07OHPmDHr27AkrKyuYm5s/898RNQwqlQobN27E3r17UVpaKo4rlUpcuXIFV65cQVJSEt555x2d5L3t2rXDuHHjsHXrVgDVjV3tHIIGBgaYOXMmJkyYII5xd0uqS1L96uHhgfj4eJw6dUo2u0qpVGLixImYNWuWeI+Liwt69+6NjRs3Ijg4GMXFxaIzJuUWDA0NRUpKCl5//XX07NkTgG4aAaIncf36dZw8eRLp6elISUlBdnY22rRpA2NjY/Ts2VPkDTIwMICbmxu8vLxw+vRpWWAhPDwcmZmZyMrKQlFRkWhzANUztLVTEQDAoUOH0KtXL/j6+tbbd6eGJSMjQ5ZkXaFQwNraGkD1gJNU1wYEBGDt2rU6u7Bv3boVXbp0EYOo96uoqICBgQEGDx6Ms2fPIjIyUvaZkZGRGD16NAwNDUWZ9fPzY3+KngoOoVKDlJWVhf3794tkg7VlYGAAX19fdOvWTZagEAA2btyIt956C8nJyaJCNTIygqenJ4KCgtCzZ0+RoJCeP5cvX8YHH3yA7777Dj/88AM2b96sE7ySfjYwMBABJY1Gg4CAALRt21a8TmoULF++XPZYmtkydepUcUxqXOTl5SEkJARxcXE4fPgwVq9ejd27d+PatWvifRqNBp07d8asWbNgZWX1lH8j1JAdPXoUQUFB2LZtm2iISgFR7QDAqVOncOTIkRp3Ixo9ejRsbW11dmgDAD8/P6xevVoEr7S3dieqK1KZs7W1ha+vLxwdHcVxaVZgVFQUbt++DaA6b5ulpSVeeeUVvPvuu6IMS7vEVlRUwMbGBvPmzRPBK6K6kp2djR9//BEfffQR9u7di8jISGRnZ0OhUODGjRsIDQ1FTk6O7D12dnbw8/ODtbW1bAarRqNBSkoKioqKxGNra2u89NJLeO2110SSa+36V6PRiI1k6PmRnZ0ty7uq3VcxNzcXA6hSv0e652unvJDyuGrn/5PaC0lJSdi9e7dYuq3d7qysrBQDALm5uSJnq0KhEBtk+fr6ivr8/r+J6hpnYFGDs2HDBmzevBkA8O6776Jly5aP1Wlq2bIlRo0aJRK/aics1A5QOTk5wcPDA35+ftzC9TmWk5ODlStXIjg4GEBVA7GoqAgbNmxAfn4+RowYAScnJ1F29PT04OLiAhMTE5SUlKC8vBxlZWWYP38+3n//fdGJUiqVyMjIwO7duzFu3DhZgmIvLy/06dMHZ86ckW2ZHRoaiszMTOTk5CA7O1tco9SonTdvnthtk55Ply9fxrJly5CUlASguqOv3dgEqnf+UavV2LdvH/z9/XU2qLC0tMTUqVOxePFinQCXvb09zMzMoFKpxM6ERE+Tm5sb4uPjcevWLVGHqtVqpKSk4NChQ5gzZ46sHOrr68PHxwc2NjY4cuQI9u/fD6VSiVmzZmHixIn1+E2oqdq+fTvWr18vZlprBxH09PSgr6+P8+fPw9/fHy1atJB14F1cXNCvXz/s2bNHVt9K59HT08P06dMxbdo02WcaGhpi9+7dIgChUChw6dIlpKWlwcnJ6el/aapXubm5+PPPPxEbGwtfX1+MGjUK7du3l5WtW7duiVlPUlvAwcEBgHwZqtQf8vf3x6lTp5CdnS0bYD148CAKCwvx8ssvi12zpTZqSUkJLly4gGPHjiEhIQFAVfDL1NQUL774IjfFoGeKASxqMM6cOYNly5aJbVsB4PTp0+jQoYNsdsujKJVKuLm5oX///ggODhaVr9TQsLCwgI+PDzw8PODp6SnyYjHv1fPnxIkT+PHHHwFATIWWgp5Sxz82NhbvvfcenJycxI2+tLQUVlZWKC8vh1qtxqVLlzB9+nQEBATgxIkTsnK0bt06DBkyBGZmZqisrBSNjGnTpuHSpUsoLCyU5WJLTEyUJYZXKBSYPn16jVvB0/Pj/kCrNNNKX18fTk5OsLe3h42NDRISEhAfHy86O3p6ekhLS0NERAQCAgJ06jmpzMbGxorRfY1Gg23btqF3795wcXGpr69Mzxlzc3P4+PiI5a/SPbu8vBwhISHw8fGBi4uLzjLWDh06YP78+XB1dUWPHj1gYWFRX1+Bmqjk5GSsWbMGFy5cEMc0Gg2aNWuG5s2bo2vXrujTpw8MDQ1hZ2dX4zIsS0tL+Pr6IiYmRifXlZGRERYuXIiOHTsCgGxW7NixY3H48GGUlJSIuvv+HeaoadqyZQs2bNgg7ttHjhzBqVOnMGLECPj4+MDV1RVAVZBTar9Kf0uz+mrq1zg5OWHSpElYsmSJbKZWaWkpjh49ipSUFHh5ecHFxQWtWrVCbGwsoqKikJycjIyMDHE9LVu2xMiRIzF48GAA7EfRs8MAFjUYoaGhyM3NhYGBATQaDVQqFS5cuIAePXrA1tYWxsbGtT6XhYUFhg8fjosXL+LevXuioSAFD4KCgkTuIO0ksPR8uXv3LkxNTVFcXCyCVtJ0ae0t2hctWoQhQ4aI5VRt2rRBRUWFGOmSRrUmT56MyMhIFBQUiOBCeXk5Vq1ahTfffBNA9RIAJycnDBs2DDt27BDXoz0SptFo0L9/f7z00ktcLvice1Cg1cbGBv7+/nB3d4ebmxsAID8/H5988gnS0tJknfzk5GQEBATo1HMGBgaYNm0aYmNjRadJOv+2bdvw8ccfc/YVPTMuLi7w9vbGjRs3RG4raUfC/fv3w8XFRac8SvdwKX8gUV2QylVmZiZ27NiBmJgYMZPfyMgIXbp0gYeHB3x8fGBvby97r1qtlgVapXN16NAB/v7+SElJEbO4AKCsrAzHjh1Dx44dZcvApF2IpRnf0uv19fUZxGrCiouL8c9//lPk69XX15cF9P/66y8cOHAAo0aNwvDhw8WsVakdC0DsFlxTUEmpVCIwMBAXL15ERESEOC4NnCYmJiIxMRH6+vpi9+37c8I2a9YMY8aMwfDhw8VEAPaj6FlhDiyqV9pLX4KCgtCyZUsRGJA6+sHBwbh+/fpjn9vZ2RnDhg0DAFlC99zcXJw5cwZAVVJCVrjPH6kh4OfnB09PTwDV6/2l56WyqVAokJ6ejlWrVmHjxo1IT0+HUqmUzUy5du0a7t69C0dHRwwfPlwc194uOzk5WRYkA4BJkybB3t5eNvVbrVajY8eO+Pbbb/Hhhx8yeEUi0ApUBUulPBiWlpaYNGmSCF5VVFSgefPmmDt3LgCIoD2Ah5YjNzc3DBkyBEB1ngwAiIiIwLlz557OlyK6j0ajgb6+Pry8vNC9e3cAkM1OjY6ORkhICAB5fc17OD0NCoUC5eXlWLduHYKDg1FRUSHqVB8fHwQFBWH8+PHiHi7Vm9JOcFI6AilfJgCYmJjAw8MD7u7uAOT5iY4ePYrLly+LoJeenh4qKipw4sQJ5OTkiLq5srIS7u7usLS0fOa/E3o2TE1N8f7772P+/Plo06YNVCoVKisrZYNMFRUV2LVrFz755BMcP34cAERbUqlUoqSkBEDN9aMUiHr99dfh7u4u+lvSwK12jjapPEvn0Wg06N69Oz7//HOMGzfusSYXENUVBrCoXmh3iqTZUTY2NjqJBYGqnV7CwsKQn58PQJ648GGMjY0xYMAAtGnTRuQXkKxcuRIlJSUwMDCoMbkxNW3Sjdje3h5eXl6ws7MTx6U/nTp1goWFhezmv337dvz888/IyspCeXm5eI+pqSnS09MBVM3CcnR0FEEwff2qia7Lli0DUFXepUCWubk5Jk2aBKBqR7lmzZrh7bffxv/+9z+xIyY9vx4UaJVGQm/fvo2wsDBxXF9fH2q1Gm3btoWtra0og2ZmZo/M8zdhwgSYmprKGsgAsHnzZrEUgehpkuratm3bwsfHB9bW1uLerVQqkZeXhy1btoiZWURP28mTJ0UqCqnMjR49Gu+99x46dOgAoDrIKtXXUrLrHTt24IUXXsC+ffsAVNfnbdq0ETkJtfO0lpWVifyvCoUCCQkJWLlyJQ4ePCi7JldXVwQFBfH/QBPn5OSEwMBAfPXVV5gyZYpIpSIFsqS2anZ2NuLj48X7pOC+tAuh9B5tUjDU2toab731FqZNmwZjY2MRIJVeLw24Sn9at26Nd955B998841Y7kpUHxjAomcqLCwMr776KhYuXIioqCgxsio1XMeNG4f27dvLthEGqnJhSUkDH2e01d7eXgTFpN2L9PX1UVpaitWrV9flV6NGRipj7u7u8PDwkCVIlQJPI0eOREBAgHhPZWUlrly5giVLluDOnTvitXl5eTA0NARQFTidPHmyeI/UQI2NjZXNHpAarUOHDoWLiwvGjRuH1atXi5kwRA8KtEplp7CwEAcPHkRubq6oK5VKJfLz88VyE41Gg44dOz4ygOXo6CiWyALVu2Ompqbi0KFDT+PrEemQ6mUPDw/06tVL1MtSR6pdu3accUVPnVqtRkFBgQg+Scesra0xZswY0XYFqndpk+rgc+fO4dVXXxVtzH379okZVNIMGVdXV/j6+gKoDi5Iydl37tyJAwcOYPXq1Th69KhIcaDRaODo6IjAwEBYWVlxx+zngEajgaWlJWbNmoWvv/4aU6ZMgYODg2xTIe2BVwBiNv+ff/6JvXv3ytqb2qRjNjY2mDZtGr766iuMHTsWXbt2hbm5OVq2bAkDAwMxQ/uNN97A4sWLMWjQoGf6OyCqiULDGpCegbS0NCxbtgxRUVHiWIcOHfDDDz+Ix9KywbNnz+K7774Tx6UGrL+/P2bOnCk6cbWVlZWFX375BZcuXRLTZKUGw+LFi+Hk5KSTFJaeL5cuXcLatWuRmJgo28Vl5MiRGDVqFM6cOYPt27eL7YK1R1ul18+YMUOWaP2zzz4TQVqlUgmVSgVra2usWLECAGQ52SoqKsSoLZE2aVp/fn4+Nm3ahAMHDoiGa2VlJYyNjTF+/HjMmDEDABAbG4t169aJRO4A4O3tDWdnZzg6OsLV1VUkudbezh0A7t27h48++kgskwUgZgr+97//haOj4zP+9vQ8i4iIwLJly3D79m107NgRL730klhaSFRXioqKYGJiotPJj4uLw+effy6WbFdWVmLYsGEin+X90tLSsHTpUkRHRwOonm1dUVGBiRMnYs6cObIcQufPn8eSJUuQmZkpy9NqYmICAwMDFBQUyNoa3bt3x9y5c7lr9nNI+16dnZ2Ns2fPYufOncjPz5flTJNo/+zm5gY/Pz8MGDAARkZGOjmxasqRVVhYiNzcXJibm8PMzEzkfSNqKJjEnZ4aKVC0Zs0a7Nq1CwDE+mqgamngwYMHMWLECFkAqV+/fjhx4gQiIiJkowrh4eHo0aMHrK2tH6uzb21tjZEjRyImJkZU9Pr6+lCpVFiyZAm++uorBq+ec926dYOXlxfS09NRWloqglLh4eHo1q0bZsyYAQcHB6xduxZZWVkAIBoM0qwtqZxKwajp06cjPj5e5HQzMDBAdnY2NmzYgBdeeEEErwAweEUPJJWr5s2bw8fHB1evXkViYqJ4vrS0FGfOnEGHDh2QnZ2N4OBgXL16VZav4sKFC2KpYbt27TB48GCMHTtWludCoVCgWbNmmD59OhYtWiS7hsLCQkRHRzOARc9Ut27dMHDgQLRs2ZIzU6nOlZaW4vfff8fdu3fxxRdf6ASwoqKiUF5eLkuoLgWPtNuslZWV+O2333DkyBEA1e1c7WVbe/bsQf/+/dGhQwcRjOjcuTP8/Pywfft22ZKw0tJSlJaWAoAIaM2ZMwcjR458Fr8WaoC079XW1tYYO3Ys3N3dcfHiRWzZsgWFhYUAqtsLUhmUcgdGR0fj2LFjGD9+vGwQS/s90vk1Gg3Mzc11Nroiaki4hJCeGmn3lAMHDgCo7qRrJ19dvXo1ysvLxeiT9Ny0adNgYmICAGJqdmlpKU6fPo0bN2489HPvn1Sop6cnGsJAdc4tAIiJiUFKSsqTf1lqtDQaDQwNDeHl5SXyTklBKSkgcOPGDQQEBODDDz+Em5ubzk6FGo1GbDQglfNu3bphwIABYjmiNIq7Z88elJWVMWhKj00KtEq5KqQydOvWLSxduhSbNm1CfHy8bKMAhUIhkghLSwKXL1+O3377DZcvX9b5jL59+8LLy0tsKODk5ITPP/9clp+Q6GmTcrdNnz6dwSuqc/v370dQUBBOnjwJExMTkasSqO7EZ2ZmisfSfV4KFGjfv/X09JCbmyt+1s4ZJKWtUKlU2LJliywfq4WFBby9vUVQ7P7k2UBVvq3Vq1czeEUA5MEmJycnjBs3DtOnT5flrbx/EyJpmeGVK1ewaNEi/Otf/0JISAju3bsnziX1vbQnGdT0mUQNBQNY9NRUVlaiWbNmYocsqQMvPaevr4/i4mKsXbsWQPXuGRqNBp06dcLgwYPFTVz6Ozo6GufPn0dxcbHsuER7pCA3N1fMlmnWrBmGDh0Ka2trsZOMp6cnFi9ejHbt2j29XwI1eFJ56dixI3x8fERuCekmHhcXh/DwcKhUKri4uODNN9/E6NGjAcgTY+bl5YnGrXR86tSposxpNBqMHj0aK1eu5FRsemwPC7Sq1WpkZWWJjS6AqkCqsbGxbAkKUD2Se+TIESxcuBDnzp0T5VaaVTB+/HgYGxvj5ZdfxuLFi9GrV69n+E2J2Gmip6O8vBwffPAB/vzzT7FLm5mZmew10kwoqa6UZlNJS7nLysrEa6V7/WuvvQagKgdmQEAAvL29dV4TGhqK8PBw2bH27dsjICAABgYGYtBAo9GgV69e+PXXX/HKK6+wvUA1kspQ8+bNZTsUAlUbYWjf+7X7Rmlpafj+++/xzTffiB3ZOaBKjQ2XENJTo6enJ6ZfA9V5g6S/pcp3z549GDlyJBwcHGTTqKdMmYKwsDARhJKWdZ0+fRouLi5wc3OTLZOR3ldcXIyIiAiEhYXB1tYWY8eOhZWVFZycnODn54dTp07hrbfegpeXV/38YqjBkcqPh4cH4uPjcerUKdEYkMpTly5d4OrqCjs7O7zyyito0aIF9u7di5ycHPTq1QuvvvqqmHItNXhtbGwwZMgQxMXFYf78+XBycqrnb0qN1f2B1pSUFOTm5spGS6XGamBgIEaMGIGSkhKkp6dj7969SE5OltWTCoUCBQUF+OWXX+Dq6oq33noLpqamAKp2uVq3bh2XthJRk3Lv3j1kZ2eLHajVajUsLS0ByDv5Go0GNjY2YhmWdE9PSkpCSUmJCCpp3+vfeecdZGZmYtiwYbC2tsbbb7+NtLQ0ABCzsDZv3owePXqIutbIyAi9evVCZGQkLl68CAcHB8ybN4/tU3ok6d4vrUqR+khmZmb49NNPkZGRgW3btiEpKQlFRUWypbAAcPnyZVy+fBk7d+7EK6+8AhcXl3r7LkSPiwEsqjM1JQY0NDSEo6OjyL+i0WjESJP21Oply5bhs88+kyUOtrS0xPjx47F06VIA1bMN0tLSEBoaCkdHR7Ro0QJAdXAsJiYGISEhiImJQXp6Olq1aoW2bdtiwIABMDU1xdSpUzFv3rxn/Juhhk4qt7a2tvD19UVSUhLS09PF8atXryI8PBzt2rUTQapJkybBzc0Nubm5YrS1plwBL7zwwjP8JtSU1RRolepFoKoBO3HiRMyaNUu8x8XFBb1798bGjRsRHByM4uJisYxaWpodGhqKlJQUvP766+jZsycA5mUjoqbHyMgIZWVlqKiokC25AuSz/hQKBSwsLEQ9qZ12Ij4+Hn369BGvlQIJ9+/ONn78ePz8888AqnceTkpKwsGDBzFx4kSRC8ve3h6DBw+Gl5cXRo0a9fS+PDUp0oxp6V4tlSelUomSkhL07NkTzs7OuH79OtavX49r166JSQLSihgbGxtMmTKFwStqdLiEkJ7ImTNncP78eQDQWaoisbe3h76+PiorK2FkZARHR0cYGxsDgKhMIyMjxXm0p8GOHj0anTt3FueVjoeEhCA+Pl58RnJyMjZv3ow1a9bgyJEjIvhw584dnDlzRuQnkIIPRA/i5uYGDw8PMZol/R0ZGYmYmBjZazt16iSCV1JZltS0bTHRk7g/0ColVdee1RoVFYXbt28DgBgosLS0xCuvvIJ3330Xtra2YpmKWq1GRUUFbGxsMG/ePBG8IiJqijIyMsTOwFLw39raGkD1kiypvRkQEABjY2PRJpUCXlu3bkVOTs4DP0ParXjw4MHw9PSU7VgMAJGRkbLk8EqlEn5+fgxe0WORypP2xkJSmba3twdQtaS1R48e+OKLL/DRRx/B29tblLnZs2dj2bJl8PX1rbfvQPR3cQYW/S2XL1/G8uXLkZiYCHNzc4wdOxZTp07VGcECqkbypSTCGo0GAQEBCAsLEwEopVIJlUqF5cuXo3fv3iIPlhQ8mDp1KhYuXIjKykoxwpCXl4eQkBBYWlri5s2bCAkJwZUrV1BcXCybAt65c2fMmjULVlZWz/6XRI2Subk5fHx8cOXKFVy5ckUcT09Px7lz5+Ds7AwbGxud2VbMIUDPkpubG+Lj43Hr1i1ZoDUlJQWHDh3CnDlzZGVSX18fPj4+sLGxwZEjR7B//34olUrMmjULEydOrMdvQkRUd7Kzs9G8eXORmF37Xm1ubo7i4mLRiZeC/ABk6S6kHK6BgYHYsWMHgOpVAElJSdi9ezdGjx4NGxsbUf9KybOlGTG5ubkiX6tCoRCBLV9fX9lucdp/E9WW1B+Syq30WCrjFhYWog1gZGQELy8veHl54fz58+jcubNsJ0KixoYBLHosOTk5WLlyJYKDgwFUddqLioqwYcMG5OfnY8SIEXBychI3cj09Pbi4uMDExAQlJSUoLy9HWVkZ5s+fj/fff18WqMrIyMDu3bsxbtw42Q5bXl5e6NOnD86cOSNytwBVCTEzMzORk5OD7OxscY3SNrPz5s2Dn5/fs/8lUaPn4uICb29v3LhxQyy5UqvViIqKQteuXTFixAg2OKle3R9olUb5y8vLERISAh8fH7i4uMi2eweADh06YP78+XB1dUWPHj3YiCWiJiE3Nxd//vknYmNj4evri1GjRqF9+/aye/WtW7dkwSsAcHBwACAPdElBAX9/f5w6dQrZ2dmyFBcHDx5EYWEhXn75ZbFjttQ+LSkpwYULF3Ds2DEkJCQAqAoumJqa4sUXX+SOrlQnpDIq9X+kfG1AVfvg/jaqFODq3bv3s71QoqeAASyqtRMnTuDHH38EADEFW5parVarsW/fPsTGxuK9996Dk5OTuNmXlpbCysoK5eXlUKvVuHTpEqZPn46AgACcOHFCVsmuW7cOQ4YMgZmZGSorK0VDY9q0abh06RIKCwtlO28lJibKEsMrFApMnz4d06dPr49fETUBGo0G+vr68PLyQkJCAiIiIkSZy83NRXh4OLp06YL27dvXmPOK6Fl5UKA1KysL+/fvh4uLi87MQKnM9uvXr56umoiobm3ZsgUbNmwQ9duRI0dw6tQpjBgxAj4+PnB1dQUAGBoa6qQHKCoqAlDzLCgnJydMmjQJS5Yskc3UKi0txdGjR5GSkgIvLy+4uLigVatWiI2NRVRUFJKTk5GRkSGup2XLlhg5ciQGDx4MoOZ8mUSPQ6PRiB2ugeolsC1atBA/a9//mdaCmhIGsKjW7t69C1NTUxQXF4uglZR3RdpyODU1FYsWLcKQIUMwYcIEAECbNm1QUVEhRgakSnTy5MmIjIxEQUGBSOheXl6OVatW4c033wQAMS3byckJw4YNE1O5pecAiOvo378/XnrpJS4XpCciNSrbtm0LHx8fXL9+XYy+qtVqxMXFITg4GI6Ojkx0TfXmYYFWtVqN6OhohISEoG/fvrJZWOw0EVFTUVxcjH/+859ITk4GULVUWns26l9//YUDBw5g1KhRGD58uFhyrT1bpU2bNgBqDioplUoEBgbi4sWLiIiIEMelQdPExEQkJiZCX19f7LytPaAKAM2aNcOYMWMwfPhwsXsh62F6UgqFAoaGhrIArEajQXFxMdum1OQxHEuPJDUG/Pz84OnpCQDixi89Ly0FVCgUSE9Px6pVq7Bx40akp6dDqVTKdri4du0a7t69C0dHRwwfPlwcl0YMDh8+jOTkZFmQDKja9c3e3l42/VutVqNjx4749ttv8eGHHzJ4RXVCKnMeHh7o1auXyIkBVM0oZOOT6tv9gVZra2uxbFupVCIvLw9btmwRM7OIiJoaU1NTvP/++5g/fz7atGkDlUqFyspKWeL1iooK7Nq1C5988gmOHz8OAKIdKe3YBtQcVJICUa+//jrc3d3FoKo0aCsNyEqzYZRKpSwPa/fu3fH5559j3LhxYvMiorpSXFyMe/fuwcDAQJS71q1by/poRE0RA1j0SFKlaG9vDy8vL9jZ2Ynj0p9OnTrBwsJC1gDYvn07fv75Z2RlZaG8vFy8x9TUFOnp6QCqZmE5OjqKIJiUdHPZsmUAqka/pECWubk5Jk2aBABQqVRo1qwZ3n77bfzvf/9D165dn80vg54LUjm2traGr68vbG1todFo4OzsjK+//hqzZ8/mCBfVuwcFWqXAf7t27RhsJaImzcnJCYGBgfjqq68wZcoUtG3bFgBEXSi1U7Ozs2W7V0udfGkXQuk92qQ61draGm+99RamTZsGY2NjsamQ9HqpzpX+tG7dGu+88w6++eYbdOzY8Wn/Cug5ZWpqCiMjI7HKxdHREaNHj+agFTV5Co3UAiZ6CGnEKT8/H5s2bcKBAwfEaH9lZSW6du0KNzc33LlzBydOnAAAMUvK29sbWVlZSElJER2uhQsXolu3bgCqc2tp78ZSWVmJBQsWiOUvenp64vkFCxagS5cuOrtsET0NRUVF+Ouvv9CyZUsMGTKkvi+HqEYRERFYtmwZbt++jY4dO+Kll15C9+7d6/uyiIieOu0le4WFhdi1axfOnj2LjIwMAFXt0fuDU9LgaM+ePeHt7Y2RI0fWqk157do1BAcH49q1a7hx4waMjY2Rn5+Prl27ws7ODp06dcLgwYPZPqWn7tatW3jttdfQrFkzzJ49G8OGDavvSyJ6JhjAosd26dIlrF27FomJibKdXEaOHIlRo0bhzJkz2L59u9gyWFqXDVQHtWbMmCFLtP7ZZ58hKipKJIdXqVSwtrbGihUrAECW0L2iooKzX+iZYKJVaiwYaCWi55200xpQtTvb2bNnsXPnTuTn54vBUO1uj/bPbm5u8PPzw4ABA2BkZKRz/6+pPVBYWIjc3FyYm5vDzMwMGo1G5LkietqKiopw+PBhjBo1CoaGhvV9OUTPDANY9NjKy8uxY8cO7Ny5E6WlpSIoZW1tjaCgILHt8Nq1a5GVlSWmb2sXtRkzZmDatGkiGHX58mX8+9//FkEvfX19VFRUYNq0aXjhhRd0toInIqIqDLQSEVXTrhPT0tJw8eJFbNmyBYWFhQDk+a60E7oDQJcuXTB+/Hi4urrCwsLigeeXViHU9JlERPT0MAcWPRaNRgNDQ0N4eXmJvFPSzlfZ2dkIDg7GjRs3EBAQgA8//BBubm46OxVqNBpcv34dAMRMqm7dumHAgAEiD5ZKpQIA7NmzB2VlZQxeERE9ADtNRETVtOtEJycnjBs3DtOnTxeJ3QHdDYikVBVXrlzBokWL8K9//QshISG4d++eOJcU6JJe/6DPJCKip4cBLHos0g26Y8eO8PHxgZWVlWwUKi4uDuHh4VCpVHBxccGbb76J0aNHA5Anx8zLyxMjYdLxqVOnwtraGhUVFdBoNBg9ejRWrlzJ6dhERERE9NikNmbz5s1lOxQCVbu4aq8QuH/m1vfff49vvvkGZ86cAQAOphIRNQD69X0B1PhIN3gPDw/Ex8fj1KlTokFQXFyMiIgIdOnSBa6urrCzs8Mrr7yCFi1aYO/evcjJyUGvXr3w6quvwtzcHABEIngbGxsMGTIEcXFxmD9/PpycnOr5mxIRERFRYyUFq27cuAGgOhermZkZPv30U2RkZGDbtm1ISkpCUVGRWCkgBbIuX76My5cvY+fOnXjllVfg4uJSb9+FiIgYwKK/Qbqp29rawtfXF0lJSUhPTxfHr169ivDwcLRr104EqSZNmgQ3Nzfk5ubC29sbQM35Al544YVn+E2IiIiIqKmScqhKKSukRO9KpRIlJSXo2bMnnJ2dcf36daxfvx7Xrl0TywqldBY2NjaYMmUKg1dERA0AA1j0RNzc3BAfH49bt26JRoJarUZkZCS6du2KPn36iNd26tRJ/Hx/Uvb7cwkQERERET0Jqa2ZlZUFoDppu0KhgL29PQDA2NgYPXr0wBdffIHo6GgcPnwY4eHhUCqVmDlzJiZNmlRv109ERHIMYNETMTc3h4+PD65cuYIrV66I4+np6Th37hycnZ1hY2OjM9uKeQSIiIiI6GmSZlxJA6XSY3NzcxQXF8PCwkK0SY2MjODl5QUvLy+cP38enTt3fuBOhEREVD847YWemIuLC7y9vWFqaiqbWRUVFYXIyEgA3J2FiIiIiJ4tKXCVnZ0tHkuJ3c3NzXXap9JzvXv3ZvCKiKgBYgCLnohGo4G+vj68vLzQvXt3AFU3f4VCgdzcXISHhyM5OVm8loiIiIjoWdBoNCgvLxdtUClA1aJFC7EroTamtCAiathYS9MTkUau2rZtCx8fH1hbW0Oj0YgGQFxcHIKDg1FRUcFZWERERET0zCgUChgaGqKoqEg8BoDi4mIYGBgwpQURUSPDABY9MWlUy8PDA7169YJCoRAjXKWlpQxcEREREVG9KC4uxr1792BgYCDapK1bt9aZfUVERA0fk7jTE5MaA9bW1vD19UVsbCxu374NZ2dnzJ07F66urvV8hURERET0PDI1NYWRkREqKioAAI6Ojhg9ejRnXxERNUIMYFGd6tatGwYOHIiWLVtiyJAh9X05RERERPQcu3XrFpKSktCsWTPMnj0bw4YNq+9LIiKiv0mhYWZtqiMajYbLBYmIiIiowSgqKsLhw4cxatQoGBoa1vflEBHRE2AAi4iIiIiIiIiIGjQmcSciIiIiIiIiogaNASwiIiIiIiIiImrQGMAiIiIiIiIiIqIGjQEsIiIiIiIiIiJq0BjAIiIiIiIiIiKiBo0BLCIiIiIiIiIiatAYwCIiIiIiIiIiogaNASwiIiIiIiIiImrQGMAiIiIiIiIiIqIGjQEsIiIiIiIiIiJq0BjAIiIiIiIiIiKiBo0BLCIiIiIiIiIiatAYwCIiIiIiIiIiogaNASwiIiIiIiIiImrQGMAiIiIiIiIiIqIGjQEsIiIiIiIiIiJq0BjAIiIiIqJnKigoCAqFQvbn5MmTT+3zUlJSdD5vwIABT+3ziIiIqO4xgEVERETUwHz++ec6ARfpz7Jlyx75/n79+j3w/URERESNEQNYRERERI3I77///tDno6KiEBIS8oyuhoiIiOjZYACLiIiIqBG5cOECwsPDH/j8b7/99gyvhoiIiOjZYACLiIiIqJF50CysgoICrF+//hlfDREREdHTxwAWERERUSOzefNm5Obm6hxfs2YNioqK6uGKiIiIiJ4uBrCIiIiIGgFDQ0Pxc0lJCVatWqXzmvtnZmm/51FUKhU2btyIGTNmwNnZGRYWFjAyMoKdnR369++PTz/9FKmpqbU617lz5zB16lTY29vD2NgY7dq1w/z583HlypVaX4+2yspKbNu2DS+++CK6dOkCS0tLGBkZwcHBASNGjMCvv/6KkpKSv3VuIiIiahwYwCIiIiJqBCZOnCjbRfCPP/6ARqMRj0+ePInLly+Lx3379oW9vX2tzh0eHg4XFxe88MIL2LRpE5KSknDv3j2Ul5cjMzMTZ86cwVdffQVnZ2d8/PHHUKvVDzzXd999h379+mHr1q24ffs2ysrKkJqaiiVLlqBXr17YuHHjY33vS5cuoXv37pgyZQrWrVuHq1evIj8/H+Xl5bh16xYOHTqEt956C87OzggODn6scxMREVHjwQAWERERUSPQpUsXDBo0SDy+evUqjh8/Lh7fP/vqjTfeqNV5g4OD0b9/fyQlJT3ytSqVCt9++y1mzpxZ4/Pr1q3DP/7xD1RWVtb4fElJCWbPno3Q0NBaXVtoaCj69u2LhISER742IyMDQ4YMwbFjx2p1biIiImpcGMAiIiIiaiRef/112WMpaHX79m3s3LlTHG/ZsiUmT578yPMVFBRg8uTJKC8vlx1v1qwZpk2bhvnz56NHjx4679u8ebNOwCw/Px//7//9P53XmpqaYsqUKXj11Vfh4uIClUqFq1evPvLaCgsLMXHiRJ2lgfb29pg+fTpeeukluLm5yZ6rqKjAjBkzkJeX98jzExERUePCABYRERFRIzFu3Dg4ODiIx7t370ZGRgaWLl2KiooKcXzevHkwMjJ65Pl++eUXZGVlyY61adMGsbGx2LRpE/744w9ERUXhvffe03nvV199BZVKJR6vXbtWJ7F8ixYtEBkZiS1btuDPP/9EbGwsZs2aVavv+ttvv+H27duyYzNnzkRycjI2btyIZcuWISoqCp999pnsNVlZWfj5559r9RlERETUeDCARURERNRI6Ovr4+WXXxaPVSoV/vjjDyxZskQcUygUmD9/fq3Ot2PHDp1jX331FZycnGTn+/bbb2Frayt7XUZGBsLDw8XjAwcO6Jzrww8/hIuLi3isVCrx888/1yq4tm3bNtljIyMj/PLLLzrv/fTTT2FiYvLQ9xIREVHjxwAWERERUSPy6quvQqlUisfffvst0tPTxePhw4ejQ4cOjzyPSqXCxYsXdY6PGjVK55ihoSGGDBmiczwiIkL8fOnSJZ3nhw8frnPMysoK3t7eD702tVqNyMhI2bGysjJYWVlBoVDI/hgYGOgsM4yNjUVhYeFDP4OIiIgaFwawiIiIiBqR1q1bY+zYseKx9tJBoPbJ23NycnSSrZuYmMDa2rrG17dp00bn2J07d8TP2dnZOs87OjrWeK4HHdc+14MSwdeGRqNBZmbm334/ERERNTwMYBERERE1Mvcnc5c4OTnVOIPqecQZWERERE2Lfn1fABERERE9niFDhsDZ2RmJiYmy46+++ir09Go3PtmiRQvo6enJZjqVlJQgOzu7xllYN27c0DnWqlUr8bO1tTUyMjJkz6enp8teo338YaytrXWuzcLCAnPnzn3o+7S1bNmy1q8lIiKiho8BLCIiIqJGRqFQ4LXXXsOHH34ojhkYGMgSvD+Kvr4+evXqpZNrav/+/XjxxRdlx8rLy3H06FGdc3h5eYmf3d3ddQJYhw8fhoeHh+xYXl6eLPl7TZRKJTw8PHD+/Hlx7N69e/jggw9qXMp4P7VaLcsTRkRERI0flxASERERNUJz585F8+bNYWRkBCMjI0yZMkVnp8BHmThxos6xf/3rX7LZVhqNBh999JFOTil7e3tZMvaRI0fqnGvRokW4evWqeKxWq/Huu++irKzssa9No9Fg8uTJuHnzZo2vLygowNatWzF69Gh88803jzw/ERERNS6cgUVERETUCLVo0QJ5eXlPdI633noLP/74I7KyssSxGzduoHv37hg1ahQsLS0REhKC6Ohonfd++umn0Nevbkq++OKL+PTTT2XXlJ2dDQ8PD4wePRqWlpYIDg5GfHx8ra/tp59+kgXOwsPD0a5dOwQEBKBt27YwNDRETk4OEhISEB8fLxLa9+7d+3F/FURERNTAMYBFRERE9JyysLDAtm3bMHToUJSXl4vj9+7dw6ZNmx74vqlTp+okkm/evDl++uknzJkzR3a8qKgImzdvFo8VCgVat279yDxYzZo1w44dOzB48GCUlpaK4yqVCseOHavV9yMiIqKmg0sIiYiIiJ5j/v7+OH36NDp27PjI1+rr62PBggXYsGFDjc/Pnj0b33zzDRQKxQPfv3jxYgwePLhW19a3b1+Ehoaie/futXo9ULW0sWfPnrV+PRERETUOnIFFRERE9Jzz9vZGQkICtmzZgj179iAiIgJ37txBWVkZrKys0KlTJwwYMACvvPIK2rZt+9BzffzxxxgwYAC+//57nDlzBrm5uWjVqhUCAgLw/vvvw8PDA0FBQbW+Nnd3d8TExGDfvn3YuXMnwsLCkJGRgYKCAhgbG6Nly5bo3LkzvLy8MHToUPTv358J3ImIiJoghUaj0dT3RRARERERERERET0IlxASEREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERUROUkpIChUIh+zNgwID6viwiIiIior+FASwiIiIiIiIiImrQGMAiIiIiIiIiIqIGjQEsIiIiIiIiIiJq0BjAIiIiIiIiIiKiBo0BLCIiIiIiIiIiatD06/sCiIiI6NHS0tKwcuVKnDp1CgkJCcjNzYW+vj7s7OzQsWNHDB06FOPGjYOzs/NjnbewsBAnTpxAREQEzp8/jxs3biAnqBWicgAAPllJREFUJwc5OTnQaDSwsLCAs7MzvL29MXv2bHh4eDz0fFFRUVi5ciVCQkJw/fp1FBQUwMDAAC1btoSNjQ1cXFzg4eGBAQMG1HiusrIyrFmzBn/99RdiY2Nx584dlJWVwdLSEi1btoSjoyPc3NzQt29fDBo0CFZWVo/1fYmIiIiocVJoNBpNfV8EERER1ay8vBwLFizAL7/8ApVK9dDXzpkzB6tWrQIApKSkoH379rLnAwICcPLkSdmxvXv3YsyYMbW+nnnz5uGPP/6AgYGBznP/+c9/8Pnnn6M2TQulUqnzfZKTkzFs2DAkJibW6lq+/PJLfPLJJ7W7cCIiIiJq1DgDi4iIqIGqqKhAYGAgjh07Vt+XIqxYsQImJib45ZdfZMdPnDiBf//730907qCgoFoHr4iIiIjo+cIAFhERUQP10Ucf1Ri8srCwwKBBg+Dg4IC8vDycO3cO169ff6LPMjc3h7u7O1q2bAlra2uYmJggPz8fUVFRiI6Olr32jz/+wPvvv48OHTqIY+vXr9c5Z8+ePeHp6QljY2Pk5eXhypUriImJQXl5uc5r09PTERwcLDvWrFkzDBkyBPb29igrK8PNmzcRExODmzdvPtF3JSIiIqLGhwEsIiKiBigjI0NnlhMAjB07FqtXr4alpaXs+IkTJ3QCTbXRo0cPBAcHw9fXt8ZlgQDwww8/4P333xeP1Wo1/vrrL7zzzjviWFpamuw9kydPxtatW3XOVVJSgtOnT2Pbtm2y4/e/HwBCQ0PRvXt3neNJSUnYs2cPWrdu/fAvR0RERERNBgNYREREDdCePXt0Ziq1a9cOmzZtgomJic7rBw4ciIEDBz7257Rt2xZt27aFRqPBhQsXEB0djbS0NBQWForPz8/P13nfhQsXZI+bNWsme5ybm4vi4mKYmprKjpuYmGDYsGEYNmzYQ98PADdv3qwxgNWxY0e89957tfuCRERERNQkMIBFRETUAIWFhekcmz17do3Bqyf1xx9/4Ouvv0Z6enqt33P37l3Z48GDB2PHjh3i8bFjx2BjYwNXV1d06dIFXbp0gaurK/r164eWLVvqnK9bt26ws7PD7du3xbHhw4ejXbt26NatGzp37gwXFxd4eXmhZ8+eUCqVf+ObEhEREVFjxQAWERFRA5SZmalzzMXFpc4/54MPPsD//ve/x35fcXGx7PG8efOwZs0aWeCtuLgY4eHhCA8PF8cUCgUGDhyIr7/+Gr6+vuK4UqnE4sWLMX36dKjVanE8JSUFKSkp2L9/vzjWqlUrzJ8/Hx9//PFTCegRERERUcOjV98XQERERPUjNjYWP/zww996r0ajkT02NjbGyZMn8e2336JTp04Pfd/x48cREBCAs2fPyp6bPHkyQkNDMWHChIcGpu7cuYMvv/wSkydP/lvXTkRERESNDwNYREREDVCrVq10jiUkJNTpZ+zZs0cnENW5c2ccOHAA2dnZqKyshEajqfXnGhsbY8GCBbh69Spu3LiB/fv346effsIbb7yhk8uqvLwc//nPf3TO4eXlhR07diA3NxcRERFYs2YN/v3vf9cY1Nq/fz9CQ0Mf81sTERERUWPEJYREREQNkI+PD1avXi07tmbNGnz88ccwNjauk8+oaee/b775BiNGjJAdqykf16M4OjrC0dERI0eOBFA188rf3x9nzpyp1XmNjIzQu3dv9O7dWxw7evQohg4dqnNtffr0eezrIyIiIqLGhTOwiIiIGqCxY8fC0NBQdiwlJQXTp0+vcVfA0NBQ/Pzzz4/1GfefHwCioqJkj2NjY/GPf/zjkedaunQpFi5ciGvXrtX4fFlZmc51l5WViZ/VajWmTJmCffv2oaSkpMZz1JQXTPscRERERNR0cQYWERFRA9S6dWu8+eabOjmqdu/eDScnJwwePBgODg4oKChAREQEEhISMGfOnMf6DE9PT51jX375JU6fPo0uXbogLS0NR44cgUqleuS5UlNT8fXXX+Of//wnHB0d0b17d7Ru3RqmpqbIzc3FqVOndHY57Ny5s/hZo9Fg27Zt2LZtG4yMjODq6opOnTrBysoKlZWVSExMxKlTp3Q+V/scRERERNR0MYBFRETUQH377be4cOGCTuCmoKAAO3fufOLzT548GR999BFu374tO37y5EmcPHlSPB4+fDgOHTpU6/Omp6frBKtq8vrrr9d4vKysDJGRkYiMjHzo+9u1a6ez3JGIiIiImiYuISQiImqgDA0NcejQIbz99ttQKpV1fn4zMzNs27YNzZs3f+Brxo4dW6uliQqFotafq1Ao8P7772P+/Pl/6/1AVfBqz549D92tkIiIiIiaDoXm/u2HiIiIqMFJSUnBypUrcerUKVy5cgW5ubnQ19eHnZ0dnJ2dMWTIEIwfPx7Ozs7i9e3bt5edIyAgQDazSpKamoqFCxfi4MGDuHXrFiwsLNC9e3fMnTsXs2fPRmpq6iPPpVKpEBoaipMnT+L8+fO4evUqbt26hcLCQiiVSjRv3hwdO3ZEv379MGfOHPTo0UPnOm7duoXDhw8jNDQU0dHRSE1NRU5ODsrLy2Fqagp7e3u4urpi9OjRmDlzJoyMjJ78F0tEREREjQIDWERERERERERE1KBxCSERERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERERERE1aPr1fQFEREQklxg0vb4v4bE4r9pU35dARERERE0cZ2AREREREREREVGDxgAWERERERERERE1aAxgERERERERERFRg8YAFhERERERERERNWgMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERERHVuVWrVkGhUMj+fP755/V9WURE1EgxgEVERERERERERA0aA1hERERE9EydPHlSZ2ZOUFBQfV8WERERNWAMYBERERERERERUYPGABYRERERERERETVoDGAREREREREREVGDxgAWERERPXNpaWn44osvMGjQIDg4OMDExATNmjVDp06dMGLECHz//fdITEys8b0lJSVYsmQJJkyYgHbt2sHc3BzGxsZwcHDAkCFDsHDhQmRlZT3ws1NSUnTyLw0YMAAAcOHCBcyYMQMODg4wNTVF165d8fnnn6OgoEC8v7KyEsuWLYOvry+aN28OCwsL+Pr64s8//4Rara7xMx+W8+n48eMYM2YMWrVqBXNzc7i7u+OHH35AeXm5eH9paSm+//57uLu7w9zcHJaWlhg4cCC2bNlSq993YWEhfvnlF4wdOxZOTk4wMzODmZkZ2rdvjxkzZmDXrl3QaDQPfP/DdpMLDw/HrFmz0KZNGxgZGcHW1haTJk1CRESEznmCgoKgUCgwcOBAnedWr179yLxYBQUF+OGHHzB8+HA4OTnB1NQUhoaGsLOzg6urKwIDA/HPf/4T+/btQ1FRUa1+N7Vx/3W1a9cOAHD37l18/PHH6N69O8zNzWFlZQV/f3+sXr36ob9PyZOW5bq+tsfZNfBBn/t33Lx5Exs2bMB7772HAQMGwMXFBba2tjA0NISZmRkcHR0xbNgwfPXVV0hPT/9bv4+bN2/i/fffR5cuXWBmZgaFQoGTJ0/+7WsmIqJnT7++L4CIiIieH+Xl5ViwYAF++eUXqFQqnecTExORmJiIQ4cOISYmBqtWrZI9v3//fsydOxd37tzRee+tW7dw69YtHDt2DF9++SUWLlyId955p9bX9scff+Dtt9+WXVdCQgK++OILbN++HUePHoWpqSkmTZqEI0eOyN4bFhaGsLAwHD16FFu2bIFCoajVZ37yySf4+uuvZceioqLw/vvvY8+ePdi3bx/y8/MxevRoXLhwQfa6kydP4uTJkwgPD8eiRYse+BmbNm3CG2+8gdzcXJ3nUlJSkJKSgk2bNsHX1xdbt26Fo6Njra4dAP71r3/h22+/RWVlpTh2584d7NixA3v27MHGjRsxefLkWp/vYSIjIxEYGFjjv31mZiYyMzMRFxeHAwcOAADWrl2LWbNm1cln1yQ0NBTjx4/XuZ7Tp0/j9OnT2Lx5M3bs2AFjY+Ma3/80y/KTXtuz9sMPP+D777+v8bmKigoUFxfj5s2bOHLkCL7++mssWrQIb775Zq3Pf/LkSUyYMAF5eXl1dMVERFQfOAOLiIiInomKigoEBgbixx9/rDF49SgbN27E6NGja+zw36+kpATvvvsuFixYUKtzR0dH44033njgdcXGxuLtt99GUFCQTvBK27Zt27B8+fJafebevXt1glfaTp48iX/+85+YOHGiTvBK2/fff4+jR4/W+NyPP/6IGTNm1Bi8ut+5c+fg6+uLW7duPfriAfz+++/45ptvZMErbSqVCi+99NIjZxDVhlqtxtSpU2v1b/8s5OTkPDCYJjlw4ADmzp1b43NPsyw/6bU1dKWlpXjrrbewbdu2Wr0+JycH48aNY/CKiKgJ4AwsIiIieiY++ugjHDt2TOe4hYWFWEqYl5eHc+fO4fr167LXJCcnY968eTpLn1q2bImRI0fC2NgYx48fR1JSkuz5//u//0NAQAACAwMfem1SgKdLly4ICAjApUuXEB4eLnvN1q1bxc+DBg1C+/btcejQIZ0lTYsXL8bLL7/80M8DgOzsbACAp6cnPDw8EBISgri4ONlrfvrpJwCAnp4eRo0ahRYtWmDv3r3ivdqfOWTIENmx8PBwfPDBBzqf6+rqit69e6OiogInT57EzZs3xXM3b97E7NmzHxqkk0gBEhcXFwQEBOD69es67ysoKMDatWvx/vvvAwCGDRsGS0tLpKenY/v27bLXdu3aFcOGDZMd8/b2BlA1o+j+MtGyZUsMGjQINjY2KCkpQWpqKqKjo+skYPYo9+7dAwBYW1sjMDAQhoaGOHz4MG7cuCF73aZNmzB79myMHDlSHHvaZflJrq2+2dvbw8XFBS1btoS1tTX09PSQlZWF0NBQnf9nH330ESZOnAg9vYePx0u/DwDo378/unfvjtzcXJw+ffqpfAciInp6GMAiIiKipy4jIwO//PKLzvGxY8di9erVsLS0lB0/ceIEoqOjxeOFCxeitLRU9hp3d3ccP34cVlZWAKpmeM2cOVMWaAKAzz///JGdfgAYMWIE9uzZAwMDA1RWVsLP7/+3d+fROd17//9fFxklQkiUoKYi5jEIKlFEJa0pFB2I3t9TB00Npa2eFm1vtIaStset2mPqfbeUg6IOMUXMU9WY0tKYikpNNSeR3x+W65edvZNcMl70+VjLWvZ7f/be72vn6lry7ufz/rTWtm3bTOMmTJigt956y/65qlatqtu3b9vP79+/X5cuXbLnlZX/9//+n2bOnCmbzaabN2+qdu3aSkxMNI37v//7P/Xu3VvSvT5dTZo0MZyPj483XTN27FjT7KhPP/1Ur776qv345s2b6tatm1avXm2PrV27VvHx8WrTpk22+b/00kuaPXu2ihYtKkkaMWKEaSlYfHy8vYD1/PPP6/nnn1dcXJypgNWsWTNNmzbN8jknT540HHt6eiohIUF+fn6msYcOHdKSJUsUEBCQbf65UatWLcXHx9tzuHnzpp555hmtX7/eMG7q1KmGIlFBfJdzmlth6dWrl6KiolS3bl3L88nJyerTp4/hO3Ps2DEdOHBADRo0yPb+Hh4eWrp0qTp27GiPpaamGvrMAQCcH0sIAQBAvlu2bJnpl8XKlStr/vz5puKVJLVt29be8yctLU1Lly41jYmJiTEUiVxdXfXPf/5Tbm5uhnG7du3Sb7/9lm2OkydPlqurq6R7M546dOhgGhMQEKARI0YYjlu2bGkaZ1WEysjNzU0TJ06098vy9PS0LBo1a9bMXrySpMaNG6tKlSqGMZcvXzYsE7x69appNlSzZs0Mxav7zxw9erTpmY4sz/Lw8NCUKVPsxStJ+vvf/24a9+uvv2Z7r+wUL17ccJyamprpUsc6deronXfe0VNPPZXr52Zl4sSJhgKap6enPv30U9O4uLg4e8GqoL7LOcmtMAUFBalu3bq6ffu2NmzYoOnTp2vMmDEaMWKEhg4dqpEjR+rKlSum67JaWpveqFGjDMUrSSpatKg8PT3zJH8AQMFgBhYAAMh3O3bsMMX69u3r0C+QiYmJpmVhxYsX15NPPmka6+/vr6CgIG3ZssUQ37Vrl7p06ZLpMwICAlSnTh1DrGzZsqZxoaGhcnFxyXZc+mVLmWnatKlplpbVvTIuDbw/LmNh6M8//7Tfb8+ePaZ+Xjt37nS4ubzVzLOMOnToIH9/f0OsXLlypnGOvIvstGrVSh4eHvZiy507d9SgQQNVr15dtWrVUo0aNRQYGKjmzZurdu3aDn/OnHJxcbEscNauXVsVK1Y0LNdLTk7WoUOH1KRJkwL5Luc0t8J07do1jRkzRjNnztS1a9ccvi4pKcmhcf369ctpagAAJ0IBCwAA5Lvz58+bYoGBgQ5da9XTqEKFCpkWKSpWrGiKZdcs2+oaqx3arMZlnCUjKdPG5gX1zNz2gTp37ly2YypVqmSKWeXvyLvITqlSpTR+/Hj7UkTp3mymo0eP6ujRo6a8hg4dqujoaMPssLzk7+8vd3d3y3MVKlQw9Zu6X2gpiO9yTnMrLMnJyerQoYO2b9/+wNfeuHEj2zE+Pj6W31UAwMOHJYQAAOAvz6rwYlVUsBr3MD3TUY7MgildurQpll8FI0kaNmyYYmNjFRYWZl/qaeXEiRMaNmyYXnvttXzL5a/EqgCZcROB3Jg5c2aOileSTI3wrVgtUQYAPJyYgQUAAPJdmTJlTLGffvrJoWszLlOTpNOnTystLc2y4JNxhklmz3+UWX3e2rVrWy4ts+KsvYE6dOigDh066Pr169q3b5+OHDmiX375RXv37tXatWuVnJxsHztjxgy9/fbbKl++fJ7nceHCBd2+fdtyplPG3fIk2ftRFcR3Oae5SbLc0e/mzZumWMam+rnx3XffmWIREREaO3asatasae9/9vnnn1v2WMtOfi8nBQAUHApYAAAg3zVv3lxz5841xObNm6dRo0ZlO8OocuXK8vf3Nyy/+vPPP7V582ZT76CkpCTt2rXLdI+goKBcZP/wadSokYoWLarU1FR7LCUlRVOnTnXoF/r01+UHq5laD/JMLy8vtWzZ0tBA/8svv9Tf/vY3+/Hdu3e1a9eufClgpaSkaO3atYqIiDDEExISTEUnV1dX1a5dW1LBfJdzmpskeXt7m+535swZU2z+/PlZ5vAgrIphc+fONc3ws+qjBwD4a2EJIQAAyHedO3c29W1KTExU7969LXcX27Ztmz755BNJ92ZQdO3a1TTmtdde0+XLl+3HycnJGjRokGm3w6CgIAUEBOT+QzxESpQooXbt2hliR48e1YABA3T9+nXLa06fPq3p06erSZMm2rRpU77mZ1UoSUhIyHT8yZMn9fLLL2vDhg2GWVbpWfVZu337ds6TzMbIkSMNS+lu3bpluWwxNDTUPqOtoL7LOclNkml3S0latWqVjh8/bj/ev3+/ZsyYkW0OjrLq57Zv3z7D8YIFC0wFcADAXw8zsAAAQL4rX768Bg8erKlTpxri3333nR5//HG1a9dOAQEBunr1qnbt2qWffvrJsHPYqFGj9NVXX9l3oZOkH3/8UdWrV1d4eLjc3d21fv16HTt2zPTssWPH5tvncmbvvfee1q5da+hh9MUXX+jbb79VmzZtFBAQoLt37+r8+fM6dOiQ5bvLL9WqVZPNZjP0MNqzZ49atWqlBg0a2Isar7/+uipWrKg7d+5o9uzZmj17try8vFSvXj1Vq1ZNJUqU0J07d3T48GHLnRNr1KiRb58hISFBNWvWVEREhNzc3BQbG2s5m2jIkCGG44L4Luc0t7p166pUqVK6ePGiPXbp0iUFBwerS5cuunTpklasWGHIPbeaNGmiAwcOGGIREREKDw9X6dKltX//fmZfAQAkUcACAAAF5MMPP9QPP/ygjRs3GuJXr17VkiVLsry2SpUqmjVrll544QVD0SMpKUnz5s3L9Lo33nhD4eHhuUv8IdWiRQtNnjzZsHOfJF25ckXLly8vpKzu8fHxUevWrU0zvbZu3aqtW7faj1988UXTTnzXr1/X9u3bs2383aJFCzVq1Cjvkk7H19dXHh4eOnv2bJbfv549e5qW8uX3dzk3ubm6umrAgAGaMGGCIf7777/riy++sB/7+fnl2e6FgwcP1rx58wyF1lu3bmnx4sWGcWFhYYqNjc2TZwIAHk4sIQQAAAXCzc1Nq1evVnR0dI52q+vTp49WrFjhUEN2T09PTZ06VR999FFOUn1kDBs2TPPnz7fcMTAz9erVK5Allx999JHDOyw+aCPuhg0b6ttvv81JWg7x8fHRqlWrsuyvFRYWlumyt/z8Luc2t3fffVfBwcGZXlulShXFx8c7lIsjmjZtqmnTplk2kJfu/ezHjx+vPn365NkzAQAPJ2ZgAQCAAuPu7q5PPvlEw4cP1+zZs7Vx40YdOXJEly5dkouLi8qWLasnnnhC7du3t+wVFB4ersTERH311VdauXKl9u7dq6SkJKWmpqpUqVKqVauW2rVrp7/97W+WO779FfXq1UvPPvusvv76a61evVp79uxRUlKSbty4IS8vLwUEBCgwMFCtWrVSx44dVa9evQLJKzg4WDt37tSkSZO0adMmnT17NtOeVdWqVdOxY8e0Zs0abd++XQcPHtSpU6d0+fJlpaSkyMvLS+XLl1fDhg3VrVs3de/ePUdF0gdRv359HThwQFOmTNGSJUuUmJgoFxcX1atXT//1X/+lqKioLAtv+fldzk1unp6eWrdunWJiYvTNN9/o559/VtGiRVWjRg0999xzeu211/J8l8ro6Gg1btxYkydP1pYtW3T58mWVKVNGwcHBio6OVps2bTRnzpw8fSYA4OFjS0s/dxkAABS6X6J6F3YKD+SJOXm3IxngjDIWeypVqqTExMTCSSYDZ84NAIC8xBJCAAAAAAAAODUKWAAAAAAAAHBqFLAAAAAAAADg1GjiDgAAgEfWzp071bdv3we+rlu3bpowYUI+ZAQAAHKCAhYAAAAeWTdu3NCRI0ce+LqzZ8/mQzYAACCnKGABAAAAWXDmTbudOTcAAPISBSwAAAA8skJDQynyAADwCKCJOwAAAAAAAJwaBSwAAAAAAAA4NQpYAAAAAAAAcGoUsAAAAAAAAODUKGABAAAAAADAqVHAAgAAAAAAgFOzpbGvMAAAAAAAAJwYM7AAAAAAAADg1ChgAQAAAAAAwKlRwAIAAAAAAIBTo4AFAAAAAAAAp0YBCwAAAAAAAE7NpbATAAAARr1/iSrsFB7I/CfmFHYKAAAAeMQxAwsAAAAAAABOjQIWAAAAAAAAnBoFLAAAAAAAADg1ClgAAAAAAABwahSwAAAAAAAA4NQoYAEAAAAAAMCpUcACAABOZc6cObLZbIY/Y8eOLey0AGTAf6sAgIJEAQsAAAAAAABOjQIWAAAA8kRUVJRpRk5cXFxhpwUAAB4BFLAAAAAAAADg1ChgAQAAAAAAwKlRwAIAAAAAAIBTo4AFAAAK1JkzZxQdHa1q1arJw8ND5cqVU5cuXbR27doc3W/Tpk0aOHCg6tevr9KlS8vV1VWlS5dWgwYNNHjwYG3bti3TaytUqGDo19SkSRPTmMWLFxvGeHt7KyUlxTDm8OHDpt5PI0aMMIzJeL5y5cqSpEuXLmn06NGqX7++vL29Vbx4cTVv3lz/8z//o9TUVMM9rl69qmLFihnu06hRo0w/382bN+Xj42MYX7VqVaWlpVmOnTlzprp166bKlSvL29tbHh4eCggIUPv27TVhwgRduHDB8jn37z137lzTubZt2zrcF2vHjh2Kjo5Wo0aN5O/vLzc3N/n7+6tFixYaPXq0fvvtt0w/a25k9rNJSkrSqFGjVKdOHXl7e8vX11dt2rTR3LlzLd9hRrl5p/mR24PsGpjZc3PizJkz+vrrrzVs2DCFhoYqMDBQjz32mNzc3OTl5aUKFSooLCxM//3f/63Tp0/n6H2cOXNGw4cPV82aNeXl5UX/NQB4BLkUdgIAAOCvY8OGDeratauuXr1qj507d07Lli3TsmXLNGTIEDVs2NChe50/f14vvfSS1qxZYzp38eJFXbx4Ufv379f06dP1zDPPaM6cOSpdurRhXEhIiL7++mv78b59+3Tt2jV5e3vbY1u2bDFcc/36de3du1dBQUH22ObNm005hIaGZvsZtmzZosjISJ0/f94Q37lzp3bu3Kk1a9Zo0aJFKlLk3v9z9PHxUbdu3Qw5//jjjzpy5Ihq1qxpuv/KlSv1559/GmJ9+/aVzWYzjevfv79+//130z3Onj2rs2fPat26dfrggw80YcIEDRkyJNvP9iCSkpLUv39/rVixwvJcUlKSduzYocmTJ2vixIl69dVX8/T5VrZt26auXbua3smmTZu0adMmLViwQIsXL5aHh4fl9fn5TnObW0GbOnWqpkyZYnkuOTlZN27c0JkzZ7RmzRqNGzdOkydP1uDBgx2+f1xcnLp166bLly/nUcYAAGfEDCwAAFAgjh49qmeffdZQvMooJiZGkydPzvZe58+fV/PmzS2LV1ZWrFihli1bmn7BzVhkSk1N1Y4dOwwxq+JUxljG46JFi+rJJ5/MMqekpCR17NjRVLxKb8mSJfryyy8NsaioKNO4BQsWWF6fMW6z2dSvXz9D7JtvvtEzzzxjWWjJ6ObNmxo6dKjefPPNbMc66o8//lBwcLBl8crq+dHR0frggw/y7PlWLl68qPDw8CzfyX/+8x/179/f8lx+vtPc5ubsbt26pVdffVWLFi1yaPzFixfVpUsXilcA8BfADCwAAFAgBg4cqOvXrxtiNptN7du3V7Vq1ZSQkKCNGzfq0KFD2d6rb9++OnHihCFWpEgRhYWFqUqVKvr555+1bt06w1Kqo0ePasCAAYaijtUsqS1btqhdu3aS7hUX9u7dazlm2LBhhuP0GjZsqBIlSmT5Ge6/i+LFi+uZZ56Ru7u7Fi9ebCrwffbZZ3rllVfsx+3atVOFChUMS60WLFig0aNHm+7//fffG2Jt2rRRlSpV7Me//vqrXn75ZdOSMz8/P3Xq1EkeHh5av369jh07Zjg/ceJEhYSEKDw8XJLss4diY2OVkJBgGBsZGakKFSoYYumPo6Ki9MsvvxjOe3p6KiwsTGXLltUvv/yi9evXG3IcM2aMQkNDsy0S5tT9WWulS5dWeHi43NzcFBsbq1OnThnGzZ8/X3379lWnTp3ssbx6p/mRW2ErV66cAgMD5efnp9KlS6tIkSK6cOGCtm3bZlo6+NZbb6l79+722YeZST/D8Mknn1SdOnV06dIlbdq0KV8+AwCg8FDAAgAA+e7gwYNav369KT5//nw999xz9uOZM2dqwIABWd5r69atio2NNcRcXFy0cuVKdejQwR5bsmSJIiMjDYWEhQsX6t1331XdunUlSdWrV1dAQICht1L6YtSOHTuUnJxsyiH9mLNnz+r48eOG844sH5SkgIAAbdmyxd7HJzo6Wi1atDA888CBA7p06ZJ8fX0l3SvUvfTSS5owYYJ9zOHDh3XgwAHVq1fPHlu+fLlu3LhheF7G2VcTJkzQrVu3DLGGDRtq/fr19uclJyfrhRde0MKFCw3jxo4day+2TJs2TdK9YlTGAtarr76a6fvYsWOHaeZVnTp1tG7dOj322GP2WGxsrCIiIuy9x9LS0jRmzBjL71ReqVWrluLj4+Xn5yfpXjHzmWeeMT1z6tSphiJRXr3T/MitsPTq1UtRUVH2/+4ySk5OVp8+ffTvf//bHjt27JgOHDigBg0aZHt/Dw8PLV26VB07drTHUlNTdefOndwnDwBwGiwhBAAA+e4///mPKRYWFmYoXknSK6+8YugtZWXx4sWm2EsvvWQoXklSt27d1KVLF0MsLS1Ny5YtM8RCQkIMx9u3b9fdu3clGQtVLVu2tPeOOnfunH3WUE77X0nSqFGjDM2xGzdurGbNmpnGJSYmGo4zFqKke8XA9DIuH/Ty8lLPnj3tx2lpaVq6dKnpPjExMfZCiyS5urrqn//8p9zc3Azjdu3aleum6ukLFvdNmjTJULyS7n1X2rZta4jFxcXpjz/+yNXzszJx4kR7gUi6Nyvs008/NY2Li4uzF6wK6p3mJLfCFBQUpLp16+r27dvasGGDpk+frjFjxmjEiBEaOnSoRo4cqStXrpiu++GHHxy6/6hRowzFK+neMl5PT888yR8A4BwoYAEAgHz3448/mmIZf+G87+mnn87yXrt27TLFIiIiLMdaxTNen7HYdPXqVR04cECSsTjVpUsXQ6P0+8WtjMsHHel/dd/zzz9vipUrV84Uy9iIvWbNmmrRooUhlr5gdfXqVVPRMDIy0tCcPjEx0bQDXvHixS1z9/f3tywsWv0sHkTGfmOSFB4ebtppzmazmfqdpaWlafv27bl6fmZcXFxMBVFJql27tipWrGiIJScn25e9FsQ7zWluhenatWt6/fXX5efnp6eeekqDBw/W+++/rylTpigmJkYxMTGWu5AmJSU5dH+rgi4A4NFDAQsAAOQ7q5kyGfsiZRe/L2OBQJLpF/es4hmbX2fWB+vu3bvatm2bPdaqVSu1bt3afny/uJVxBpYj/a8kydvbW6VKlTLFrXaOuz8jLL2Mv7QfO3ZMu3fvliQtXbpUt2/fNpzP2Pzd6j1WqFDBtEPhfY68ywdllcODOHfuXK6uz4y/v7/c3d0tz1l9P+8XWgrineY0t8KSnJysDh066OOPP9a1a9ce6NqMS2Ct+Pj4qFKlSjlNDwDwEKGABQAA/tJq1KhhmvW0ZcsWHTx40L6syd3dXU2bNlWrVq3sYzZv3qxr165p3759hmsdXT5YunRpy3jRokUdur53796mQsb9WVgZlw9WqlTJ4bweJg9aEIGZVXE0L5dmzpw5M8cz5TI2wrdSsmTJHN0bAPDwoYk7AADId1bFmoy7jmUXv8/f39/UKPzUqVOWvaMy7swmSWXKlDHFQkJCDD2kNm/ebJht1bRpU7m7uxtiR44c0ffff29vLH5fQRWKSpYsqS5duujbb7+1x7799lu99dZbpuV2ffv2Nc0C8vf3N93z9OnTSktLs5wx5Oi7fBBlypQx/SxffvllFS9e3KHrGzZsmKvnZ+bChQu6ffu25Uwnq+/n/X5UBfFOc5qbJMsd/W7evGmKnTx5MsscHsR3331nikVERGjs2LGqWbOm/Wf9+eef6+9///sD3z+z2W0AgEcPBSwAAJDvGjZsqK+//toQi42N1fDhw01jV69eneW9goKCFB8fb4itXLlSkZGRprErV660vD6j0NBQQwHr5MmThuP7M6+eeOIJPfbYYzp//rzS0tI0adIkw30epP9VXoiKijIUsE6ePKmRI0cadjG02WyWPYIqV64sf39/w7K3P//8U5s3bzZ9hqSkJMveTBnfpdXssdTU1EzzDwoK0saNGw2xp556Si+88EKm16S/r6Oz1R5USkqK1q5da+qhlpCQYCo6ubq6qnbt2pLy553mVW6SDD3Q7jtz5owplnFDgNywKobNnTvXVNS26ocGAEB6LCEEAAD5zqox++rVq0270P3rX//Szp07s7xX9+7dTbF58+aZmkAvWbLEtCOczWZT586dTddbzZpKXyRLv3Qw/d/37NljuMbR/ld5JSwszLT8cfbs2Ybj1q1bq1q1aqZrbTabunbtaoq/9tprunz5sv04OTlZgwYN0p07dwzjgoKCFBAQYIhZFUgOHz6caf5WP8shQ4YYeo+ll5ycrA0bNuiVV16xzD0vjRw50rCU7tatW3rttddM40JDQ+273eXHO82r3CSpSpUqpjGrVq3S8ePH7cf79+/XjBkzss3BURl3WpRkWna7YMECzZ07N8+eCQB4NDEDCwAA5Lt69eqpbdu22rBhgyHes2dPdejQQdWqVVNCQoLi4uKyvVfLli0VFham2NhYeywlJUUdO3ZUWFiYqlatqp9//llr16419dDp2bOn6tata7pnzZo1VbZs2Uybgrds2dL+99atW2vx4sWW4wq6z1TRokX14osvmmaCpZfVDm2jRo3SV199pVu3btljP/74o6pXr67w8HC5u7tr/fr1OnbsmOnasWPHmmLVq1c3xd58801t3rxZZcuWlc1mU7ly5fTmm29KkoKDg9WpUyfDjol//PGHWrZsqSZNmigwMFAlS5bUlStXdPz4ce3fv9/e9yokJCTTz5UXEhISVLNmTUVERMjNzU2xsbGWs4mGDBliOM7rd5qXudWtW1elSpXSxYsX7bFLly4pODhYXbp00aVLl7RixQpD7rnVpEkT+66e90VERCg8PFylS5fW/v37mX0FAHAIBSwAAFAgpk+friZNmhh2FktLSzMUoqR7O7NZ9QZKb968eWrevLlOnDhhj929e1erVq3K9JoaNWro888/z/R8SEiIqfm5JAUGBhr6CKXvg5VRYTRK79evX6YFrGLFium5557L9NoqVapo1qxZeuGFFwzFvqSkJM2bNy/T69544w2Fh4eb4p06dZLNZjPc6+bNm4ZljnXq1LEXsKR7y8mCg4NNBZ09e/aYZrgVFF9fX3l4eOjs2bNZvoeePXualvLl9TvNy9xcXV01YMAATZgwwRD//fff9cUXX9iP/fz88mz3wsGDB2vevHmGZvG3bt0yFYEzFqUBAMiIJYQAAKBABAYG6rvvvsuyQfeLL76oMWPGZHuvxx57TNu3b1f79u0denZERIS2bt2a5Y5lmRWf0i8ZlKRGjRqpWLFipnEF3f/qvjp16qhp06aW57p3755tQ/Q+ffpoxYoVDjVk9/T01NSpU/XRRx9Znq9WrZqio6OzTzodf39/bd++XV26dHH4mmLFiuXru/bx8dGqVatUvnz5TMeEhYVluuwtL99pXuf27rvvKjg4ONNrq1SpYuoxlxtNmzbVtGnTLBvIS/eWXY4fP159+vTJs2cCAB5NFLAAAECBad++vQ4dOqRBgwapcuXKcnNzk7+/v8LCwrRo0SJ99dVXDjfmLlu2rNasWaONGzdqwIABqlu3rnx9feXi4iJfX1/Vr19fgwYN0tatW7VixQrLnRDTc7SA5eLioubNm5vGFXT/q/QyWyYYFRXl0PXh4eFKTEzU559/ri5duujxxx9XsWLF5O7urnLlyumpp57SuHHjdOLECQ0dOjTLe8XExGju3Llq27atSpUqlWnhIj0/Pz8tXbpUe/fu1fDhw9W8eXOVKVNGbm5u8vDwUNmyZdWqVSsNGjRI//73v3X+/Hl98MEHDn22nKpfv74OHDigf/zjH6pdu7aKFSsmHx8ftWrVSrNmzdKqVasM/aUyyst3mpe5eXp6at26dZowYYLq168vT09PeXt7q3Hjxvrwww916NAh1apV64HyyU50dLTi4+PVtWtX+fv7y9XVVeXLl1ePHj0UFxenUaNG5enzAACPJltaxuYQAACgUPX+JaqwU3gg85+YU9gpALlis9kMx5UqVVJiYmLhJJOBM+cGAEBBYgYWAAAAAAAAnBoFLAAAAAAAADg1ClgAAAAAAABwai6FnQAAAACQUzt37lTfvn0f+Lpu3bppwoQJ+ZARAADIDxSwAAAA8NC6ceOGjhw58sDXnT17Nh+yAQAA+YUCFgAAAP7SnHlTbmfODQCAgkQBCwAAAA+t0NBQijwAAPwF0MQdAAAAAAAATo0CFgAAAAAAAJwaBSwAAAAAAAA4NQpYAAAAAAAAcGoUsAAAAAAAAODUKGABAAAAAADAqVHAAgAAAAAAgFOzpaWlpRV2EgAAAAAAAEBmmIEFAAAAAAAAp0YBCwAAAAAAAE6NAhYAAAAAAACcGgUsAAAAAAAAODWXwk4AAAAYRfX+pbBTeCBz5j9R2CkAAADgEccMLAAAAAAAADg1ClgAAAAAAABwahSwAAAAAAAA4NQoYAEAAAAAAMCpUcACAAAAAACAU6OABQAAAAAAAKdGAQsAAAAAAABOjQIWAAAAUMgSExNls9kMf0JDQws7LQAAnAYFLAAAAAAAADg1ClgAAABOaM6cOaYZOWPHji3stAAAAAoFBSwAAAAAAAA4NQpYAAAAAAAAcGoUsAAAAAAAAODUKGABAIACUblyZVNPJytjx441jZszZ45D97p+/brGjx+vRo0aycfHRz4+PgoKClJMTIySk5NNz2rdurXhHq6urvrjjz8y/Qzt27c3jC9SpIh+/fVX07i0tDStWLFC/fv3V2BgoEqWLCk3NzeVKVNGzZo104gRI3To0CHLZ4SGhspms6l///6mc++9957DfbFOnjyp0aNHKyQkROXKlZO7u7tKliypOnXqaPDgwdqzZ0+mnzM37uef/k9iYqJSUlL02WefKTg4WKVKlZKXl5fq1aun999/X9evX8/2vrl5p/mR24PsGpjZc3Pi2rVrWr58uUaPHq3w8HDVq1dP5cuXl6enpzw8PFSmTBm1bNlSQ4cO1Q8//JCj93Hr1i1NmjRJQUFB8vX1pf8aAMApuBR2AgAAAHnh559/1tNPP63jx48b4rt379bu3bv11VdfKTY2VqVKlbKf69evn7Zs2WI/TklJ0aJFizRgwADT/X///XfFxcUZYm3atFGVKlVMefTu3duyeHDhwgVduHBBu3bt0scff6yoqCh99tlnKlasWE4+sqWUlBS98847mjJlilJSUgzn7ty5oytXrujw4cOaPn26Xn75ZU2fPl3u7u559nwrFy5cUGRkpOmdHDx4UAcPHtTcuXO1du1a07u8Lz/faW5zK2hxcXHq3Llzpufvv49t27YpJiZGL7/8smbMmCFXV1eH7v/bb78pPDxcCQkJeZUyAAB5ghlYAADgkdC+fXtT8Sq9PXv2qHPnzrp796491qtXL3l6ehrGLViwwPL6RYsWKTU11RCLiooyHP/0009q1qxZtjNfpHszimbPnq2OHTvqzp072Y53RFpamp577jl99NFHpuKVlVmzZunZZ581fa681qNHjyzfyfHjxxUWFmY52ym/32lucnsYzJo1S8OGDXN4fK9evSheAQCcEjOwAADAI+HkyZPy9vZWeHi4SpYsqfj4eP3000+GMVu2bNHnn3+ugQMHSpJ8fHzUtWtXffPNN/YxGzdu1Llz51S2bFnDtRkLW15eXurRo4f9ODU1VZGRkbp8+bJhnLu7u55++mmVK1dO+/bt07Zt2wznN2/erHfffVcfffSRpHsFlYYNG+rw4cNas2aNYWzz5s3VokULQyz98ccff6wlS5YYzttsNoWEhKhGjRpKSkrSqlWrdOPGDfv5NWvW6MMPP9Q//vEP5ZeTJ0/Kzc1NnTp1Urly5bRr1y7TEsZffvlF77//vv09SHn3TvMjt8Lm7e2thg0bys/PT6VLl5anp6euXLmiffv2af/+/YaxM2bM0PDhw1W1atVs73v69GlJUs2aNfXkk0/qzp072rt3b758BgAAHgQFLAAA8Eh47LHHtHXrVvsv6ampqYqKitL//u//GsZNmzbNXsCS7s2iSl/Aunv3rhYuXKjo6Gh77LffftPmzZsN94mMjJS3t7f9eMGCBTp8+LBhjI+Pj+Lj49WgQQPD8zPOiPn00081cuRI+fn56dVXX5UkzZkzx1TAevrppzPtRXTjxg1NmDDBECtevLhWr16t4OBge+zUqVMKDg7WmTNn7LFJkyYpOjpaPj4+lvfOrWLFiikuLk5BQUH22Ntvv23Kd+bMmRozZox9+V9evdP8yK2w1KtXT/Hx8WrRokWmywKnTp2q4cOH249TU1O1fPlyDRkyxKFnvP/++3rnnXcMfequXbuWu8QBAMgllhACAIBHwjvvvGOYYVK0aFF98sknpv5OR48e1bFjx+zH7du3V/ny5Q1j5s+fbzheuHChYemhZF4+uHjxYlNOr7/+uqHQIklDhw5Vo0aNDLGbN28qNjY2k0/mmLi4OFMD+oEDBxqKV5JUsWJFDRo0yBC7cuWK1q5dm6vnZ2XgwIGGApF0r0gSEBBgiF2+fNkwm6og3mlOcysslSpV0pNPPikXFxf98MMPmjNnjt5//3298cYbGjp0qIYOHWqagSXJoSWYktS2bVu9++67pk0W0hdrAQAoDBSwAADAI6FTp06mmK+vr5o1a2aK//jjj/a/FylSRC+99JLh/LZt23Tq1Cn7ccblg5UrVzbtOLdr1y7TcyIiIixzDQ8PN8Wsrn8QO3bsMMUmTpxo2mXOZrNZLhfMz+KM1c/GxcVF7dq1M8XT/2wK4p3mNLfCNGPGDD3++ONq0qSJ+vfvrzFjxmjSpEmKiYlRTEyMaddOSUpKSnLo3v369cvjbAEAyBsUsAAAwCOhYsWKlvEKFSqYYhl/mc/4S3taWpq9aHXixAlt377dcL5v376mGSoXLlxwOCer+O+//2451lFWz38Q586dy9X1Wcnpz6Yg3mluvjeF4fXXX9fAgQPtvaoclb7vWVbq16+fk7QAAMh3FLAAAEChybgsT5JpGVxBCAwMVPPmzQ2x+wWsb7/9Vmlpafa4zWZT3759CzS/gkCPo9yz+j5LefedPnjwoKZOnZqja9N/h7NSsmTJHN0fAID8RhN3AABQIIoUMf9/s5s3b8rLy8sQO3nyZI7uf+rUKVWrVs0Ut5qpYtXYu1+/foZleLt379axY8dMywdbt25t+Rx/f39T7qdOnVKZMmUsc83IatyDsLo+LCxMtWrVcuj6evXq5er5WTl16pRq1Khhimf3symId5rT3DL7PmeUlpZmmVtOLFu2zFSIqlGjhmJiYtSsWTP5+vrKZrPpyJEjCgwMzNEzMs4sBADAWVDAAgAABcKqCfSZM2cMxYM//vjDtPOeo1atWqXBgwcbYpcvX9bOnTtNYxs2bGiK9e7dW8OGDdPt27ftsXHjxmnPnj2GcRmbt98XFBRkKrasXLlSTZo0MY1duXKl5fXpFS1a1DQmNTXV8tlW10vS448/rmnTpmV6Tfr7Wj0vr6xatcrUUyolJUXr1683jU3/s8nrd5qXuWX2fba6/5UrV7LNwxFWxd3x48fr6aefNsSs+qEBAPCwYwkhAAAoEFWqVDHFpk+fbv/73bt39cYbb1jOYnHEBx98oOPHj9uPU1NTNXToUENBSpKqV69uOYPK19dXnTt3NsRmz55tOC5WrJh69uxp+fzu3bubYlOmTDHtCBcTE6O9e/caYp6engoLCzPErAokhw8ftny2JIWGhsrX19cQmzVrlr744otMl7bt27dPY8aMUaVKlTK9b16YPn26du/ebYi99957poJPiRIlDLsm5vU7zcvcfH19VaJECcOYs2fPaunSpfbjy5cv6+233842B0e5ubmZYvv27TMcHzx4UG+88UaePRMAAGfBDCwAAFAgQkJCtGzZMkMsJiZGhw8fVtWqVbVp06YsCzTZOX/+vBo0aKCIiAiVLFlS8fHxSkhIMI0bMmRIpveIiorSwoULMz3fvXt3FS9e3PJcr169NG7cOMNnuHLlipo1a6ZOnTqpXLly2rdvn7Zu3Wq6Njo62rSssXr16qZxS5YsUVhYmGrUqCEXl3v/jBs3bpy8vLzk5eWlt956S2+++aZ9/N27d/XKK69o/Pjxatq0qcqUKaObN2/q9OnT2rdvX64bxzvqxo0batWqlcLDw1WuXDnt2rXLVDSSpL/97W8qVqyY/Tiv32le5maz2dSmTRstX77cMK5Xr16KjIyUh4eHvv/++zx9x1Yzzz744ANt2rRJNWvW1MmTJ7VmzRqlpKTk2TMBAHAWtjRHOzoCAIACEdX7l8JO4YHMmf+EQ+MuXLigJ554QlevXs1ynJ+fn2m3t9mzZxuW7lWuXFknTpwwjKldu3a2BbAWLVpo8+bNmS6XS01NVYUKFTLdkW/t2rWm5Wbp/fTTTwoODtbly5ezzCO91q1ba926dabZNXfv3lXVqlVNnzOjCxcu2As1d+/eVWRkpGEWkKPy6p+EoaGh2rhxoyHmyM+matWq2rdvn2nmWV6+07zObfXq1ablexkVLVpUPj4+unTpkiH+66+/qnLlyvbjxMRE0yzFkJAQxcXF2Y+vX7+uJ554ItsdIzt27KjVq1dneS/J+n1kzAsAAGfBEkIAAFAg/P399a9//cs+cygjm82mN99809THylGrVq1S7dq1Mz3fqFEjLV++PMteT0WLFtWLL75oee7xxx/XU089lWUOgYGB2rlzpxo3bpxtvjabTf3799fq1astl4YVKVJEkydPtmwWnpkiRYpo4cKFevvtt+Xq6urwNe3bt3f4GTnx9ddfq3Xr1pmer1y5smJjYy2XTeblO83r3Dp27JjljD5vb2/Nnz9f9evXdyiX7Hh5eWnRokWmpYvpde7cWZ988kmePA8AAGdCAQsAABSYHj16aOvWrYqMjFSZMmXk6uqqgIAA9enTR1u3btWHH36Y43tXrFhRu3fv1ocffqhGjRqpePHi8vb2VuPGjfXxxx9rx44dDi0p69evn2W8b9++Du3QVr16de3evVvLly9Xv379VLNmTfn4+MjFxUV+fn5q2rSphg8frgMHDmjWrFmGZWkZ9ejRQxs3blRkZKTKly/vUFHKxcVF48aN04kTJzRu3Dh16NBB5cuXl6enp1xdXVW6dGk1atRIL774or788kudOnUqx43zHVWiRAnFxcVp+vTpCg4OVsmSJeXp6ak6depozJgxOnDggGVfsvvy8p3mdW7Tpk3T/PnzFRISohIlSsjDw0PVq1fXkCFDlJCQoB49ejzQu8pOq1attG/fPg0YMECVKlWSm5ub/Pz8FBISojlz5mjp0qUOF+8AAHiYsIQQAAAn86guIcxLVksI+SeNc3DmZWnOnBsAAMgaM7AAAAAAAADg1ChgAQAAAAAAwKlRwAIAAAAAAIBTs94GCAAAAH9JgYGBD3xN+fLltW7dunzIBgAA4B4KWAAAALA7cuTIA19z69atfMgEAADg/0cBCwAAPHQSExMLOwVkIi4urrBTyJQz5wYAALJGAQsAAAB2aWlphZ0CAACACU3cAQAAAAAA4NQoYAEAAAAAAMCpUcACAAAAAACAU6OABQAAAAAAAKdGAQsAAAAAAABOjQIWAAAAAAAAnJotjb2SAQAAAAAA4MSYgQUAAAAAAACnRgELAAAAAAAATo0CFgAAAAAAAJwaBSwAAAAAAAA4NQpYAAAAAAAAcGoUsAAAAAAAAODUKGABAAAAAADAqf1/CQUuwTNTcIAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 400,
       "width": 600
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure Size: (600 x 400)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p9.ggplot(acc_df,\n",
    "  p9.aes(x = 'model', y = 'mean', fill='class')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.ggtitle(\"Accuracy\") + \\\n",
    "  p9.geom_bar(stat = 'identity', position = p9.position_dodge()) + \\\n",
    "  p9.geom_errorbar(p9.aes(ymin = 'mean - std', ymax = 'mean + std'), position = p9.position_dodge2(), width = .9) + \\\n",
    "  p9.xlab('Model') + \\\n",
    "  p9.ylab('Value') + \\\n",
    "  p9.ylim(0, 1) + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'bottom',\n",
    "    legend_direction='vertical',\n",
    "    figure_size = (6, 4),\n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15),\n",
    "    legend_box_margin = -6,    \n",
    "    text = p9.element_text(size=10,  weight='bold')\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 6 x 4 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\acc.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(acc_df,\n",
    "  p9.aes(x = 'model', y = 'mean', fill='class')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.ggtitle(\"Accuracy\") + \\\n",
    "  p9.geom_bar(stat = 'identity', position = p9.position_dodge()) + \\\n",
    "  p9.geom_errorbar(p9.aes(ymin = 'mean - std', ymax = 'mean + std'), position = p9.position_dodge2(), width = .9) + \\\n",
    "  p9.xlab('Model') + \\\n",
    "  p9.ylab('Value') + \\\n",
    "  p9.ylim(0, 1) + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'bottom',\n",
    "    legend_direction='vertical',\n",
    "    figure_size = (6, 4),\n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15),\n",
    "    legend_box_margin = -6,    \n",
    "    text = p9.element_text(size=10,  weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\acc.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 9 x 10 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\upvote_popular2.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(other_metrics_df[other_metrics_df['class'] == 'upvote_popular'],\n",
    "  p9.aes(x = 'model', y = 'mean', fill = 'label')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.ggtitle(\"upvote_popular\") + \\\n",
    "  p9.geom_bar(stat = 'identity', position = p9.position_dodge()) + \\\n",
    "  p9.geom_errorbar(p9.aes(ymin = 'mean - std', ymax = 'mean + std'), position = p9.position_dodge2(), width = .9) + \\\n",
    "  p9.xlab('Model') + \\\n",
    "  p9.ylab('Value') + \\\n",
    "  p9.facet_wrap(facets = '~metric_name', ncol = 1) + \\\n",
    "  p9.ylim(0, 1.1) + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'bottom',\n",
    "    legend_direction='horizontal',\n",
    "    figure_size = (9, 10),\n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15),\n",
    "    text = p9.element_text(size=25,  weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\upvote_popular2.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 9 x 10 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\downvote_popular2.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(other_metrics_df[other_metrics_df['class'] == 'downvote_popular'],\n",
    "  p9.aes(x = 'model', y = 'mean', fill = 'label')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.ggtitle(\"downvote_popular\") + \\\n",
    "  p9.geom_bar(stat = 'identity', position = p9.position_dodge()) + \\\n",
    "  p9.geom_errorbar(p9.aes(ymin = 'mean - std', ymax = 'mean + std'), position = p9.position_dodge2(), width = .9) + \\\n",
    "  p9.xlab('Model') + \\\n",
    "  p9.ylab('Value') + \\\n",
    "  p9.facet_wrap(facets = '~metric_name', ncol = 1) + \\\n",
    "  p9.ylim(0, 1.1) + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'bottom',\n",
    "    legend_direction='horizontal',\n",
    "    figure_size = (9, 10),\n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15),\n",
    "    text = p9.element_text(size=25,  weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\downvote_popular2.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 9 x 10 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\comments_popular2.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(other_metrics_df[other_metrics_df['class'] == 'comments_popular'],\n",
    "  p9.aes(x = 'model', y = 'mean', fill = 'label')\n",
    "  ) + \\\n",
    "  p9.theme_bw() + \\\n",
    "  p9.ggtitle(\"comments_popular\") + \\\n",
    "  p9.geom_bar(stat = 'identity', position = p9.position_dodge()) + \\\n",
    "  p9.geom_errorbar(p9.aes(ymin = 'mean - std', ymax = 'mean + std'), position = p9.position_dodge2(), width = .9) + \\\n",
    "  p9.xlab('Model') + \\\n",
    "  p9.ylab('Value') + \\\n",
    "  p9.facet_wrap(facets = '~metric_name', ncol = 1) + \\\n",
    "  p9.ylim(0, 1.1) + \\\n",
    "  p9.theme(\n",
    "    legend_position = 'bottom',\n",
    "    legend_direction='horizontal',\n",
    "    figure_size = (9, 10),\n",
    "    axis_text_x  = p9.element_text(angle = 30, hjust = 15),\n",
    "    text = p9.element_text(size=25,  weight='bold')\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\comments_popular2.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>6</th>\n",
       "      <th>4</th>\n",
       "      <th>text_2</th>\n",
       "      <th>0</th>\n",
       "      <th>title_0</th>\n",
       "      <th>title_14</th>\n",
       "      <th>title_9</th>\n",
       "      <th>1</th>\n",
       "      <th>title_11</th>\n",
       "      <th>...</th>\n",
       "      <th>text_3</th>\n",
       "      <th>edited</th>\n",
       "      <th>title_19</th>\n",
       "      <th>text_5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>title_length</th>\n",
       "      <th>image</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003268</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.147865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002345</td>\n",
       "      <td>-0.003137</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003268</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.028197</td>\n",
       "      <td>0.133620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.143611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003268</td>\n",
       "      <td>-0.003137</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.029185</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>0.145844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.003870</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.002535</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.130852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.001801</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>0.025075</td>\n",
       "      <td>0.149342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.151687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.150281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.002066</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.139509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>-0.002403</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>0.021203</td>\n",
       "      <td>0.142894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        self         6         4    text_2         0   title_0  title_14  \\\n",
       "0  -0.003268 -0.003398 -0.001333  0.003542 -0.000193 -0.000600 -0.002667   \n",
       "1   0.002345 -0.003137 -0.000733 -0.003268  0.000870 -0.001333 -0.003398   \n",
       "2  -0.001066 -0.000466 -0.001465 -0.000330 -0.000330 -0.001333 -0.000133   \n",
       "3  -0.003268 -0.003137 -0.001465 -0.000193 -0.001200 -0.000600  0.001470   \n",
       "4  -0.000466 -0.003870 -0.001465  0.001286 -0.002535  0.001470 -0.001333   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.001801 -0.001801 -0.002196  0.002028  0.000135  0.002206 -0.001934   \n",
       "96 -0.003527 -0.001667 -0.001465  0.001008  0.000000  0.000734  0.000000   \n",
       "97 -0.004255 -0.001200 -0.001465  0.001607  0.000408 -0.001200 -0.001333   \n",
       "98 -0.002667 -0.002667 -0.001465  0.000686 -0.000865 -0.002066  0.000870   \n",
       "99 -0.001200 -0.000600 -0.001333 -0.001532 -0.002403 -0.000466 -0.001333   \n",
       "\n",
       "     title_9         1  title_11  ...    text_3    edited  title_19    text_5  \\\n",
       "0  -0.001333 -0.000466 -0.000466  ...  0.005929  0.007100  0.004281  0.010964   \n",
       "1   0.000000 -0.001333  0.001333  ...  0.005164  0.002627  0.003683  0.005907   \n",
       "2   0.001470  0.002944 -0.001333  ...  0.005929  0.008291  0.007262  0.007100   \n",
       "3   0.000734 -0.001333  0.000000  ...  0.004878  0.003683  0.004140  0.005617   \n",
       "4  -0.000733 -0.000600 -0.001200  ...  0.002068  0.004567  0.005475  0.005020   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.000135  0.003542 -0.000466  ...  0.002206  0.007843  0.003085  0.004423   \n",
       "96 -0.000733  0.001470 -0.001465  ...  0.004281  0.004113  0.002944  0.004878   \n",
       "97  0.000000  0.001607 -0.000133  ...  0.004597  0.002944  0.003542  0.001607   \n",
       "98  0.000870  0.000734 -0.001333  ...  0.007405  0.006214  0.003542  0.005020   \n",
       "99  0.000000 -0.001066  0.001333  ...  0.005761  0.004423  0.004140  0.001333   \n",
       "\n",
       "           7         8  title_length     image  text_length  post_year  \n",
       "0   0.003264  0.003683      0.013894  0.018131     0.019139   0.147865  \n",
       "1   0.005333  0.005761      0.028587  0.019728     0.028197   0.133620  \n",
       "2   0.005055  0.007247      0.014749  0.022082     0.018661   0.143611  \n",
       "3   0.003725  0.005617      0.029185  0.021080     0.026671   0.145844  \n",
       "4   0.005650  0.006054      0.019207  0.018377     0.024914   0.130852  \n",
       "..       ...       ...           ...       ...          ...        ...  \n",
       "95  0.005789  0.006799      0.014586  0.021080     0.025075   0.149342  \n",
       "96  0.002395  0.002944      0.015418  0.016689     0.023430   0.151687  \n",
       "97  0.004321  0.003226      0.016433  0.021668     0.027613   0.150281  \n",
       "98  0.003862  0.001746      0.020906  0.021433     0.019728   0.139509  \n",
       "99  0.008594  0.006650      0.010752  0.019312     0.021203   0.142894  \n",
       "\n",
       "[100 rows x 57 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps = pd.read_csv(r\"C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\importances_up.csv\")\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>karma_High</th>\n",
       "      <th>text_S_neutral</th>\n",
       "      <th>text_T_dont</th>\n",
       "      <th>title_S_negative</th>\n",
       "      <th>title_T_anxieti</th>\n",
       "      <th>title_T_social</th>\n",
       "      <th>title_T_life</th>\n",
       "      <th>title_S_neutral</th>\n",
       "      <th>title_T_peopl</th>\n",
       "      <th>...</th>\n",
       "      <th>text_T_feel</th>\n",
       "      <th>edited</th>\n",
       "      <th>title_T_today</th>\n",
       "      <th>text_T_life</th>\n",
       "      <th>karma_Low</th>\n",
       "      <th>karma_Medium</th>\n",
       "      <th>title_length</th>\n",
       "      <th>image</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003268</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.010964</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.147865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002345</td>\n",
       "      <td>-0.003137</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.003268</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005164</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.028197</td>\n",
       "      <td>0.133620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001066</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005929</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.005055</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.022082</td>\n",
       "      <td>0.018661</td>\n",
       "      <td>0.143611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.003268</td>\n",
       "      <td>-0.003137</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.003725</td>\n",
       "      <td>0.005617</td>\n",
       "      <td>0.029185</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>0.026671</td>\n",
       "      <td>0.145844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.003870</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>-0.002535</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.005475</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.006054</td>\n",
       "      <td>0.019207</td>\n",
       "      <td>0.018377</td>\n",
       "      <td>0.024914</td>\n",
       "      <td>0.130852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.001801</td>\n",
       "      <td>-0.001801</td>\n",
       "      <td>-0.002196</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.006799</td>\n",
       "      <td>0.014586</td>\n",
       "      <td>0.021080</td>\n",
       "      <td>0.025075</td>\n",
       "      <td>0.149342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.001667</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000733</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.004113</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.015418</td>\n",
       "      <td>0.016689</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.151687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.004255</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004597</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.004321</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.021668</td>\n",
       "      <td>0.027613</td>\n",
       "      <td>0.150281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>-0.002066</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.006214</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.001746</td>\n",
       "      <td>0.020906</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.019728</td>\n",
       "      <td>0.139509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.001200</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.001532</td>\n",
       "      <td>-0.002403</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005761</td>\n",
       "      <td>0.004423</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.006650</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>0.019312</td>\n",
       "      <td>0.021203</td>\n",
       "      <td>0.142894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        self  karma_High  text_S_neutral  text_T_dont  title_S_negative  \\\n",
       "0  -0.003268   -0.003398       -0.001333     0.003542         -0.000193   \n",
       "1   0.002345   -0.003137       -0.000733    -0.003268          0.000870   \n",
       "2  -0.001066   -0.000466       -0.001465    -0.000330         -0.000330   \n",
       "3  -0.003268   -0.003137       -0.001465    -0.000193         -0.001200   \n",
       "4  -0.000466   -0.003870       -0.001465     0.001286         -0.002535   \n",
       "..       ...         ...             ...          ...               ...   \n",
       "95 -0.001801   -0.001801       -0.002196     0.002028          0.000135   \n",
       "96 -0.003527   -0.001667       -0.001465     0.001008          0.000000   \n",
       "97 -0.004255   -0.001200       -0.001465     0.001607          0.000408   \n",
       "98 -0.002667   -0.002667       -0.001465     0.000686         -0.000865   \n",
       "99 -0.001200   -0.000600       -0.001333    -0.001532         -0.002403   \n",
       "\n",
       "    title_T_anxieti  title_T_social  title_T_life  title_S_neutral  \\\n",
       "0         -0.000600       -0.002667     -0.001333        -0.000466   \n",
       "1         -0.001333       -0.003398      0.000000        -0.001333   \n",
       "2         -0.001333       -0.000133      0.001470         0.002944   \n",
       "3         -0.000600        0.001470      0.000734        -0.001333   \n",
       "4          0.001470       -0.001333     -0.000733        -0.000600   \n",
       "..              ...             ...           ...              ...   \n",
       "95         0.002206       -0.001934      0.000135         0.003542   \n",
       "96         0.000734        0.000000     -0.000733         0.001470   \n",
       "97        -0.001200       -0.001333      0.000000         0.001607   \n",
       "98        -0.002066        0.000870      0.000870         0.000734   \n",
       "99        -0.000466       -0.001333      0.000000        -0.001066   \n",
       "\n",
       "    title_T_peopl  ...  text_T_feel    edited  title_T_today  text_T_life  \\\n",
       "0       -0.000466  ...     0.005929  0.007100       0.004281     0.010964   \n",
       "1        0.001333  ...     0.005164  0.002627       0.003683     0.005907   \n",
       "2       -0.001333  ...     0.005929  0.008291       0.007262     0.007100   \n",
       "3        0.000000  ...     0.004878  0.003683       0.004140     0.005617   \n",
       "4       -0.001200  ...     0.002068  0.004567       0.005475     0.005020   \n",
       "..            ...  ...          ...       ...            ...          ...   \n",
       "95      -0.000466  ...     0.002206  0.007843       0.003085     0.004423   \n",
       "96      -0.001465  ...     0.004281  0.004113       0.002944     0.004878   \n",
       "97      -0.000133  ...     0.004597  0.002944       0.003542     0.001607   \n",
       "98      -0.001333  ...     0.007405  0.006214       0.003542     0.005020   \n",
       "99       0.001333  ...     0.005761  0.004423       0.004140     0.001333   \n",
       "\n",
       "    karma_Low  karma_Medium  title_length     image  text_length  post_year  \n",
       "0    0.003264      0.003683      0.013894  0.018131     0.019139   0.147865  \n",
       "1    0.005333      0.005761      0.028587  0.019728     0.028197   0.133620  \n",
       "2    0.005055      0.007247      0.014749  0.022082     0.018661   0.143611  \n",
       "3    0.003725      0.005617      0.029185  0.021080     0.026671   0.145844  \n",
       "4    0.005650      0.006054      0.019207  0.018377     0.024914   0.130852  \n",
       "..        ...           ...           ...       ...          ...        ...  \n",
       "95   0.005789      0.006799      0.014586  0.021080     0.025075   0.149342  \n",
       "96   0.002395      0.002944      0.015418  0.016689     0.023430   0.151687  \n",
       "97   0.004321      0.003226      0.016433  0.021668     0.027613   0.150281  \n",
       "98   0.003862      0.001746      0.020906  0.021433     0.019728   0.139509  \n",
       "99   0.008594      0.006650      0.010752  0.019312     0.021203   0.142894  \n",
       "\n",
       "[100 rows x 57 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = ['title_S_negative', 'title_S_neutral', 'title_S_positive', 'text_S_negative', 'text_S_neutral', 'text_S_positive',\n",
    "        'karma_High', 'karma_Low', 'karma_Medium', 'karma_Very high', 'karma_Very low']\n",
    "imps.rename(columns={'0': cats[0], '1': cats[1], '2': cats[2], '3': cats[3],\n",
    "                   '4': cats[4], '5': cats[5], '6': cats[6], '7': cats[7], '8': cats[8],\n",
    "                   '9': cats[9], '10': cats[10]}, inplace=True)\n",
    "text = ['text_T_anxieti', 'text_T_day', 'text_T_dont', 'text_T_feel', 'text_T_ive', 'text_T_life', 'text_T_peopl', 'text_T_start', 'text_T_thing', 'text_T_time',\n",
    "        'text_T_work', 'text_T_year']\n",
    "\n",
    "imps.rename(columns={'text_0': text[0], 'text_1': text[1], 'text_2': text[2], 'text_3': text[3],\n",
    "                   'text_4': text[4], 'text_5': text[5],'text_6': text[6],  'text_7': text[7], 'text_8': text[8],\n",
    "                   'text_9': text[9], 'text_10': text[10], 'text_11': text[11]}, inplace=True)\n",
    "title = ['title_T_anxieti', 'title_T_day', 'title_T_depress', 'title_T_dont', 'title_T_feel', 'title_T_friend', 'title_T_hate', 'title_T_health', 'title_T_ill',\n",
    "        'title_T_life', 'title_T_mental', 'title_T_peopl', 'title_T_person', 'title_T_ptsd', 'title_T_social', 'title_T_suicid', 'title_T_talk', 'title_T_thing',\n",
    "        'title_T_time', 'title_T_today', 'title_T_year']\n",
    "\n",
    "imps.rename(columns={'title_0': title[0], 'title_1': title[1], 'title_2': title[2], 'title_3': title[3],\n",
    "                   'title_4': title[4], 'title_5': title[5], 'title_6': title[6], 'title_7': title[7], 'title_8': title[8],\n",
    "                   'title_9': title[9], 'title_10': title[10],\n",
    "                   'title_11': title[11], 'title_12': title[12], 'title_13': title[13],\n",
    "                   'title_14': title[14], 'title_15': title[15], 'title_16': title[16], 'title_17': title[17], 'title_18': title[18],\n",
    "                   'title_19': title[19], 'title_20': title[20]}, inplace=True)\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 10 x 4 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\feat_imp_up.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(imps.iloc[:, -11:].melt(),\n",
    "  p9.aes(x = 'reorder(variable, value)', y = 'value', group = 'variable')\n",
    "  ) + \\\n",
    "  p9.theme_gray() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.ggtitle(\"Feature Importance for upvote_popular\") + \\\n",
    "  p9.geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \\\n",
    "  p9.coord_flip() + \\\n",
    "  p9.xlab('Feature') + \\\n",
    "  p9.ylab('Change in F1-score') + \\\n",
    "  p9.ylim(-0.007, 0.175)+ \\\n",
    "  p9.theme(\n",
    "    legend_title = p9.element_blank(),\n",
    "    legend_position = 'top',\n",
    "    legend_box_spacing = 0,\n",
    "    legend_box_margin = 1,\n",
    "    figure_size = (10, 4),\n",
    "    text = p9.element_text(size=12)\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\feat_imp_up.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spoiler</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "      <th>original</th>\n",
       "      <th>stickied</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>locked</th>\n",
       "      <th>self</th>\n",
       "      <th>post_month</th>\n",
       "      <th>...</th>\n",
       "      <th>8</th>\n",
       "      <th>video</th>\n",
       "      <th>2</th>\n",
       "      <th>9</th>\n",
       "      <th>3</th>\n",
       "      <th>6</th>\n",
       "      <th>10</th>\n",
       "      <th>title_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.040023</td>\n",
       "      <td>0.065270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>0.044656</td>\n",
       "      <td>0.064269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.001939</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>0.060292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002512</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.069642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.060790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>-0.004410</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.035221</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.074340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.068820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>0.063645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.021568</td>\n",
       "      <td>0.045034</td>\n",
       "      <td>0.074003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004997</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.019582</td>\n",
       "      <td>0.028666</td>\n",
       "      <td>0.059381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     spoiler         5         4  original  stickied         0         1  \\\n",
       "0  -0.001382 -0.000569 -0.000557       0.0       0.0 -0.000534 -0.000286   \n",
       "1  -0.000825  0.002764 -0.001113       0.0       0.0  0.003318 -0.000268   \n",
       "2  -0.000825  0.001095  0.000000       0.0       0.0 -0.000569 -0.001939   \n",
       "3  -0.000825 -0.001681 -0.000825       0.0       0.0 -0.002512  0.005260   \n",
       "4  -0.001382 -0.002238 -0.000557       0.0       0.0  0.000265  0.000023   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.000825 -0.002238 -0.000286       0.0       0.0  0.007513 -0.004410   \n",
       "96 -0.001382  0.002764  0.000000       0.0       0.0  0.002759  0.001382   \n",
       "97  0.000000 -0.001113  0.000556       0.0       0.0 -0.002238  0.002210   \n",
       "98  0.000000  0.001382  0.000556       0.0       0.0  0.007773 -0.004439   \n",
       "99 -0.000825  0.001929  0.000556       0.0       0.0 -0.004997  0.002204   \n",
       "\n",
       "      locked      self  post_month  ...         8     video         2  \\\n",
       "0   0.001098  0.000556    0.003917  ... -0.000569  0.003037  0.002204   \n",
       "1   0.001098 -0.000825    0.004146  ...  0.000543  0.001382  0.004698   \n",
       "2   0.001098  0.003319    0.000833  ...  0.001650  0.003867  0.004698   \n",
       "3   0.001098 -0.001382    0.000262  ...  0.004145  0.002764  0.005812   \n",
       "4   0.001098 -0.000557    0.012201  ...  0.002209  0.002483 -0.000557   \n",
       "..       ...       ...         ...  ...       ...       ...       ...   \n",
       "95  0.001098 -0.001113    0.001958  ...  0.001650  0.001654  0.004425   \n",
       "96  0.001098  0.003048    0.008292  ...  0.007468  0.002764  0.004145   \n",
       "97  0.001098  0.001668    0.004174  ... -0.000534  0.005528  0.002759   \n",
       "98  0.000270  0.005251    0.005812  ...  0.003335  0.001654  0.005528   \n",
       "99  0.001098 -0.000557    0.009248  ...  0.000543  0.004421  0.001098   \n",
       "\n",
       "           9         3         6        10  title_length  text_length  \\\n",
       "0   0.000580  0.001941  0.005528  0.011652      0.025670     0.040023   \n",
       "1   0.000270  0.009248  0.003313  0.006090      0.043151     0.044656   \n",
       "2   0.005808  0.003318  0.005808  0.009688      0.026355     0.043459   \n",
       "3   0.005000  0.006411  0.006636  0.003867      0.042423     0.031021   \n",
       "4   0.006635  0.004799  0.003867  0.009138      0.017725     0.032069   \n",
       "..       ...       ...       ...       ...           ...          ...   \n",
       "95 -0.001091  0.003360  0.004698  0.005531      0.035221     0.030309   \n",
       "96  0.001668  0.004757  0.002759  0.006096      0.039130     0.026742   \n",
       "97  0.006915  0.004707  0.008853  0.010524      0.039989     0.039657   \n",
       "98  0.004700  0.002210  0.008036  0.004700      0.021568     0.045034   \n",
       "99  0.002224 -0.002228  0.004698  0.016326      0.019582     0.028666   \n",
       "\n",
       "    post_year  \n",
       "0    0.065270  \n",
       "1    0.064269  \n",
       "2    0.060292  \n",
       "3    0.069642  \n",
       "4    0.060790  \n",
       "..        ...  \n",
       "95   0.074340  \n",
       "96   0.068820  \n",
       "97   0.063645  \n",
       "98   0.074003  \n",
       "99   0.059381  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps = pd.read_csv(r\"C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\importances_down.csv\")\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spoiler</th>\n",
       "      <th>text_S_positive</th>\n",
       "      <th>text_S_neutral</th>\n",
       "      <th>original</th>\n",
       "      <th>stickied</th>\n",
       "      <th>title_S_negative</th>\n",
       "      <th>title_S_neutral</th>\n",
       "      <th>locked</th>\n",
       "      <th>self</th>\n",
       "      <th>post_month</th>\n",
       "      <th>...</th>\n",
       "      <th>karma_Medium</th>\n",
       "      <th>video</th>\n",
       "      <th>title_S_positive</th>\n",
       "      <th>karma_Very high</th>\n",
       "      <th>text_S_negative</th>\n",
       "      <th>karma_High</th>\n",
       "      <th>karma_Very low</th>\n",
       "      <th>title_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.025670</td>\n",
       "      <td>0.040023</td>\n",
       "      <td>0.065270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.000268</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0.043151</td>\n",
       "      <td>0.044656</td>\n",
       "      <td>0.064269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000569</td>\n",
       "      <td>-0.001939</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043459</td>\n",
       "      <td>0.060292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002512</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.001382</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.069642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.012201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>0.017725</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>0.060790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>-0.004410</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.001091</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.005531</td>\n",
       "      <td>0.035221</td>\n",
       "      <td>0.030309</td>\n",
       "      <td>0.074340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.008292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.004757</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.006096</td>\n",
       "      <td>0.039130</td>\n",
       "      <td>0.026742</td>\n",
       "      <td>0.068820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001113</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002238</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.004174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.006915</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.039989</td>\n",
       "      <td>0.039657</td>\n",
       "      <td>0.063645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>-0.004439</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.005812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003335</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.008036</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.021568</td>\n",
       "      <td>0.045034</td>\n",
       "      <td>0.074003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.000825</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004997</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.004421</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>-0.002228</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>0.019582</td>\n",
       "      <td>0.028666</td>\n",
       "      <td>0.059381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     spoiler  text_S_positive  text_S_neutral  original  stickied  \\\n",
       "0  -0.001382        -0.000569       -0.000557       0.0       0.0   \n",
       "1  -0.000825         0.002764       -0.001113       0.0       0.0   \n",
       "2  -0.000825         0.001095        0.000000       0.0       0.0   \n",
       "3  -0.000825        -0.001681       -0.000825       0.0       0.0   \n",
       "4  -0.001382        -0.002238       -0.000557       0.0       0.0   \n",
       "..       ...              ...             ...       ...       ...   \n",
       "95 -0.000825        -0.002238       -0.000286       0.0       0.0   \n",
       "96 -0.001382         0.002764        0.000000       0.0       0.0   \n",
       "97  0.000000        -0.001113        0.000556       0.0       0.0   \n",
       "98  0.000000         0.001382        0.000556       0.0       0.0   \n",
       "99 -0.000825         0.001929        0.000556       0.0       0.0   \n",
       "\n",
       "    title_S_negative  title_S_neutral    locked      self  post_month  ...  \\\n",
       "0          -0.000534        -0.000286  0.001098  0.000556    0.003917  ...   \n",
       "1           0.003318        -0.000268  0.001098 -0.000825    0.004146  ...   \n",
       "2          -0.000569        -0.001939  0.001098  0.003319    0.000833  ...   \n",
       "3          -0.002512         0.005260  0.001098 -0.001382    0.000262  ...   \n",
       "4           0.000265         0.000023  0.001098 -0.000557    0.012201  ...   \n",
       "..               ...              ...       ...       ...         ...  ...   \n",
       "95          0.007513        -0.004410  0.001098 -0.001113    0.001958  ...   \n",
       "96          0.002759         0.001382  0.001098  0.003048    0.008292  ...   \n",
       "97         -0.002238         0.002210  0.001098  0.001668    0.004174  ...   \n",
       "98          0.007773        -0.004439  0.000270  0.005251    0.005812  ...   \n",
       "99         -0.004997         0.002204  0.001098 -0.000557    0.009248  ...   \n",
       "\n",
       "    karma_Medium     video  title_S_positive  karma_Very high  \\\n",
       "0      -0.000569  0.003037          0.002204         0.000580   \n",
       "1       0.000543  0.001382          0.004698         0.000270   \n",
       "2       0.001650  0.003867          0.004698         0.005808   \n",
       "3       0.004145  0.002764          0.005812         0.005000   \n",
       "4       0.002209  0.002483         -0.000557         0.006635   \n",
       "..           ...       ...               ...              ...   \n",
       "95      0.001650  0.001654          0.004425        -0.001091   \n",
       "96      0.007468  0.002764          0.004145         0.001668   \n",
       "97     -0.000534  0.005528          0.002759         0.006915   \n",
       "98      0.003335  0.001654          0.005528         0.004700   \n",
       "99      0.000543  0.004421          0.001098         0.002224   \n",
       "\n",
       "    text_S_negative  karma_High  karma_Very low  title_length  text_length  \\\n",
       "0          0.001941    0.005528        0.011652      0.025670     0.040023   \n",
       "1          0.009248    0.003313        0.006090      0.043151     0.044656   \n",
       "2          0.003318    0.005808        0.009688      0.026355     0.043459   \n",
       "3          0.006411    0.006636        0.003867      0.042423     0.031021   \n",
       "4          0.004799    0.003867        0.009138      0.017725     0.032069   \n",
       "..              ...         ...             ...           ...          ...   \n",
       "95         0.003360    0.004698        0.005531      0.035221     0.030309   \n",
       "96         0.004757    0.002759        0.006096      0.039130     0.026742   \n",
       "97         0.004707    0.008853        0.010524      0.039989     0.039657   \n",
       "98         0.002210    0.008036        0.004700      0.021568     0.045034   \n",
       "99        -0.002228    0.004698        0.016326      0.019582     0.028666   \n",
       "\n",
       "    post_year  \n",
       "0    0.065270  \n",
       "1    0.064269  \n",
       "2    0.060292  \n",
       "3    0.069642  \n",
       "4    0.060790  \n",
       "..        ...  \n",
       "95   0.074340  \n",
       "96   0.068820  \n",
       "97   0.063645  \n",
       "98   0.074003  \n",
       "99   0.059381  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = ['title_S_negative', 'title_S_neutral', 'title_S_positive', 'text_S_negative', 'text_S_neutral', 'text_S_positive',\n",
    "        'karma_High', 'karma_Low', 'karma_Medium', 'karma_Very high', 'karma_Very low']\n",
    "imps.rename(columns={'0': cats[0], '1': cats[1], '2': cats[2], '3': cats[3],\n",
    "                   '4': cats[4], '5': cats[5], '6': cats[6], '7': cats[7], '8': cats[8],\n",
    "                   '9': cats[9], '10': cats[10]}, inplace=True)\n",
    "text = ['text_T_anxieti', 'text_T_day', 'text_T_dont', 'text_T_feel', 'text_T_ive', 'text_T_life', 'text_T_peopl', 'text_T_start', 'text_T_thing', 'text_T_time',\n",
    "        'text_T_work', 'text_T_year']\n",
    "\n",
    "imps.rename(columns={'text_0': text[0], 'text_1': text[1], 'text_2': text[2], 'text_3': text[3],\n",
    "                   'text_4': text[4], 'text_5': text[5],'text_6': text[6],  'text_7': text[7], 'text_8': text[8],\n",
    "                   'text_9': text[9], 'text_10': text[10], 'text_11': text[11]}, inplace=True)\n",
    "title = ['title_T_anxieti', 'title_T_day', 'title_T_depress', 'title_T_dont', 'title_T_feel', 'title_T_friend', 'title_T_hate', 'title_T_health', 'title_T_ill',\n",
    "        'title_T_life', 'title_T_mental', 'title_T_peopl', 'title_T_person', 'title_T_ptsd', 'title_T_social', 'title_T_suicid', 'title_T_talk', 'title_T_thing',\n",
    "        'title_T_time', 'title_T_today', 'title_T_year']\n",
    "\n",
    "imps.rename(columns={'title_0': title[0], 'title_1': title[1], 'title_2': title[2], 'title_3': title[3],\n",
    "                   'title_4': title[4], 'title_5': title[5], 'title_6': title[6], 'title_7': title[7], 'title_8': title[8],\n",
    "                   'title_9': title[9], 'title_10': title[10],\n",
    "                   'title_11': title[11], 'title_12': title[12], 'title_13': title[13],\n",
    "                   'title_14': title[14], 'title_15': title[15], 'title_16': title[16], 'title_17': title[17], 'title_18': title[18],\n",
    "                   'title_19': title[19], 'title_20': title[20]}, inplace=True)\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 10 x 4 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\feat_imp_down.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(imps.iloc[:, -11:].melt(),\n",
    "  p9.aes(x = 'reorder(variable, value)', y = 'value', group = 'variable')\n",
    "  ) + \\\n",
    "  p9.theme_gray() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.ggtitle(\"Feature Importance for downvote_popular\") + \\\n",
    "  p9.geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \\\n",
    "  p9.coord_flip() + \\\n",
    "  p9.xlab('Feature') + \\\n",
    "  p9.ylab('Change in F1-score') + \\\n",
    "  p9.ylim(-0.007, 0.175)+ \\\n",
    "  p9.theme(\n",
    "    legend_title = p9.element_blank(),\n",
    "    legend_position = 'top',\n",
    "    legend_box_spacing = 0,\n",
    "    legend_box_margin = 1,\n",
    "    figure_size = (10, 4),\n",
    "    text = p9.element_text(size=12)\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\feat_imp_down.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>self</th>\n",
       "      <th>0</th>\n",
       "      <th>locked</th>\n",
       "      <th>original</th>\n",
       "      <th>stickied</th>\n",
       "      <th>1</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>4</th>\n",
       "      <th>3</th>\n",
       "      <th>...</th>\n",
       "      <th>7</th>\n",
       "      <th>5</th>\n",
       "      <th>edited</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>post_month</th>\n",
       "      <th>image</th>\n",
       "      <th>title_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.030239</td>\n",
       "      <td>0.093996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.004933</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.087357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000502</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>-0.006595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.021194</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.105953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.096430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001873</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.021291</td>\n",
       "      <td>0.015970</td>\n",
       "      <td>0.087777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.001557</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.022280</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.092093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.004291</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.005251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>0.091124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.097859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.008246</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003311</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.090233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.001557</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.109624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           2      self         0    locked  original  stickied         1  \\\n",
       "0  -0.000306 -0.004050  0.000371  0.000000       0.0       0.0 -0.001374   \n",
       "1   0.001933 -0.004933  0.001120  0.000000       0.0       0.0  0.002711   \n",
       "2  -0.000502 -0.001936 -0.006595  0.000000       0.0       0.0  0.003565   \n",
       "3  -0.002997 -0.000995  0.002492  0.000000       0.0       0.0  0.000255   \n",
       "4  -0.001873 -0.004291  0.001890  0.000000       0.0       0.0 -0.000048   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95 -0.002681 -0.001557 -0.003623 -0.000813       0.0       0.0  0.000868   \n",
       "96 -0.004291 -0.004369 -0.005251  0.000000       0.0       0.0 -0.001936   \n",
       "97 -0.002997 -0.004291 -0.000813  0.000000       0.0       0.0  0.000560   \n",
       "98 -0.002997 -0.001063 -0.008246  0.000560       0.0       0.0 -0.003311   \n",
       "99 -0.001557 -0.004369 -0.003933  0.000000       0.0       0.0  0.001374   \n",
       "\n",
       "     spoiler         4         3  ...         7         5    edited         6  \\\n",
       "0   0.000560  0.000560  0.000815  ...  0.003799  0.002190  0.003799  0.005469   \n",
       "1   0.000560 -0.000561  0.005181  ...  0.003241  0.004090  0.006564  0.004090   \n",
       "2   0.000560  0.000000 -0.001374  ...  0.002152 -0.001122  0.000212  0.004356   \n",
       "3   0.000560 -0.000252  0.002152  ...  0.004913  0.001331  0.004625  0.005181   \n",
       "4   0.001374  0.003007  0.004896  ...  0.002711  0.004356  0.004913  0.005204   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.002190  0.000255  0.001593  ...  0.003007  0.000212  0.005181  0.007392   \n",
       "96  0.000560  0.000000 -0.004438  ...  0.006293  0.001331  0.000474  0.005738   \n",
       "97  0.000560  0.002449  0.002416  ...  0.002152  0.003241  0.001890  0.005452   \n",
       "98  0.000560  0.000815  0.006837  ...  0.006837  0.004068  0.002449  0.004625   \n",
       "99  0.000560 -0.001683 -0.003623  ...  0.001631  0.004090  0.005204  0.006564   \n",
       "\n",
       "           8  post_month     image  title_length  text_length  post_year  \n",
       "0   0.009608    0.007065  0.008822      0.022918     0.030239   0.093996  \n",
       "1   0.007392    0.002170  0.005450      0.025448     0.023460   0.087357  \n",
       "2   0.007674    0.016592  0.008530      0.021194     0.026307   0.105953  \n",
       "3   0.003241   -0.003313  0.008795      0.021888     0.015032   0.096430  \n",
       "4   0.005181    0.010489  0.004333      0.021291     0.015970   0.087777  \n",
       "..       ...         ...       ...           ...          ...        ...  \n",
       "95  0.007404    0.013800  0.009375      0.022280     0.025448   0.092093  \n",
       "96  0.003826    0.004674  0.007133      0.019563     0.032896   0.091124  \n",
       "97  0.004941    0.002854  0.006291      0.001823     0.008530   0.097859  \n",
       "98  0.006837    0.006198  0.008822      0.019017     0.020859   0.090233  \n",
       "99  0.006836    0.004072  0.007977      0.019443     0.026597   0.109624  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imps = pd.read_csv(r\"C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\importances_com.csv\")\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_S_positive</th>\n",
       "      <th>self</th>\n",
       "      <th>title_S_negative</th>\n",
       "      <th>locked</th>\n",
       "      <th>original</th>\n",
       "      <th>stickied</th>\n",
       "      <th>title_S_neutral</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>text_S_neutral</th>\n",
       "      <th>text_S_negative</th>\n",
       "      <th>...</th>\n",
       "      <th>karma_Low</th>\n",
       "      <th>text_S_positive</th>\n",
       "      <th>edited</th>\n",
       "      <th>karma_High</th>\n",
       "      <th>karma_Medium</th>\n",
       "      <th>post_month</th>\n",
       "      <th>image</th>\n",
       "      <th>title_length</th>\n",
       "      <th>text_length</th>\n",
       "      <th>post_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.003799</td>\n",
       "      <td>0.005469</td>\n",
       "      <td>0.009608</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.022918</td>\n",
       "      <td>0.030239</td>\n",
       "      <td>0.093996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.004933</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.005450</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.087357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000502</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>-0.006595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003565</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001374</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>-0.001122</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.007674</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.021194</td>\n",
       "      <td>0.026307</td>\n",
       "      <td>0.105953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000252</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.021888</td>\n",
       "      <td>0.015032</td>\n",
       "      <td>0.096430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001873</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.004356</td>\n",
       "      <td>0.004913</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.010489</td>\n",
       "      <td>0.004333</td>\n",
       "      <td>0.021291</td>\n",
       "      <td>0.015970</td>\n",
       "      <td>0.087777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.002681</td>\n",
       "      <td>-0.001557</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.007392</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.013800</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>0.022280</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.092093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.004291</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.005251</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001936</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>0.091124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.004291</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>0.004941</td>\n",
       "      <td>0.002854</td>\n",
       "      <td>0.006291</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.008530</td>\n",
       "      <td>0.097859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.002997</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>-0.008246</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003311</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.004068</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.006837</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>0.008822</td>\n",
       "      <td>0.019017</td>\n",
       "      <td>0.020859</td>\n",
       "      <td>0.090233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-0.001557</td>\n",
       "      <td>-0.004369</td>\n",
       "      <td>-0.003933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.001683</td>\n",
       "      <td>-0.003623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.006564</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.004072</td>\n",
       "      <td>0.007977</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>0.109624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    title_S_positive      self  title_S_negative    locked  original  \\\n",
       "0          -0.000306 -0.004050          0.000371  0.000000       0.0   \n",
       "1           0.001933 -0.004933          0.001120  0.000000       0.0   \n",
       "2          -0.000502 -0.001936         -0.006595  0.000000       0.0   \n",
       "3          -0.002997 -0.000995          0.002492  0.000000       0.0   \n",
       "4          -0.001873 -0.004291          0.001890  0.000000       0.0   \n",
       "..               ...       ...               ...       ...       ...   \n",
       "95         -0.002681 -0.001557         -0.003623 -0.000813       0.0   \n",
       "96         -0.004291 -0.004369         -0.005251  0.000000       0.0   \n",
       "97         -0.002997 -0.004291         -0.000813  0.000000       0.0   \n",
       "98         -0.002997 -0.001063         -0.008246  0.000560       0.0   \n",
       "99         -0.001557 -0.004369         -0.003933  0.000000       0.0   \n",
       "\n",
       "    stickied  title_S_neutral   spoiler  text_S_neutral  text_S_negative  ...  \\\n",
       "0        0.0        -0.001374  0.000560        0.000560         0.000815  ...   \n",
       "1        0.0         0.002711  0.000560       -0.000561         0.005181  ...   \n",
       "2        0.0         0.003565  0.000560        0.000000        -0.001374  ...   \n",
       "3        0.0         0.000255  0.000560       -0.000252         0.002152  ...   \n",
       "4        0.0        -0.000048  0.001374        0.003007         0.004896  ...   \n",
       "..       ...              ...       ...             ...              ...  ...   \n",
       "95       0.0         0.000868  0.002190        0.000255         0.001593  ...   \n",
       "96       0.0        -0.001936  0.000560        0.000000        -0.004438  ...   \n",
       "97       0.0         0.000560  0.000560        0.002449         0.002416  ...   \n",
       "98       0.0        -0.003311  0.000560        0.000815         0.006837  ...   \n",
       "99       0.0         0.001374  0.000560       -0.001683        -0.003623  ...   \n",
       "\n",
       "    karma_Low  text_S_positive    edited  karma_High  karma_Medium  \\\n",
       "0    0.003799         0.002190  0.003799    0.005469      0.009608   \n",
       "1    0.003241         0.004090  0.006564    0.004090      0.007392   \n",
       "2    0.002152        -0.001122  0.000212    0.004356      0.007674   \n",
       "3    0.004913         0.001331  0.004625    0.005181      0.003241   \n",
       "4    0.002711         0.004356  0.004913    0.005204      0.005181   \n",
       "..        ...              ...       ...         ...           ...   \n",
       "95   0.003007         0.000212  0.005181    0.007392      0.007404   \n",
       "96   0.006293         0.001331  0.000474    0.005738      0.003826   \n",
       "97   0.002152         0.003241  0.001890    0.005452      0.004941   \n",
       "98   0.006837         0.004068  0.002449    0.004625      0.006837   \n",
       "99   0.001631         0.004090  0.005204    0.006564      0.006836   \n",
       "\n",
       "    post_month     image  title_length  text_length  post_year  \n",
       "0     0.007065  0.008822      0.022918     0.030239   0.093996  \n",
       "1     0.002170  0.005450      0.025448     0.023460   0.087357  \n",
       "2     0.016592  0.008530      0.021194     0.026307   0.105953  \n",
       "3    -0.003313  0.008795      0.021888     0.015032   0.096430  \n",
       "4     0.010489  0.004333      0.021291     0.015970   0.087777  \n",
       "..         ...       ...           ...          ...        ...  \n",
       "95    0.013800  0.009375      0.022280     0.025448   0.092093  \n",
       "96    0.004674  0.007133      0.019563     0.032896   0.091124  \n",
       "97    0.002854  0.006291      0.001823     0.008530   0.097859  \n",
       "98    0.006198  0.008822      0.019017     0.020859   0.090233  \n",
       "99    0.004072  0.007977      0.019443     0.026597   0.109624  \n",
       "\n",
       "[100 rows x 24 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = ['title_S_negative', 'title_S_neutral', 'title_S_positive', 'text_S_negative', 'text_S_neutral', 'text_S_positive',\n",
    "        'karma_High', 'karma_Low', 'karma_Medium', 'karma_Very high', 'karma_Very low']\n",
    "imps.rename(columns={'0': cats[0], '1': cats[1], '2': cats[2], '3': cats[3],\n",
    "                   '4': cats[4], '5': cats[5], '6': cats[6], '7': cats[7], '8': cats[8],\n",
    "                   '9': cats[9], '10': cats[10]}, inplace=True)\n",
    "text = ['text_T_anxieti', 'text_T_day', 'text_T_dont', 'text_T_feel', 'text_T_ive', 'text_T_life', 'text_T_peopl', 'text_T_start', 'text_T_thing', 'text_T_time',\n",
    "        'text_T_work', 'text_T_year']\n",
    "\n",
    "imps.rename(columns={'text_0': text[0], 'text_1': text[1], 'text_2': text[2], 'text_3': text[3],\n",
    "                   'text_4': text[4], 'text_5': text[5],'text_6': text[6],  'text_7': text[7], 'text_8': text[8],\n",
    "                   'text_9': text[9], 'text_10': text[10], 'text_11': text[11]}, inplace=True)\n",
    "title = ['title_T_anxieti', 'title_T_day', 'title_T_depress', 'title_T_dont', 'title_T_feel', 'title_T_friend', 'title_T_hate', 'title_T_health', 'title_T_ill',\n",
    "        'title_T_life', 'title_T_mental', 'title_T_peopl', 'title_T_person', 'title_T_ptsd', 'title_T_social', 'title_T_suicid', 'title_T_talk', 'title_T_thing',\n",
    "        'title_T_time', 'title_T_today', 'title_T_year']\n",
    "\n",
    "imps.rename(columns={'title_0': title[0], 'title_1': title[1], 'title_2': title[2], 'title_3': title[3],\n",
    "                   'title_4': title[4], 'title_5': title[5], 'title_6': title[6], 'title_7': title[7], 'title_8': title[8],\n",
    "                   'title_9': title[9], 'title_10': title[10],\n",
    "                   'title_11': title[11], 'title_12': title[12], 'title_13': title[13],\n",
    "                   'title_14': title[14], 'title_15': title[15], 'title_16': title[16], 'title_17': title[17], 'title_18': title[18],\n",
    "                   'title_19': title[19], 'title_20': title[20]}, inplace=True)\n",
    "imps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:587: PlotnineWarning: Saving 10 x 4 in image.\n",
      "C:\\Users\\joana\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\plotnine\\ggplot.py:588: PlotnineWarning: Filename: C:\\Users\\joana\\Documents\\GitHub\\CSCI6405\\src\\data\\experiments\\feat_imp_com.png\n"
     ]
    }
   ],
   "source": [
    "plot = p9.ggplot(imps.iloc[:, -11:].melt(),\n",
    "  p9.aes(x = 'reorder(variable, value)', y = 'value', group = 'variable')\n",
    "  ) + \\\n",
    "  p9.theme_gray() + \\\n",
    "  p9.geom_boxplot() + \\\n",
    "  p9.ggtitle(\"Feature Importance for comments_popular\") + \\\n",
    "  p9.geom_hline(yintercept = 0, linetype = 'dashed', color = 'red') + \\\n",
    "  p9.coord_flip() + \\\n",
    "  p9.xlab('Feature') + \\\n",
    "  p9.ylab('Change in F1-score') + \\\n",
    "  p9.ylim(-0.007, 0.175)+ \\\n",
    "  p9.theme(\n",
    "    legend_title = p9.element_blank(),\n",
    "    legend_position = 'top',\n",
    "    legend_box_spacing = 0,\n",
    "    legend_box_margin = 1,\n",
    "    figure_size = (10, 4),\n",
    "    text = p9.element_text(size=12)\n",
    "  )\n",
    "plot\n",
    "save_file = f'C:\\\\Users\\\\joana\\\\Documents\\\\GitHub\\\\CSCI6405\\\\src\\\\data\\\\experiments\\\\feat_imp_com.png'\n",
    "plot.save(filename = save_file, dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
